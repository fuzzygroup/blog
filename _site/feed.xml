<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FuzzyBlog</title>
    <description>Scott Johnson writing about the usual array of nerd stuff: AWS / Ansible / Ruby / Rails / Elixir / Misc / Hyde.
</description>
    <link>https://fuzzyblog.io/blog/</link>
    <atom:link href="http://fuzzyblog.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 09 Sep 2019 15:26:44 -0400</pubDate>
    <lastBuildDate>Mon, 09 Sep 2019 15:26:44 -0400</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Working with ZSTD Files</title>
        <description>&lt;p&gt;I love open source developers but there are times when I question their damn naming practices.  I’m currently working with a giant data repository started as a “Z Standard” or “zstd” compressed file.  And while I know that means “Z Standard”, I can’t help but look at it as “Z std”.  Oy.&lt;/p&gt;

&lt;p&gt;Anyway.  Zstd is a &lt;a href=&quot;https://github.com/facebook/zstd&quot;&gt;Facebook standard for data compression&lt;/a&gt; and it is strikingly effective.  I’ve got over 100 gigs of JSON encoded data stored in a 13.7 gig file.  Now I am aware that text compresses actually quite well but still 100 gigs in 13.7 gigs of space feels like &lt;strong&gt;wow&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tools&quot;&gt;Tools&lt;/h2&gt;

&lt;p&gt;If you’re on a Mac then brew, as always, is your very best friend:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew install zstd
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;useful-command-lines&quot;&gt;Useful Command Lines&lt;/h2&gt;

&lt;p&gt;Assume that pol.zst is the name of the archive and it is located in your current directory.&lt;/p&gt;

&lt;p&gt;Examining a handful of records:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zstd -cd pol.zst | head -n100
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;this dumps a stream of records out that are then fed into head which limits the quantity to 100.&lt;/p&gt;

&lt;p&gt;The zstd -c and -d options mean:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-c     : force write to standard output, even if it is the console
-d     : decompression
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Integrating the often useful jq (which just gets a single json element out):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zstd -cd pol.zst | jq '.timestamp'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And like all good *nix pipelines, this is composable (this example would extract the first 1000 records and then reduce them to only the comment element from the json):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zstd -cd pol.zst | head -n1000 | jq '.comment'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Happily help is also available with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zstd --help
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;thank-yous&quot;&gt;Thank You’s&lt;/h2&gt;

&lt;p&gt;Kudos to Facebook for another great bit of Open Source contributed back to the world.  Also thanks to &lt;a href=&quot;https://grantrvd.github.io/&quot;&gt;Grant Vousden-Dishington&lt;/a&gt;, the contributor of these command lines. He’s been doing Zstd for a while; I’m the noob here.&lt;/p&gt;

</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2019/09/09/working-with-zst-files.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2019/09/09/working-with-zst-files.html</guid>
        
        <category>linux</category>
        
        <category>zstd</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Becoming a Rails Developer - Bundle Install Completes; What's Next</title>
        <description>I currently bootstrapping a friend into a Rails development cycle and here's their status:

1. RbEnv is installed.
2. Ruby is installed.
3. Application is checked out.
4. Bundle Install has completed.
5. The database is created i.e. bundle exec rake db:create has completed.

So from their parlance, &quot;the application compiles&quot; -- but what now brown cow?

The first thing you want to do is verify that things work.  I mean they should but how do you know?  

The first step is to run the test suite:

    bundle exec rspec spec
    
That should execute and not show any errors.  If that succeeds then the next step in making sure your installation is complete is to run the Rails console and do something simple:

    bundle exec rails c (and press enter and wait for it to load)
    User.where(username: 'fuzzygroup') (and press enter)
    
What this should do is launch the interactive rails console -- a REPL, similar to what you get when you type python.  A REPL (read-evaluate-print-loop) is an interactive way to do development and manipulate the state of the system.  There are two different REPLs in the ruby world -- there is **irb** which is a REPL for the ruby language and then **rails c** which is the ruby REPL but loaded with all the &quot;objects&quot; (really classes) for your application.

The command:

    User.where(username: 'fuzzygroup')
    
is a &quot;ActiveRecord&quot; statement which says &quot;Execute the User class and return any database objects with the username of 'fuzzygroup' drom the users table&quot;.  ActiveRecord is an ORM (object-relational mapper) that drives the database layer of Rails.  Unlike most ORMs, ActiveRecord actually works strikingly well.  

What you should get back from User.first at this point is basically [] and a response like this:

    User Exists? (56.5ms)  SELECT 1 AS one FROM `users` WHERE `users`.`username` = 'fuzzygroup' LIMIT 1
    []
    
The default operation for ActiveRecord's where statement is to query the database and return a collection of objects (think array although its actually richer).  In this case there was no matching object so you just got back an empty array (like class).

This brings us to an interesting ruby-ism which messes up a lot of people -- the concept of the default return.  Here's an example

    def multiply(x,y)
      x * y
    end
    
If you executed this method with multiply(2,3) then you'd get back 6 even though there is no actual return statement.  Ruby operates by returning the last evaluated expression at the end of a statement and we can use that concept right now.  Back in the Rails console type:

    result = _
    
And you'll get this:

    2.6.3 :018 &gt; result = _
      User Exists? (8.3ms)  SELECT 1 AS one FROM `users` WHERE `users`.`username` = 'fddd' LIMIT 1
    []
    2.6.3 :019 &gt; result.class
    User::ActiveRecord_Relation &lt; ActiveRecord::Relation
    
In the Rails console, the last result is always available as the special variable **_**. And you can always look at an object's class with the .class method.  

To exit the Rails console, just type quit and press enter:

    quit
    
What you have now done is to validate that the Rails system itself works.  If you can bring up a Rails console and connect to the database thru ActiveRecord then pretty much 100% of Rails itself works (the connection to Redis is something else but we're not ready for that yet).  

And this brings up to the nasty truth about any Rails environment -- without data to operate on, things, well, really suck.  This process of getting an initial set of development data to work on is referred to as &quot;seeding&quot;.  And there is both a default way to do this -- and then the way that I do it.

The default want to do this is:

    bundle exec rake db:seed
    
And that should load a series of YAML files which define the state of the objects in the system.  And while that works, I have always found that database seeding needs to be far more controllable and idempotent than a simple YAML loader.

Note: Idempotence is a fancy computer term that means:

  Idempotence (UK: /ˌɪdɛmˈpoʊtəns/, US: /ˌaɪdəm-/) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application.  [Wikipedia](https://en.wikipedia.org/wiki/Idempotence)
  
From a personal perspective, in this context, idempotence means that an object in the database should be created correctly when it doesn't exist **and** that creation command should be able to be run again -- and not have that object altered.  Here's an example - you want a user named fuzzygroup (yep that's me; it is short for me and my cats while also being a mild reference to fuzzy logic) to exist in the system.  And when you extend the database seeding routine to be more powerful, you don't want the work that was done incrementally using the user interface to be wiped out.  

Here's how I manage database seeding:

    bundle exec rake seed:init --trace
    
**Note**: Yes you should do this now if you are working on one of my codebases; otherwise this won't do a damn thing.

The command rake is a way to execute a set of ruby scripts that run from the command line. Rake is a creation of the late Jim Weirich, a wonderful rubyist, and &quot;rake tasks&quot; are a key part of the Rails infrastructure.  This command is saying: &quot;Run the seed namespace and call the init task within it.  Also give me a detailed trace of any errors&quot;.  Rake tasks live in this part of your rails codebase:

    lib/tasks/
    
All Rails codebases have a standard directory structure and lib is a top level directory in the hierarchy.  

At the end of this you should have a database that is now correctly populated with a series of objects related to the problem domain you are solving.  

The next step is to launch the default Rails web server with:

    bundle exec rails s -p3000
    
This runs Puma which is a ruby web server and you can navigate to:

    http://localhost:3000/
    
in Chrome or your default browser of choice and you should see a home page -or- a sign in page.  At this point you should dig into lib/tasks/user.rake and see if there are credentials to use (there are).</description>
        <pubDate>Sun, 08 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/09/08/becoming-a-rails-developer-bundle-install-completes-what-s-next.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/09/08/becoming-a-rails-developer-bundle-install-completes-what-s-next.html</guid>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Setting Up a Github Pages Jekyll Blog</title>
        <description>I don't recommend that anyone blog the way I do.  Git and Jekyll are both fairly user hostile but I found myself having to do this the other day so i thought I'd document it at least for myself.

Note the First: This assumes that you have a working local ruby installation and the Jekyll gem installed.  

Note the Second: These instructions are rough -- there's a fair bit of jiggery / pokery going on here and I may have missed a step.  Dust off your Google Fu, sacrifice a goat and pray.  That's usually how I setup a new blog.

Things in **bold** or otherwise emphasized are what you type.

1.  Do a **jekyll new blogname** and create a new blog.  blogname is how this is going to appear online so get that name right.
2. Change into that directory.  I don't know what it is so the only help I can give you is **cd**.
3.  In that directory, do a **git init**.
4. On github create a new repository named blogname (in step 1).
5. On your local machine, in that directory, do a:

    git remote add origin git@github.com:fuzzygroup/blogname.git

6. Do a **bundle install** to setup all the code for your Jekyll blog.
7. Edit _config.yml and setup your metadata.
8.  Do a **git add .**
9.  Do a **git commit -m &quot;initial commit&quot;**
10.  Do a **git branch gh-pages**.
11. Do a **git co gh-pages**.
12. Do a **git push origin gh-pages**

Try and view it on github. I know that's not very specific and I apologize; a bit rushed this morning.

# References

* [https://help.github.com/pages/](https://help.github.com/pages/)
* [https://help.github.com/en/articles/setting-up-your-github-pages-site-locally-with-jekyll](https://help.github.com/en/articles/setting-up-your-github-pages-site-locally-with-jekyll)

</description>
        <pubDate>Sat, 07 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jekyll/2019/09/07/setting-up-a-github-pages-jekyll-blog.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jekyll/2019/09/07/setting-up-a-github-pages-jekyll-blog.html</guid>
        
        <category>jekyll</category>
        
        <category>blogging</category>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>12 Practical Tips for Job Seekers</title>
        <description>My last job search took sending out almost 100 job applications and during that process, I managed to collect a handful of practical tips that should be helpful to job seekers:

1. Regularly Check All Mail Not Just Inbox
2. Stack Overflow has an Inbox Also
3. Cover Letters Really, Really Matter
4. If You Do Homework Make a Screencast Demoing It
5. Interviews Matter Much, Much More to You Than to the Company
6. Find Out What Coding Test They Use
7. Always, Always Review the Call Details Before the Call
8. Never Tell Your Salary
9. You'll Never Know Why You Didn't Get It; Let It Go
10. Don't Ever Stop Sending Out Resumes; Not Over Until the Fat Person Hires You
11. The Older You Are, The Harder This Is
12. The Job Market Has Changed - It Is About Keeping Bad People Out Not Getting Good People In

# 1. Regularly Check All Mail Not Just Inbox

The simplest thing I learned was obvious but absolutely unapparent.  I've been a regular user of Gmail ever since I could get an account and like most Gmail users, I rely on the default Inbox view.  The thing to understand is that Inbox doesn't always get it right in a job context.  There are lots of places in the job hunt process where a program sends you an automated email and I've found that those automated emails often do NOT show up in Inbox.

I've been faithfully using GMail since it came out and one of the things that really surprised me is the number of messages related to an ongoing job search that GMail happily filed away under &quot;All Messages&quot; and not in Inbox. Even when I had been actively in discussion with people at a given company, I would still find messages from the company showing up only in All Messages. This is hugely frustrating and it definitely cost me an interview or two until I started religiously checking All Mail every day.

# 2. Stack Overflow Has an Inbox Too

This means that at least sometimes a job you reach out to via Stack Overflow will reply to you via your Stack Overflow.  And if you aren't monitoring that Inbox then you're going to miss the opportunity.

# 3. Cover Letters Really, Really Matter

While cover letters nowadays aren't the old fashioned paper letter, they really do matter.  A cover letter gives you an additional opportunity to highlight your skills.  The cover letters I focus on most are the ones that at lease appear to be going to an actual human, say &quot;lisa@company.com&quot;.  The ones going to &quot;jobs@company.com&quot;, I worry less about.

# 4. If You Do Homework Make a Screencast Demoing It

A lot of the companies now a days require you to do homework as a way of &quot;screening&quot; you for a job.  And while I think that's a bullshite tactic - just read the damn resume people - it is a real thing.  One of the best ways you can highlight your work is to make a screencast demonstrating it.  This not only showcases your ability to communicate but also addresses any issues that people might have with installing / testing your code.

# 5. Interviews Matter Much, Much More to You Than to the Company

One thing to understand is that any one interview matters much, much more to you than to the company.  If you miss your interview then you are generally unlikely to get a second chance.  This means treat each interview carefully, put it on your calendar, etc.  

# 6. Find Out What Coding Test They Use

If the company assesses candidates with coding tests, again **bullshite** then try and find out what coding test they use before you have to take it.  Coding tests are pretty widely documented and you can bone up specifically on say the [TripleByte coding test](https://www.google.com/search?q=triplebyte+coding+test).  

# 7. Always, Always Review the Call Details Before the Call

This one is obvious but important - always review your notes / the calendar appointment prior to the call.  When you are juggling a lot of interviews, it is easy to forget that job x requires a video call that uses client software y.

# 8. Never Tell Your Salary

One solid rule of thumb that I always follow is simple - **never tell them your salary**.    I got this tip from Josh Doody of Fearless Salary Negotiation.  Josh makes the argument that hiring is inherently unfair - the company has all the information and you, the candidate has none.  Due to this information asymmetry, giving them your current salary only increases their advantage over you.  Also keep in mind that it is [illegal for California companies](https://www.shrm.org/resourcesandtools/legal-and-compliance/state-and-local-updates/pages/california-salary-history-ban-questions.aspx) to ask you your salary history. 

**Note**: Despite this being illegal in California, when a well known, venture funded California company asked me for my salary history and I cited this law, they said &quot;Well you're remote and in Indiana so we can ask you&quot;.  Needless to say I noped out of that possible job opportunity -- yes what they said might have been legally valid but their HR department's ready willingness to play fast and loose with the law told me everything I wanted to know about that company.

# 9. You'll Never Know Why You Didn't Get It; Let It Go

It has now been 2.5 years since I didn't get Job X (a Senior Software engineering slot at a leading Wedding technology company).  After passing the coding test, the coding project and being flown down for the onsite interview and absolutely nailing the whiteboard exercise, I didn't get it -- and I was never told why.  You will never know why you don't get a particular job and you simply need to let it go.

**Note**: This is a lot harder than it seems.  I still haven't let it go.

# 10. Don't Ever Stop Sending Out Resumes; Not Over Until an HR Person Hires You

Once you send out say 75 resumes and get maybe 5 interviews, you feel very much like &quot;why am I bothering?&quot;.  That's crap -- there are jobs out there but sometimes you have to keep at it.  And at it.  And at it.  Etc.

# 11. The Older You Are, The Harder This Is

If you work in technology, you will find that the older you are, the harder it gets to get a technical job.  I'm over 50 and I still write code every single day -- and I'm good at it.  But I noticed a very different response from companies after I turned 50.  I didn't take the step of editing my resume so my age wasn't apparent but it is something I would likely do my next time out.

# 12. The Job Market Has Changed - It Is About Keeping Bad People Out Not Getting Good People In

My final observation is that the HR goal is now to keep bad people out. This is a dramatic change from tech hiring in years past; it really has shifted to keeping people out not getting them in. And the corollary to this is that good people fall by the wayside – in droves.

You need to keep this in mind when you are applying for jobs and really think hard about how you differentiate yourself.

# 13. Job Hound

My final tip is a plug for one of my current side projects, [JobHound](https://jobhound.io/).  JobHound makes the process of getting a tech job, well, suck less.  Its free and you can sign up today and use it anytime.

</description>
        <pubDate>Sat, 07 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jobhound/2019/09/07/12-practical-tips-for-job-seekers.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jobhound/2019/09/07/12-practical-tips-for-job-seekers.html</guid>
        
        <category>jobhound</category>
        
        <category>hiring</category>
        
        <category>job</category>
        
        
        <category>jobhound</category>
        
      </item>
    
      <item>
        <title>Moving to Zsh on OSX in 2019</title>
        <description>Note: Before you follow these instructions, see Postscript at the end.

I have a long and twisted relationship with *nix shells.  My computing history started on 8 bit boxes where the was close to no shell (just Basic) and then transitioned to DOS.  I moved to Unix initially in 88 (Sun 3s) and then went to Linux full time in '99 and OSX finally in 2005.  And despite all that, I still struggle with *nix shells; my prompt is never right, my shell scripting blows chunks, etc.  Sigh.  But the one thing I hate, hate, hate, *censored* hate is that with Bash, my history is always screwed up.  I actually like Bash a bit but I am so damn tired of every Bash session on my machine having a different history and each terminal window racing in a Darwinian competition of &quot;let my history win!&quot;.  

Just to clarify, in a terminal your history is the list of commands that you executed.  And since the types of commands I issue can be frighteningly complex, I want access to all my previous commands - *period*.  I'm absolutely binary on this issue, access to all your commands is a good thing.  And commands being lost is a bad thing; a *censored*ing bad thing.

My good friend Sean and regular pairing partner, Sean Kennedy, told me that Z-Shell solves this and, when I had an opportunity, I was ready for it.  Here's the short process:

1. Close everything.
2. Install Mojave if you haven't (yes I'm antiquatedly retro in the speed by which I upgrade operating systems; sorry / not sorry).
3. Install Z Shell with Home Brew
4. Install Oh My Zsh which makes ZShell so much better
5. Execute a change shell command 
6. Update Your .zshrc File
7. Log out and then in on your Mac.  Yes this shouldn't be necessary but #$(#$*# Apple.

Steps 3 forward are covered below.

# 3 - Install Z Shell with Home Brew

Execute this:

    brew install zsh

# 4- Install Oh My Zsh

Execute this command:

    sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;

# 5 - Change Your Shell

You may find that installing Oh My Zsh changes your shell.  If it doesn't then you need to do this:

    chsh -s $(which zsh)

And then a:

    ln -s &quot;$ZSH_CUSTOM/themes/spaceship-prompt/spaceship.zsh-theme&quot; &quot;$ZSH_CUSTOM/themes/spaceship.zsh-theme&quot;

# 6 - Update Your .zshrc

The file .zshrc is your zsh configuration file.  Edit it as follows:

1. Open this file in an editor. 
2. Comment out the robbyrussell theme line by prefixing it with a # character.
3. Add the following lines after the robbyrussell theme line (change the username csphere to your own):

    # Oh My Zsh
    export SPACESHIP_GIT_SYMBOL=''
    export SPACESHIP_PACKAGE_SYMBOL='package '
    export SPACESHIP_NODE_SYMBOL='node '
    export SPACESHIP_RUBY_SYMBOL='ruby '
    export SPACESHIP_ELIXIR_SYMBOL='elixir '
    export SPACESHIP_XCODE_SYMBOL='xcode '
    export SPACESHIP_SWIFT_SYMBOL='swift '
    export SPACESHIP_GOLANG_SYMBOL='go '
    export SPACESHIP_PHP_SYMBOL='php '
    export SPACESHIP_RUST_SYMBOL='rust '
    export SPACESHIP_DOCKER_SYMBOL='docker '
    export SPACESHIP_AWS_SYMBOL='aws '
    export ZSH=/Users/csphere/.oh-my-zsh
    ZSH_THEME=&quot;spaceship&quot;
    ZSH_CUSTOM=~/.zsh-custom/
    plugins=(
      git
    )
    source $ZSH/oh-my-zsh.sh

# A Slack Conversation About This

    scott.johnson [9:26 AM]
    So I just tried to search my history and am I doing it right:

    history | grep ssh
      69* ssh 172.31.21.29

    that’s not showing the ssh line I got from you to get into jenkins which concerns me about the command line history.  Am I doing something wrong?

    So I found it by looking in ~/.zsh_history

    Sean Kennedy [9:32 AM]
    edit your zsh history
    err
    edit your zshrc

    scott.johnson [9:33 AM]
    ok…

    Sean Kennedy [9:33 AM]
    ```HISTFILE=~/.zsh_history
    SAVEHIST=100000```
    then source it
    will probably need to source it in every active terminal

    scott.johnson [9:33 AM]
    thx man

    Sean Kennedy [9:33 AM]
    np

# Postscript

So I just tried to follow this exactly on a new machine and it both worked and didn’t work.  It worked in the terminal window where I started but it failed in any new window.  I think the issue amounted to:

* On OSX Mojave after a chsh operation you need to log out and log back in.  My guess is that this one thing fixes everything but just in case, follow the next steps.
* I needed (perhaps) a bootstrap shell script called ~/sourcezsh.sh which has in it only three lines:

    #!/bin/bash
    # adjust as needed
    export ZSH=/Users/sjohnson/.oh-my-zsh
    source $ZSH/oh-my-zsh.sh

In the event that this fails for you then here are the links which helped me sort it out:

* [Stack Overflow on bash to ZSH](https://superuser.com/questions/362372/how-to-change-the-login-shell-on-mac-os-x-from-bash-to-zsh)
* [Thoughtbot Issues on Zsh](https://github.com/thoughtbot/laptop/issues/447)
* [Robby Russell Issues on Zsh](https://github.com/robbyrussell/oh-my-zsh/issues/6405)
* [AutoLoad and Zsh](https://stackoverflow.com/questions/30840651/what-does-autoload-do-in-zsh)

Note: I’d have solved this more definitively but my machine is rebuilding a 100gig archive of hate speech for data processing and, well, I simply can’t log in and out right now.  Sigh.

And I wouldn't have posted this except that it mostly worked and I needed it myself to move forward.  Still I hate like hell that it is ambiguous.  Sorry.

# References

 * [Rick Cogley](https://rick.cogley.info/post/use-homebrew-zsh-instead-of-the-osx-default/) 
</description>
        <pubDate>Fri, 06 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/shell/2019/09/06/moving-to-zsh-on-osx-in-2019.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/shell/2019/09/06/moving-to-zsh-on-osx-in-2019.html</guid>
        
        <category>shell</category>
        
        <category>zsh</category>
        
        <category>osx</category>
        
        
        <category>shell</category>
        
      </item>
    
      <item>
        <title>What to Do When an Engineer Leaves Your Organization</title>
        <description>No one likes it when an employee leaves the company, let alone an engineer, let alone a senior engineer.  However, the reality is that employees leave companies all the time and the role of the company in this situation is really, really simple: **protect itself**.  Once an employee leaves, the only rational choice of the company is to assume that the interests of the company and the employee are no longer aligned and to treat the departed employee as a security risk.

**Note**: This is mildly AWS specific but there should be enough goodness in here for any organization.

This is particularly acute when the engineer in question is a senior engineer as senior engineers tend to have &quot;the keys to the kingdom&quot;.  At the role I just left, I was responsible for deployment of our application which means I had access to:

* Passwords
* SSH Keys
* API Keys
* Close to every system we had accounts on  

Here is the advice I gave my former company about how to handle this:

1. Start by eliminating access to all github or bitbucket repos.  This preserves source code integrity and ensures that post leaving commit isn't snuck into a repo with some nefarious means.  This should be done before the engineer leaves the building.
2. Eliminate VPN Access.  This should be done before the engineer leaves the building.
3. Eliminate Gmail / Google Docs access.
4. Eliminate Slack Access (but bear in mind that this is a loss of institutional memory; [see here](https://fuzzyblog.io/blog/startup/2019/07/20/employee-transitions-don-t-kill-your-organizational-memory.html)).
5. If all your AWS boxes are fronted thru a VPN then you can lessen the priority on this one.  Eliminate SSH access to all public boxes.  If you don't have a VPN then lock down SSH access almost immediately.  This requires going into the /home/username/.ssh/authorized_keys file for EVERY account and eliminating all ssh keys.  Do not simply check only the user's ~/.ssh/ directory because if the departing employee had admin access, he might have attached his public ssh key onto another account.  You should note that this is sometimes done by people as a deliberate, good faith, backdoor in the event of problems.  I've done this myself on problematic boxes. 
6. Rotate your AWS credentials.  This should be done almost immediately after the engineer leaves.  Once you rotate them, they need to be refreshed on all boxes in the ~/.aws/credentials file.
7. Eliminate access to third party development tools such as [Data Dog](https://www.datadoghq.com/), [New Relic](https://newrelic.com/), [Timber.io](https://www.timber.io) or [HoneyBadger](https://www.honeybadger.com/)
8. Eliminate FTP Access.
9. Update every application level secret that is configured on your machine(s).  This means email api keys, twitter api keys, facebook keys and, well, * -- any api key that your application uses really needs to be updated.

In closing, I will admit that this is an absolute pain in the arse.  In the application I worked on, we very diligently worked hard to make sure that all application level secrets where never stored under version control, conformed to the [12 Factor](https://12factor.net/) mandate but there are still 81 plus application secrets that really should be updated -- and that is the work of probably 1 to 2 days at least. </description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/management/2019/09/05/what-to-do-when-an-engineer-leaves-your-organization.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/management/2019/09/05/what-to-do-when-an-engineer-leaves-your-organization.html</guid>
        
        <category>management</category>
        
        <category>hr</category>
        
        <category>startup</category>
        
        
        <category>management</category>
        
      </item>
    
      <item>
        <title>Rails on Windows Resources</title>
        <description>I took the time recently to help a friend get up and running with doing development on Rails under windows.  They were using the WSL i.e. Linux under Windows and it all proceeded mostly pretty smoothly.  

Here were the key resources that they used:

* [Chris Oliver's Go Rails on Windows](https://gorails.com/setup/windows/10). Highly Recommended
* [Nick Janetakis on Using WSL on Windows as a Dev Environment](https://nickjanetakis.com/blog/using-wsl-and-mobaxterm-to-create-a-linux-dev-environment-on-windows)
* [Hanselman on Rails on Windows](https://www.hanselman.com/blog/RubyOnRailsOnWindowsIsNotJustPossibleItsFabulousUsingWSL2AndVSCode.aspx)
* [Stack Overflow on MySQL on Windows](https://stackoverflow.com/questions/52487644/install-mariadb-in-windows-subsystem-linux-wsl)
* [FindShank on MySQL on Windows](https://www.findshank.com/2018/09/15/Install-mysql-on-wsl/)


</description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/09/05/rails-on-windows.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/09/05/rails-on-windows.html</guid>
        
        <category>rails</category>
        
        <category>windows</category>
        
        <category>ruby</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>A Stupid Mac Performance Optimization for 2019</title>
        <description>It is 2019 people, 2019.  And this is the stupidest performance optimization thing that I've seen in years.  I take screenshots quite a bit and I've recently noticed that it was taking a considerable amount of time from the point when I pressed the OSX screenshot key (command + shift + 4 and then enter) to the screenshot becoming available on the Desktop -- something like 20 to 30 seconds.  

What I did this morning was create a single folder on the desktop called &quot;old&quot; and then I moved almost everything into &quot;old&quot;.  Then I took a screenshot and it appeared in almost real time.  

Now before you say &quot;Oh it is just Scott -- he has a ton of things on his desktop&quot; -- there were only 654 files on my Desktop.  And, sure, 654 sounds like a lot but this is 2019 and computers are **FAST** so *WHAT THE HELL APPLE?*

Now I've seen this type of filesystem slowdown before but usually it is at the N thousands of files per folder (remember &quot;Desktop&quot; is just a folder):

* [Stack Overflow](https://stackoverflow.com/questions/466521/how-many-files-can-i-put-in-a-directory)
* [8 Million Files Per Directory](http://be-n.com/spw/you-can-list-a-million-files-in-a-directory-but-not-with-ls.html)

And yes I am running Mojave and this is the new [Apple File System](https://en.wikipedia.org/wiki/Apple_File_System) on a 2015 Mac Pro with 16 gigs of ram.  There is no excuse for this kind of slow down.  Sigh.

Oh yes and the Downloads folder gets faster when you move everything into a folder so this seems to be a generalized Apple File System issue.
</description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/mac/2019/09/05/a-stupid-mac-performance-optimization-for-2019.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mac/2019/09/05/a-stupid-mac-performance-optimization-for-2019.html</guid>
        
        <category>mac</category>
        
        <category>osx</category>
        
        
        <category>mac</category>
        
      </item>
    
      <item>
        <title>17 Years of Blogging</title>
        <description>So I was looking up information on using the wc or Word Count command in *nix on [Stack Overflow](https://askubuntu.com/questions/351326/how-do-i-list-word-count-in-multiple-files) and it made me wonder how many words I've written in 17 years of on and off blogging.  A quick command:

    wc -w _posts/*

    wc: _posts/_site: read: Is a directory
      543089 total
      
This is one of those numbers which seems staggering but I recently saw that [Russell Beattie](https://www.russellbeattie.com/blog/) claimed over a million words over 15 years.  So it seems possible.  

**Note**: If I look at the number of posts using wc -l _posts/20*.md, I get a number that seems way, way too high so I'm not going to talk about number of posts at all.

A bit of math:

    select 543089 / 17;
    +-------------+
    | 543089 / 17 |
    +-------------+
    |  31946.4118 |
    +-------------+    

So that means 32,000 words per year.  If we assume 250 words per page then:

    select 31946.4118 / 250;
    +------------------+
    | 31946.4118 / 250 |
    +------------------+
    |     127.78564720 |
    +------------------+

Or about [one novella](https://en.wikipedia.org/wiki/Novella) a year.

Now I took a pretty serious hiatus from blogging for about 8 years (2006 to 2014) in my post Feedster seclusion so that makes the years actually 17 - 8 or 9 and this changes the math a bit:

    select 543089 / 9;
    +------------+
    | 543089 / 9 |
    +------------+
    | 60343.2222 |
    +------------+

and

     select 60343.2222 / 250;
     +------------------+
     | 60343.2222 / 250 |
     +------------------+
     |     241.37288880 |
     +------------------+ 
     
If we assume that there are 250 writing days in a year (i.e the weekdays) then we can look at words per day:

    select 543089 / (9*250); 
    +------------------+
    | 543089 / (9*250) |
    +------------------+
    |         241.3729 |
    +------------------+

And that's interesting -- just about a page per day.  I am trying hard to build a writing habit these days and tracking my metrics much, much more closely.  Onward!</description>
        <pubDate>Thu, 05 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/blogging/2019/09/05/17-years-of-blogging.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/blogging/2019/09/05/17-years-of-blogging.html</guid>
        
        <category>blogging</category>
        
        <category>linux</category>
        
        
        <category>blogging</category>
        
      </item>
    
      <item>
        <title>Interesting Data Science Utilities</title>
        <description>Hacker News had an [excellent article on tools for large scale CSV / TSV / etc utilities](https://news.ycombinator.com/item?id=20848581).  If you do this type of work a lot / look at sizable amounts of raw data, I'd be strongly surprised if you didn't find a new tool here.  The things I'm looking at are visidata and octosql and gron.

Here are some of the interesting takeaways on the tool front:

* [http://jmespath.org/](http://jmespath.org/)
* [https://github.com/BurntSushi/xsv](https://github.com/BurntSushi/xsv)
* [https://github.com/dinedal/textql](https://github.com/BurntSushi/xsv)
* [https://github.com/n3mo/data-science](https://github.com/BurntSushi/xsv)
* [https://stedolan.github.io/jq/](https://github.com/BurntSushi/xsv)
* [https://gitlab.redox-os.org/redox-os/parallel](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/willghatch/racket-rash](https://gitlab.redox-os.org/redox-os/parallel)
* [https://visidata.org/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/tomnomnom/gron](https://gitlab.redox-os.org/redox-os/parallel) - JSON grep
* [https://github.com/dflemstr/rq](https://gitlab.redox-os.org/redox-os/parallel)
* [https://www.gnu.org/software/datamash/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/johnkerl/miller](https://gitlab.redox-os.org/redox-os/parallel) (written in D)
* [https://github.com/mechatroner/RBQL](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/shellbound/jwalk/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://www.rdocumentation.org/packages/plyr/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/google/crush-tools](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/python-mario/mario](https://gitlab.redox-os.org/redox-os/parallel) (python for manipulation)
* [https://github.com/cube2222/octosql/](https://gitlab.redox-os.org/redox-os/parallel) (sql for manipulation)
* [https://github.com/dkogan/vnlog](https://gitlab.redox-os.org/redox-os/parallel)
* [https://csvkit.readthedocs.io/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/eBay/tsv-utils-dlang](https://gitlab.redox-os.org/redox-os/parallel)
* [http://harelba.github.io/q/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/BatchLabs/charlatan](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/dinedal/textql](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/BurntSushi/xsv](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/dbohdan/sqawk](https://gitlab.redox-os.org/redox-os/parallel)
* [https://stedolan.github.io/jq/](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/benbernard/RecordStream](https://gitlab.redox-os.org/redox-os/parallel)
* [https://github.com/noyesno/awka](https://gitlab.redox-os.org/redox-os/parallel) (awk)
</description>
        <pubDate>Tue, 03 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/data_science/2019/09/03/interesting-data-science-utilities.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/data_science/2019/09/03/interesting-data-science-utilities.html</guid>
        
        <category>data_science</category>
        
        <category>machine_learning</category>
        
        
        <category>data_science</category>
        
      </item>
    
      <item>
        <title>Great Chrome Extensions - Tab Switcher and The Great Suspender</title>
        <description>A buddy just turned me onto [Tab Switcher](https://chrome.google.com/webstore/detail/tab-switcher/gcilookdakgpccpbcjgnpaecofklimck?hl=en), a tool that gives the ability to SEARCH THE TITLES OF YOUR TABS !!!!  How the hell doesn't this exist in Chrome already?  

Just install it and then press: 

* OSX: Control + Shift + K
* Windows: Control + Shift + K

And you will get a window like this:

![Tab Switcher](/blog/assets/tab_switcher.png) 

And beyond Tab Switcher, [The Great Suspender](https://chrome.google.com/webstore/detail/the-great-suspender/klbibkeccnjlkjkiokjodocebajanakg) is absolutely fantastic.  The Great Suspender turns off open tabs that are taking up too much memory -- it is like putting Chrome on a damn diet and it is wonderful.</description>
        <pubDate>Tue, 03 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/chrome/2019/09/03/great-chrome-extensions.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/chrome/2019/09/03/great-chrome-extensions.html</guid>
        
        <category>chrome</category>
        
        
        <category>chrome</category>
        
      </item>
    
      <item>
        <title>Generating a Good Password from the Command Line</title>
        <description>I saw my pairing partner do this about six months ago and the elegance of it just struck me.  Here's the command line:

    date +%s | sha256sum | base64 | head -c 8 ; echo
    
Let's break that down:

    date +%s
    1567406528
    
The date + %s returns what I suspect is a unix epoch i.e. the number of seconds since a date in the 1970s.

The sha256sum takes that epoch and then returns something like this:

    4d00bbff5a359a8619f48ade07860704b63a0287d80097dd041c6e538fa3ddb3
    
The base64 then takes that output and returns:

    ODBmNGIwOGZiNzc2NjU5NThlMTk2ZjY3MGFmOWYxNjQ3NWViZWNkNjBjNDg5ODUyZDgwMDMzZmM2NTkzNjE5ZCAgLQo=
    
The head -c 8 then takes 8 characters out of this:

    YzMyMGQ4
    
**Note**: The reason that the final output YzMyMGQ4 doesn't appear in the base64 example is that I keep re-running this command pipeline and date %s keeps changing since it is tied to microseconds.  And, yes, it took me a few moments to realize that.  *chuckle*

Thank you Sean Kennedy for giving me another tool in the toolbox.</description>
        <pubDate>Mon, 02 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2019/09/02/generating-a-good-password-from-the-command-line.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2019/09/02/generating-a-good-password-from-the-command-line.html</guid>
        
        <category>linux</category>
        
        <category>password</category>
        
        <category>security</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Testing A Health Check Endpoint with Curl</title>
        <description>Given the move to auto scaling architectures front ended by load balancers, it is increasingly important to build into your application what is called a &quot;health check&quot;.  This is nothing more than a known url that some external service can monitor to ensure that your application is &quot;up&quot;.  A health check does nothing more than return an HTTP 200 (and sometimes it returns a JSON representation).  Here's a simple Rails controller that I use for health checks:

    class HealthController &lt; ApplicationController
      def index
        results = {:status =&gt; &quot;ok&quot;}
        respond_to do |format|
          format.html { render :status =&gt; 200, :html =&gt; &quot;ok&quot; and return }
          format.json { render :status =&gt; 200, :json =&gt; results.to_json and return }
        end
      end
      
      # additional site monitoring functionality normally goes here
      
    end
    

Here's how to test this with curl and get the headers you'd want to see:

    curl -s -I https://foo.bar.something.com/health_check        

The options have the following meaning:

* -I, --head          Show document info only
* -s, --silent        Silent mode (don't output anything)

**Note**: The -s is silencing the normal curl output of what it fetched from the url.

What you should see is something like this:

    HTTP/2 200
    date: Thu, 29 Aug 2019 19:08:18 GMT
    content-type: text/html
    server: nginx/1.10.3 (Ubuntu)
    x-frame-options: SAMEORIGIN
    x-xss-protection: 1; mode=block
    x-content-type-options: nosniff
    set-cookie: _mkra_ctxt=3b46df998ac73a6cb44bbb8dc2a09918--200; path=/; max-age=5; HttpOnly; secure
    cache-control: no-cache
    x-request-id: 7158be3a-7990-4549-b780-fca5757069d3
    x-runtime: 0.208121
    strict-transport-security: max-age=15552000; includeSubDomains

## References

Here are some great curl references:
* [JVNS.CA - I can't recommend her work strongly enough](https://jvns.ca/blog/2019/08/27/curl-exercises/)
* [Stack Exchange](https://unix.stackexchange.com/questions/84814/health-check-of-web-page-using-curl)


</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/curl/2019/09/01/testing-a-health-check-endpoint-with-curl.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/curl/2019/09/01/testing-a-health-check-endpoint-with-curl.html</guid>
        
        <category>curl</category>
        
        <category>serverless</category>
        
        
        <category>curl</category>
        
      </item>
    
      <item>
        <title>Software Engineering Management - Rules for Successful Employee Transitions</title>
        <description>No matter how much you don't want them to, the simple fact is that employees leave your organization.  Here are some rules that I have found tend to make employee transitions more successful:

1. Don't try and get them to stay
2. Get them to be a consultant
3. Get them to write stuff down 
4. Get them to make screencasts of their development / debugging process 
5. Have them pair program with someone they don't normally do 
6. Get their contact info
7. Stay in touch

# Don't Try and Save Them

As a manager, when someone tells you they are about to leave, if they have been a good employee then your natural reaction is to try and save it, to convince them to stay.  Don't, Just **don't**.  What 30 plus years of hiring technical people has shown me is that the decision to leave an organization is **terribly hard** for technical professionals.  A big part of the reason for this is that if the technical person is any good, they tend to bond to their code base -- even if it is a bad code base.  We don't make this decision lightly and once it is made, well, it tends to be done.  Of all the people I've tried to save, I think I succeeded once -- and they left within the next six months anyway.

Here is a good [Harvard Business Review article on Considering Your Boss's Counteroffer](https://hbr.org/2019/01/if-youre-about-to-take-a-new-job-should-you-consider-your-bosss-counteroffer) that talks about whether or not to try and convince people to stay.  Thanks to reader [Lisa Meece of StarBase Indy](https://www.starbaseindy.org/) fame for the contribution.

# Consulting

Whenever I have a decent technical person leave my team, I always try to negotiate a consulting arrangement with them, right down to:

* Are you willing to consult for us?
* How much time do you have for consulting?
* What hourly rate will you charge me?

Employee transitions are a natural consequence of a free market economy and there should never be **bad feelings** -- these things simply happen.  If the person who is leaving is truly talented then why wouldn't you want them to consult for you?  This person is already:

* Trained
* Trusted
* Knowledgeable

The peculiar nature of software engineering is that certain individuals end up being domain experts and it can be hard to get a new person up to speed in their areas of expertise.  Having a consulting arrangement defined before someone leaves is a great way around this.

# Write Stuff Down

We all know that engineers are generally horrible at adding comments / writing documentation.  Despite that, you want to allocate time for any departing engineer to write documentation / comment code.  My personal suggestion would be to run that engineer's code thru a complexity analyzer like Flay (this is a Ruby tool; look for anything that measures cyclomatic complexity for your particular language) and have them document the most complex bits.

# Screencasts

Writing documentation is a lengthy, time consuming process and most people leave on a two week notice period.  An alternative to documentation is to get them to record screen casts.  This can be done by simply starting a screen recorder software tool while they narrate whatever they are doing.  I personally have done this for a multiple day period and while the screencasts weren't great quality, they were adequate for internal use.

Screencasts of their debugging / development processes, in particular, are very useful.  

# Pairing

As anyone who reads my stuff knows, I am a huge believer in pair programming.  A very useful training tool for people who are departing is to require them to pair program with someone who isn't their normal pairing partner.  This generally forces learning to occur.

# Contact Info

Even tho you might think that you have someone's current contact info, you always want to verify this.  

# Stay in Touch

People in high tech often have long careers.  I've had people that worked for me at one job, left, worked two or three places and then happened to work for me again -- when I was actually at a new job.  Staying in touch with talented individuals is a very, very positive thing and highly recommended.</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/09/01/software-engineering-management-rules-for-successful-employee-transitions.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/09/01/software-engineering-management-rules-for-successful-employee-transitions.html</guid>
        
        <category>software_engineering</category>
        
        <category>startup</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Remote Work 02 - Use Video Conferencing</title>
        <description>This is the second in an N part series of short, succinct articles about Remote Work that I'll be writing.

[All Remote Articles](https://fuzzyblog.io/blog/category.html#remote-work)

## Faces Matter So Use Video Conferencing

I know that the idea that your co-worker is a fellow human is **obvious** but I actually think that it is fairly important.  When you are a remote worker, so much of what you do is relating to people through labels, an email address, an instant message handle or a username.  These text based labels are inherently dehumanizing and that makes it much, much easier to engage in bad types of online behavior -- trolling, flaming, etc.

The simplest way to humanize an online interaction is to simply see a real, live face -- not a stylized avatar but an actual face and that's why I strongly recommend video conferencing as a key communications tool for remote work. Not only will video conferencing increase the overall &quot;humanity&quot; of your worker to work interaction but it will also clue you in to people's overall mood -- because faces tend to inherently show emotions.  And if you are a manager of remote workers, knowing if they are happy on an overall basis (anyone can have a bad day) is a key management tool because, generally, happy workers are better workers.

## My Remote Work Background

In 1996 I started my first experience with remote work when I ran an engineering team with these characteristics:

* 25 people
* 5 locations (Massachusetts, Albany, Ohio, Colorado and Leiscester, UK)
* 3 time zones
* A lead engineer who was remote
* 2 core engineering team members who came into the office so infrequently that they were officially characterized as Remote

And ever since then, I have either been remote myself or managed remote workers or both.  When I was lead developer of AppData, we took that to over $3.1 million in aggregate revenue with a fully remote development team.

</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/remote_work/2019/09/01/remote-work-02-use-video-conferencing.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/remote_work/2019/09/01/remote-work-02-use-video-conferencing.html</guid>
        
        <category>remote_work</category>
        
        
        <category>remote_work</category>
        
      </item>
    
      <item>
        <title>Remote Work 01 - Write It Down</title>
        <description>This is the first in an N part series of short, succinct articles about Remote Work that I'll be writing.  I'm taking a new remote gig and I'm finding that codifying my thoughts on remote work is useful both for me and for people that I will likely end up managing.

[All Remote Articles](https://fuzzyblog.io/blog/category.html#remote-work)

What I have found is that successful remote work cultures are ones of **documentation** or as I would say &quot;Write It Down&quot;.  In a classical work environment, you can always pop your head over the cubicle and ask the person next door.  In a remote work environment, not only is that not an option but you are often working asynchronously from your co-workers so they may not even be awake when you need something.  

I don't personally care whether the writing mechanism is a blog, a wiki, a shared Google doc, or a paid Slack environment where messages are never deleted -- it simply needs to be:

* Always Available
* Persistent
* Searchable
* Linkable 
* Easy to Write In

I'll write more about specific writing tools and how they function in a remote work environment over the next N articles.

If you don't build a culture of documentation for your remote work environment, what you are going to find are subtle impedances to successful work.  People will constantly be scrambling for key information and there will be resulting task slippage.  

## My Remote Work Background

In 1996 I started my first experience with remote work when I ran an engineering team with these characteristics:

* 25 people
* 5 locations (Massachusetts, Albany, Ohio, Colorado and Leiscester, UK)
* 3 time zones
* A lead engineer who was remote
* 2 core engineering team members who came into the office so infrequently that they were officially characterized as Remote

And ever since then, I have either been remote myself or managed remote workers or both.  When I was lead developer of AppData, we took that to over $3.1 million in aggregate revenue with a fully remote development team.</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/remote_work/2019/09/01/remote-work-01-write-it-down.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/remote_work/2019/09/01/remote-work-01-write-it-down.html</guid>
        
        <category>remote_work</category>
        
        
        <category>remote_work</category>
        
      </item>
    
      <item>
        <title>Color Coding Your Gmail with Labels</title>
        <description>Back in 2001 or 2002, I took a stab at making client side software once again with an Outlook Add In called Inbox Buddy. Developed by myself and my co-founder, Inbox Buddy acted to dynamically organize your Outlook Inbox.  The single best feature in Inbox Buddy was that it color coded your email based on observing your messaging patterns as well as a content analysis and a small training step.  It is now almost 20 years later and I am still missing Inbox Buddy.  

**Note:** Inbox Buddy failed not because it was a bad product but because Outlook's plugin architecture at the time, well, sucked monkey chunks.  Also I made the foolish decision to target Outlook 2000 instead of subsequent releases which had a better approach to plugins. And then I started a [blog search engine](https://fuzzyblog.io/blog/category.html#feedster) and got really, really busy ... Ah, well, live and learn.  Sigh.

Color coding is an incredibly powerful tool for organizing information and one so damn simple that everyone misses it.  Recently I thought to google for &quot;Color Code&quot; Gmail and, surprise, surprise, this can be done [albeit manually](https://www.zdnet.com/article/gmail-quick-tip-use-color-coded-labels-to-add-organization-to-your-inbox/).  

I found the Zdnet technique either confusing or not entirely accurate with the current version of Gmail so I wrote up my own description.  

The way that this works is:

1. You create a label that represents the context of the mail you want to color (I have two, personal and ADL for my new gig at the ADL).
2. You set the color of the label.
3. You create a filter or filters that apply that label.

# Step 1: Create a Label

Open Gmail and in the sidebar of labels, example the More link at the bottom of the sidebar.  Scroll all the way down and select the Create New Label option.

# Step 2: Color the Label

Scroll to the label you just created and select the 3 vertical dots option to get a context menu about the label.  Select the Label Color option and then choose a color for the label.

# Step 3: Create a Filter

At the right hand top of Gmail there is a gear icon.  Select the Settings option from the menu.  From the Gmail Settings window, select Filters and Blocked Addresses option.  Scroll to the bottom and click Create a new filter.  Enter an email address or domain name in the From field (or write the filter however you like) and select Create filter.  On the second screen, select Apply the label and choose the label you just created.  You might also want to select the Never send it to spam option.  Finally you should generally always select the Apply to existing conversations option so the label is applied to all messages in your inbox.</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/gmail/2019/09/01/color-coding-your-gmail-with-labels.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/gmail/2019/09/01/color-coding-your-gmail-with-labels.html</guid>
        
        <category>gmail</category>
        
        <category>email</category>
        
        <category>inbox_buddy</category>
        
        
        <category>gmail</category>
        
      </item>
    
      <item>
        <title>A Python and Data Science Tooling Vocabulary for a Rubyist</title>
        <description>A Python and Data Science Tooling Vocabulary for a Rubyist

I am unabashedly a Ruby guy.  I've now spent 13 years immersed in Ruby on a daily basis.  But a new consulting gig has me delving into Python both as a light implementer and as a likely remote manager of some Python folk in a Data Science / Machine Learning context.  

I wrote this as a regularly updated document so I have a place to stick new vocabulary items I learn.

A lot of this is the names of libraries and tools because learning any language isn't just the language, it is the constellation of stuff that make it useful for a given task.  The focus here is clearly on scientific computing and machine learning.

You should also likely see [Python Glossary](https://www.pythonforbeginners.com/cheatsheet/python-glossary).


# A 
* **Anaconda** - A packaged distribution of Python and R aimed at Data Science.  [More...](https://www.anaconda.com/distribution/
#download-section).  Includes multiple bits of tooling such as Jupyter.
*** Anaconda Cloud** - A platform for sharing notebooks and packages.

# B
* Bert - Bert or Bidirectional Encoder Representation from Transformers is a state of the art (2018) language model for natural language processing (NLP).  Bert is based on a Google paper which shows that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. [More...](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
* **Bokeh** - Bokeh is an interactive visualization library that targets modern web browsers for presentation.  [More...](https://bokeh.pydata.org/en/latest/index.html)


# C
* **Conda** - an open source package manager for &quot;any language&quot; but originally for Python.  [More...](https://conda.io/en/latest/)


# D 
* **Dask** - Dask is a flexible library for parallel computing in Python. Dask is composed of two parts: ... “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments.  [More...](https://dask.org/)
* **DataShader** - Datashader is a graphics pipeline system for creating meaningful representations of large datasets quickly and flexibly. Datashader breaks the creation of images into a series of explicit steps that allow computations to be done on intermediate representations.  [More...](http://datashader.org/)


# E 
* **Egg** - a Python component.  Think Ruby gem. 


# F

# G

# H
* **H20.ai **- H2O.ai is the creator of the leading open source machine learning and artificial intelligence platform trusted by hundreds of thousands of data scientists.  [More...](https://www.h2o.ai/)
* **Holoviews** - HoloViews is an open-source Python library designed to make data analysis and visualization seamless and simple.  [More...](http://holoviews.org/)

# I 

# J
* **Jupyter Notebook** - The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.  python3 -m pip install jupyter and then run it with: jupyter notebook
* 
* 
* 

# K 

# L

# M
* **MatPlotLib** - Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. [More...](https://matplotlib.org/)

# N
* **Numba** - Numba is an open-source JIT compiler that translates a subset of Python and NumPy into fast machine code using LLVM.  [More...](https://en.wikipedia.org/wiki/Numba)
* NumPy - A standard numerical library for Python.  NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.  [More...](https://en.wikipedia.org/wiki/NumPy)

# O

# P 
* **Pandas** - pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.  [More...](https://pandas.pydata.org/)
* **Pip** - A Python package installer.  Example for python 3: python3 -m pip install --upgrade pip
* 
* **PyCharm** - an ide for Python from the JetBrains folk.
* **Pythonic** - something that is done in a very Python like way.
* **PyTorch** - PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's artificial intelligence research group. It is free and open-source software released under the Modified BSD license. [More...](https://en.wikipedia.org/wiki/PyTorch)

# Q

# R
* **Repl** - This is short for Read Evaluate Print Loop and it is the result of what you get when you type python at your command prompt.  A repl gives you a place to type Python code you want executed. Type quit() to exit the Python repl -- the () are required as quit is a method not a statement.  Think irb or &quot;rails c&quot;.

* **Roberta** - A robustly optimized method for pretraining natural language processing (NLP) systems that improves on Bidirectional Encoder Representations from Transformers, or BERT, the self-supervised method released by Google in 2018. [More...](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)

# S
* **Scikit-learn** (formerly scikits.learn) is a free software machine learning library for the Python programming language.   It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.[More...](https://scikit-learn.org/stable/)
* **SciPy** - SciPy is a free and open-source Python library used for scientific computing and technical computing. [More...](https://en.wikipedia.org/wiki/SciPy)

# T
* **TensorFlow** - TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.[5] It is used for both research and production at Google.  [More...](https://en.wikipedia.org/wiki/TensorFlow)
* **Torch** - Torch is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language.[3] It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation. As of 2018, Torch is no longer in active development.[4] However, PyTorch is actively developed as of August 2019. [More...](https://en.wikipedia.org/wiki/Torch_(machine_learning))

# U

# V
* **VirtualEnv** - a virtual environment manager allowing you to have more than one Python on a machine.  Think RbEnv or RVM.  [More...](https://www.geeksforgeeks.org/python-virtual-environment/) pip install virtualenv

# W

# X

# Y

# Z</description>
        <pubDate>Sun, 01 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/python/2019/09/01/a-python-and-data-science-tooling-vocabulary-for-a-rubyist.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/python/2019/09/01/a-python-and-data-science-tooling-vocabulary-for-a-rubyist.html</guid>
        
        <category>python</category>
        
        <category>data_science</category>
        
        <category>machine_learning</category>
        
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Fixing Ruby readline.bundle Image Not Found</title>
        <description>I've seen a lot of ruby weirdness over the years but this one was new to me:

    1) Metric#color reps should be green when the goal is increase_is_better = true and the change is +
       Failure/Error: byebug

       LoadError:
         dlopen(/Users/sjohnson/.rvm/rubies/ruby-2.5.1/lib/ruby/2.5.0/x86_64-darwin16/readline.bundle, 9): Library not loaded: /usr/local/opt/readline/lib/libreadline.7.dylib
           Referenced from: /Users/sjohnson/.rvm/rubies/ruby-2.5.1/lib/ruby/2.5.0/x86_64-darwin16/readline.bundle
           Reason: image not found - /Users/sjohnson/.rvm/rubies/ruby-2.5.1/lib/ruby/2.5.0/x86_64-darwin16/readline.bundle
           
The solution turned out to be:

    ln -s /usr/local/opt/readline/lib/libreadline.8.0.dylib /usr/local/opt/readline/lib/libreadline.7.dylib
    
Thank you to [Zuhlfreelancer](https://gist.github.com/zulhfreelancer/47efc39584cb9f006da43c41c014e03a)
</description>
        <pubDate>Fri, 30 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2019/08/30/fixing-ruby-readline-bundle-image-not-found.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2019/08/30/fixing-ruby-readline-bundle-image-not-found.html</guid>
        
        <category>ruby</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Getting Your First Tech Job</title>
        <description>This blog post is what I call micro-focused.  A friend of a friend emailed me about his concerns about breaking into the tech field and rather than solve his issues on a 1:1 basis, I'm writing it here because I suspect this is useful to more people than just him (or her).

Let's start with his email:

Hope everything is going well. Have gotten real discouraged on the app project I had told you about. The initial meeting has been postponed twice, nothing on the books now either. The person I have been talking to about it just gives me the impression those in charge are real disorganized and this isn't exactly a high priority in their busy lives. So I am thinking just letting them come to me if they want but otherwise taking a passive approach. With that being said, I am taking 9 credit hours, all 400 level CS classes, this semester. The dream is to find some part time work in the IT field, but am trying to be cautious in how much I take on, have definitely over burdened myself in the past and it was horrible. So that's where I am at now. I was hoping you could give my some guidance on the whole process. I have been lucky and able to find work generically and by word of mouth, but am absolutely horrible with the normalized resume and application process. Not sure even where to start asking questions but I image you could provide some great guidance. As always I know you are a busy guy, but appreciate any time and help ya can give. Eager to hear back from ya!

So let's begin with an understanding of the goal in high tech hiring:

The goal is to keep bad people out.  This is a dramatic change from tech hiring in years past; it really has shifted to keeping people out not getting them in.  And the corollary to this is that good people fall by the wayside -- in droves.

That's the goal of high tech hiring circa 2019.  I don't care what any HR department tells you, the goal is simply risk avoidance.  No one wants to take a chance on a bad decision because bad people in an engineering context leave a terrifying legacy (sometime ask me in person about def run and I'll tell you a terrifying tale about someone I hired once for 1/2 day and how that broke deploy on our code base for 3 damn days; sigh).

What you have to keep in mind is that if no one wants to take a chance then you really need to stand out, particularly to get past the f*cking wall that is automated resume scanners, keyword matchers, etc.  Getting a tech job these days is hard for anyone -- hell the second to last time I was in the market for a job, I had to write [Job Hound](https://www.jobhound.io) just to manage the damn process; sigh.

Now, all that said, there are definitely things that you can do to stand out and here's what I would recommend to this individual:

* **Know what you want**.  There is no such thing as a &quot;IT Job&quot;; you might be a developer, you might be a tester, you might be QA, but figure out what you actually want to do because passion, trite as it sounds, is really, really important.  If what you want to do is Python based Data Science then write your resume for that -- trust me there is a market for almost anything.
* **Make Sure Your Resume Works**.  For my new gig, I trusted enough to let a friend take a swing at radically changing my resume and, what do you know, employment (ok consulting but still).  And [my friend](https://www.lisameece.com/) is now offering this as a service.  And to put my money where my mouth is, here's [my resume that they fixed](https://fuzzyblog.io/blog/assets/jsjohnson_resume_2019.pdf); the end result was far more readable and far more scannable -- and I got the job!
* **Network / Meetups**.  Getting hired via a resume over the transom is a suck ass process.  You will find things work much better if you network and while that sounds intimidating, there is an easy way -- Meetup.  There are meetups in virtually every technology and if you go there you can find the people in your area who really care about what you do.  Let's say you are in the Indianapolis area and you care about Python and Data Science.  Well here is the search for [Python](https://www.meetup.com/find/events/?allMeetups=false&amp;keywords=python&amp;radius=10&amp;userFreeform=Indianapolis%2C+IN&amp;mcId=z46226&amp;mcName=Indianapolis%2C+IN&amp;eventFilter=mysugg) and here is the search for [Data Science](https://www.meetup.com/find/events/?allMeetups=false&amp;keywords=data+science&amp;radius=10&amp;userFreeform=Indianapolis%2C+IN&amp;mcId=z46226&amp;mcName=Indianapolis%2C+IN&amp;eventFilter=mysugg).  And if you're intimidated going by yourself the first time, well, bring a friend.  Also if you think that &quot;I won't be accepted&quot;, well, keep in mind that Meetups do tend to be pretty inclusive and are often micro focused -- there is even a &quot;La Femme Pythonista&quot; meetup here in Indianapolis.  
* **Practice Coding Tests**.  It is pretty common nowadays for new people in particular to have to take coding tests.   This is a skill that you can practice like anything else.  Personally I'd start with [TripleByte](https://triplebyte.com/) but there are lots of them.  As a side note, I HATE coding tests and find them to be a crap ass metric that only tells an HR department that this person can pass (or cheat) a test but no one ever asks me ...
* **Read / Learn**.  If you are entering the tech world then a continuous focus on learning is highly recommended.  I follow tech news in two ways, I read [Hacker News](https://news.ycombinator.com) religiously for the big picture and then I follow [Ruby Weekly](https://rubyweekly.com/) for stuff about my environment of choice.
* **Side Project**.  As a noob, it is easy to hear &quot;start a side project&quot; but that is intimidating as hell.  A simpler approach is to approach someone with a side project and say &quot;Hey - can I help?&quot;.  As an example, JobHound is dying for some love and it is definitely a way to learn.  But there are literally tens of thousands of side projects that need some help.  And once you start helping on a side project that goes on your damn resume.
* **Github Profile**.  If you don't have a Github profile in 2019, run, don't walk and get one.  And then do something with it -- write docs for some open source project, find a tool you can contribute to, etc.  One of the best guys I ever hired, I did so not for the code he wrote on Github but for the docs he contributed to Ruby.  No one writes docs and someone that does is a damn precious resource.  
* **Answer Stack Overflow Questions**.  [Stack Overflow](https://www.stackoverflow.com) is an absolutely precious resource and one that always needs help.  Find the question stream on your technology of choice and start answering.  
* **Read Josh Doody**.  Josh Doody is the author of [Fearless Salary Negotiation](https://fearlesssalarynegotiation.com/) and Fearless is the single best book I've ever read on salary negotiation.  I recommended it to a friend recently who got a 38% boost in her comp when she changed jobs.  She then pushed me to follow it and I got a 45% boost in my comp.  I cannot recommend this book strongly enough.  Even if you don't negotiate on your first job (which you might not because you lack power, you want to know this for your future).
* **Research the Process**.  The [Ask a Manager blog](https://www.askamanager.org/) is a pretty great resource in terms of resumes, cover letters and the like, particularly for early career folk.
* **Manage the Process**.  This last item is a plug for one of my current side projects, [JobHound](https://jobhound.io/).  JobHound makes the process of getting a tech job, well, suck less.  

In closing there are a lot of things that you can do to make yourself more marketable and I'm not saying that you have to (or even can) do all of them but there are definitely steps to take.</description>
        <pubDate>Thu, 29 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jobhound/2019/08/29/getting-your-first-tech-job.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jobhound/2019/08/29/getting-your-first-tech-job.html</guid>
        
        <category>jobhound</category>
        
        <category>hiring</category>
        
        <category>job</category>
        
        
        <category>jobhound</category>
        
      </item>
    
      <item>
        <title>Nushell Rocks - Go Yehuda Go</title>
        <description>NuShell is a damn interesting approach to a shell.  Here's a description:

&quot;Today, we’re introducing a new shell, written in Rust. It draws inspiration from the classic Unix philosophy of pipelines, the structured data approach of PowerShell, functional programming, systems programming, and more.&quot;

Nu is written by Jonathan Turner, Yehuda Katz and more.  Given that I'm a rubyist, it is no surprise that I'm a Yehuda Katz fan (hence the reference to Yehuda in the title).  But this is Jonathan Turner's project.

I had a bunch of issues getting running with Nu so I wrote up this blog post.  
# Installing Nu

In order to install Nu, you need the whole Rust environment and toolchain.  Additionally you need to make sure that you are using the nightly build of Rust.  I'm on OSX and this is what I did:

    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
    source $HOME/.cargo/env
    rustup default nightly
    cargo install nu

Depending on the state of your OSX Mojave installation, you may need to run:

    xcode-select --install
    
before your installation will complete (in this event, just re-run from the curl statement).

# Running Nu

Just call the nu executable:

    nu

Here's an example of what Nu looks like:

     ls Desktop/| sort-by name | first 5

    ---+--------------------+------+----------+----------+--------------+-------------+-------------
     # | name               | type | readonly | size     | created      | accessed    | modified 
    ---+--------------------+------+----------+----------+--------------+-------------+-------------
     0 | Desktop/.DS_Store  | File |          | 92.2 KB  | 2 years ago  | 2 weeks ago | a day ago 
     1 | Desktop/.localized | File |          | &lt;empty&gt;  | 12 years ago | 2 years ago | 2 years ago 
     2 | Desktop/1.png      | File |          | 255.2 KB | 2 years ago  | 2 years ago | 2 years ago 
     3 | Desktop/2.png      | File |          | 269.7 KB | 2 years ago  | 2 years ago | 2 years ago 
     4 | Desktop/3.png      | File |          | 276.3 KB | 2 years ago  | 2 years ago | 2 years ago 
    ---+--------------------+------+----------+----------+--------------+-------------+-------------

And nu extends into even ps:

    ps | where cpu &gt; 1 | sort-by cpu | first 5

    ---+-------+----------+------+--------------------------------------------------------------------------------------------------
     # | pid   | status   | cpu  | name 
    ---+-------+----------+------+--------------------------------------------------------------------------------------------------
     0 | 89054 | Runnable | 1.34 | /Applications/Firefox.app/Contents/MacOS/firefox 
     1 | 313   | Runnable | 2.45 | /System/Library/CoreServices/Finder.app/Contents/MacOS/Finder 
     2 | 766   | Runnable | 4.80 | /System/Library/PrivateFrameworks/PhotoLibraryPrivate.framework/Versions/A/Support/photolibraryd 
    ---+-------+----------+------+--------------------------------------------------------------------------------------------------

# Exiting Nu

Press ctrl+c twice.

# Example Commands

Here are some interesting command examples:

    ls
    ls | where size &gt; 4kb
    ls | where size &gt; 1gb
    ps | where cpu &gt; 5
    open file.json
    ls | get name | echo $it
    open https://www.jonathanturner.org/feed.xml
    ls | sort-by size
    ls | pick name size
    ls | sort-by size | first 5
    ls | sort-by size | first 5 | skip 2
    ls | sort-by name
    enter docs 
    (this is changing into a directory and then you have a second embeded shell 
      which you can navigate between with n and p)

# References
* [Website](http://www.nushell.sh/)
* [Introduction](http://www.jonathanturner.org/2019/08/introducing-nushell.html)
* [Hacker News](https://news.ycombinator.com/item?id=20783006)
* [The Nu Book](https://book.nushell.sh/en/)
* [Rustup](https://rustup.rs/)
* [Nushell Github](https://github.com/nushell/nushell)
* [Reddit](https://www.reddit.com/r/rust/comments/cukfnj/announcing_nushell_a_modern_shell_written_in_rust/)
* [Rust Nightly](https://www.oreilly.com/library/view/rust-programming-by/9781788390637/e07dc768-de29-482e-804b-0274b4bef418.xhtml)
* [Installation](https://book.nushell.sh/en/installation)
</description>
        <pubDate>Tue, 27 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/nushell/2019/08/27/nushell-rocks-go-yehuda-go.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/nushell/2019/08/27/nushell-rocks-go-yehuda-go.html</guid>
        
        <category>nushell</category>
        
        <category>rust</category>
        
        
        <category>nushell</category>
        
      </item>
    
      <item>
        <title>Forensic Computing 3 - When Docker Breaks SSH</title>
        <description>I've written a few of these now.  I term it &quot;forensic computing&quot; when I've been given a problem to solve that I really have no idea how to approach and it is just a damn mystery.  The problem here was that for my day job:

* We are an AWS shop and all of this happens on AWS
* We moved from OpenVPN as a VPN client to something called TunnelBlick
* We lost the ability to ssh in - or ping - our Jenkins slave box.  Not any other box.  Even the Jenkins master box was fine.  This only affected the Jenkins slave box.
* Of all the computers we own, only I had the ability to SSH in / ping the Jenkins slave box
* Given that I'm a cranky, old engineer, of all the people we had, only I refused to install the new VPN client (in my defense; I saw that a number of people had problems with it and I *have* to have VPN access at all time).

**Note 1**: I should note that even though I'm the one writing this up, our Director of Cloud Operations (ST) was instrumental to solving this and his assistance was greatly appreciated.

**Note 2**: Yes the answer is in the title but keep reading, it gets interesting.

So all I really knew as I started looking into this was that, somehow, the VPN was likely involved because this problem only started after the new VPN came into being.

# Step 1: Confirmation

The first step was to confirm that this problem still exists.  We did this by both of us pinging the Jenkins slave box. And sure enough:

**Me**: worked

**ST**: failed

The fact that ping itself failed was very interesting because ping is such a core bit of Internet technology.  SSH can be mildly complicated but ping is **simple**.  Ping should always work.  Always.

# Step 2: Firewall Settings

The next step was to take the obvious dive into firewall settings at the EC2 instance level and make sure that the Jenkins slave box had the same firewall settings as the Jenkins master box.  Not only were there the same firewall settings but they were in the same order.

This left us with the conclusion that it was something about the actual box itself -- but what???

# Step 3: So What's Different About the Jenkins Slave???

A brief description here of Jenkins is likely necessary as not everyone who reads my stuff is, &quot;Engineer Nerdy AF&quot;, which is the description of the (few) readers I generally have.  

[Jenkins](https://jenkins.io/) is described as:: &quot;The leading open source automation server, Jenkins provides hundreds of plugins to support building, deploying and automating any project.&quot;

Jenkins is a big Java program that runs as both the Master node which controls everything and then as Slave nodes which do the actual work.

So, for us, everyone could access the Jenkins Master box but only I could access the Jenkins Slave box.

When you look at Linux networking issues, two key tools are UFW (another firewall) and iptables (how to handle packets / networking).  Here's the result of ufw:

    sudo ufw status verbose
    Status: inactive

This ruled out ufw as a source of the problems so it was onto iptables:

    sudo iptables -L
    Chain INPUT (policy ACCEPT)
    target     prot opt source               destination
    
    Chain FORWARD (policy DROP)
    target     prot opt source               destination
    DOCKER-USER  all  --  anywhere             anywhere
    DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
    DOCKER     all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
    DOCKER     all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere
    ACCEPT     all  --  anywhere             anywhere

    Chain OUTPUT (policy ACCEPT)
    target     prot opt source               destination

    Chain DOCKER (2 references)
    target     prot opt source               destination
    ACCEPT     tcp  --  anywhere             ip-172-18-0-2.some-aws-region-8.compute.internal  tcp dpt:6379
    ACCEPT     tcp  --  anywhere             ip-172-18-0-3.some-aws-region-8.compute.internal  tcp dpt:postgresql

    Chain DOCKER-ISOLATION-STAGE-1 (1 references)
    target     prot opt source               destination
    DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
    DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere
    RETURN     all  --  anywhere             anywhere

    Chain DOCKER-ISOLATION-STAGE-2 (2 references)
    target     prot opt source               destination
    DROP       all  --  anywhere             anywhere
    DROP       all  --  anywhere             anywhere
    RETURN     all  --  anywhere             anywhere

    Chain DOCKER-USER (1 references)
    target     prot opt source               destination
    RETURN     all  --  anywhere             anywhere

And this is where ST came fully into play -- he looked at the iptables and almost immediately seized on the 172-18 address as a key issue.  Apparently this is part of how he has configured our VPN.  And since the 172-18 issue was tied to  [Docker](https://www.docker.com/), we were on the hunt for how Docker might mess with networking that would break SSH.   

# Step 4: So Docker ...

A good friend of mine is a [Docker instructor](https://nickjanetakis.com/) so I'm pretty familiar with Docker.  Here are some things about Docker:

* Docker is always running so it is a candidate for interfering with stuff
* Docker has pretty extensive networking facilities 
* Docker modifies existing system stuff like the Routes table
* Docker can be a furball of complexity because Docker gives you a computer within your computer (virtualization) and that means it is actually an operating system at its core.

So, technically, Docker could mess with SSH even if it feels absolutely bizarre for this to even be possible.

# Step 5: Proving This

The first step was for us to prove this and rather than mess with Docker configuration, the much easier approach was just to turn off Docker using:

    systemctl stop docker

And after that, we repeated the earlier ping test:

**Me**: worked

**ST**: worked

And that told us that, Yes Virginia, Docker really can break SSH, DAMN IT!  At least for **us** and how we do networking / VPN.

# Step 6: Addressing This

So our basic thesis is this:

1. Docker starts up.
2. Docker grabs network addresses in the range of 172.18.
3. We use 172.18 ourselves in a VPN context.
4. This prevents the Jenkins slave box from being accessible via SSH or even ping.
5. We need to tell Docker NOT to use these addresses.

And with that thesis, we knew what was next -- Google.  The research that we did and things we found are listed below under References.

# Step 7: Fixing This or Docker is a Dirty, Lazy Teenager

Our first thought was that the 172-18 network address specification was located in docker-compose.yml.  And, while we knew that existed, we had no idea where it was but a quick:

    sudo find / -name &quot;docker-compose.yml&quot;

revealed its location.  Groveling through it, though, proved to be a wash -- no networking commands.  A similar trawl through files on the machine proved fruitless as there were no Dockerfile(s) telling us what was going on.  

Google, as always, delivered the answer.  We learned that the core Docker daemon (the background program governing all of Docker) itself can be customized with a file called daemon.json.  So we did this:

    sudo vi /etc/docker/daemon.json

and we discovered that by default, it simply isn't there.  Here's what we started with from the Internet:

    {
      &quot;bip&quot;: &quot;192.168.1.5/24&quot;,
      &quot;fixed-cidr&quot;: &quot;192.168.1.5/25&quot;,
      &quot;fixed-cidr-v6&quot;: &quot;2001:db8::/64&quot;,
      &quot;mtu&quot;: 1500,
      &quot;default-gateway&quot;: &quot;10.20.1.1&quot;,
      &quot;default-gateway-v6&quot;: &quot;2001:db8:abcd::89&quot;,
      &quot;dns&quot;: [&quot;10.20.1.2&quot;,&quot;10.20.1.3&quot;]
    }

And here's what ended up working:

    {
      &quot;default-address-pools&quot;:[
        {&quot;base&quot;:&quot;10.10.0.0/16&quot;,&quot;size&quot;:24},
        {&quot;base&quot;:&quot;10.11.0.0/16&quot;,&quot;size&quot;:24}
      ]
    }

And we were certain that we were **right** and all would be good -- only it didn't *censored* work.  Now I have been using Docker on and off for a bunch of years and this experience made me remember my opinion of Docker's engineering practices.  Docker often seems to run engineering as if they are dirty, lazy teenagers.  Specifically Docker doesn't seem to clean up after itself (example - tickets stay open way too long; things are poorly documented, etc).  

**Note**: I am a parent to teenagers.  I can say this.

So my suspicion was that Docker had modified something at the machine level and I needed to find out what that change was and delete it.  At this point, I'm going to cut to the chase now because this is way too long.  The short answer was a change was left behind by Docker in the routes table that needed to be deleted.  A quick look at routes gave us:

    sudo ip route show

    default via 172.31.16.1 dev ens5 proto dhcp src 172.31.23.113 metric 100
    10.10.0.0/24 dev docker0 proto kernel scope link src 10.10.0.1 linkdown
    172.18.0.0/16 dev br-9942c98d99d1 proto kernel scope link src 172.18.0.1 linkdown
    172.31.16.0/20 dev ens5 proto kernel scope link src 172.31.23.113
    172.31.16.1 dev ens5 proto dhcp scope link src 172.31.23.113 metric 100

and then it was just:

    sudo route del -net 172.18.0.0/16

With that and then a final Docker stop and start all this madness was done.  The final task was to then launch our test suite via Jenkins and watch it function correctly.

# But What About Verifying the Fix

I can't explain why the first time we turned Docker off, we were able to verify that it worked but yet there was a permanent route left in at the machine level.  My suspicion is some kind of edge case glitch but who knows.  I mean when you use teenager style engineering practices ...

# Thank You

A robust thank you to our Director of Cloud Operations, ST, who was stellar at working this with me.

# References

* [IP Tables How To](https://help.ubuntu.com/community/IptablesHowTo)
* [Deleting Routes](https://www.poftut.com/delete-route-ubuntu-linux/)
* [Viewing Routes](https://vitux.com/how-to-view-the-network-routing-table-in-ubuntu/)
* [Stopping Docker](https://stackoverflow.com/questions/42365336/how-to-stop-docker)
* [Restarting Docker](https://docs.docker.com/engine/reference/commandline/restart/)
* [Work with Network Commands](https://docs.docker.com/v17.09/engine/userguide/networking/work-with-networks/)
* [Finding Files in Linux](https://www.linode.com/docs/tools-reference/tools/find-files-in-linux-using-the-command-line/)
* [Docker Presentation](https://sudo-bmitch.github.io/presentations/dc2019/tips-and-tricks-of-the-captains.html#p21)
* [Daemon](https://docs.docker.com/engine/reference/commandline/dockerd/)
* [Customizing the Default Bridge Network](https://docs.docker.com/v17.09/engine/userguide/networking/default_network/custom-docker0/)</description>
        <pubDate>Tue, 27 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2019/08/27/forensic-computing-3-when-docker-breaks-ssh.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2019/08/27/forensic-computing-3-when-docker-breaks-ssh.html</guid>
        
        <category>docker</category>
        
        <category>linux</category>
        
        <category>networking</category>
        
        <category>ssh</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>The Power of Why</title>
        <description>**Note:** I wrote this as a section of an unfinished book about managing software engineers, &quot;Managing the Unmanageables&quot;, when you are **not** a software engineer.  I keep copying it and pasting it to people so I thought it should just live here.

# Applying Psychology: The Power of Why

One of the powerful tools in your arsenal of management techniques for dealing with software engineers is three letters long — **why**.  As a manager, even as a non technical person managing technical people, you always have the ability to ask **why**.  I would argue that if you don’t ask why, at least some of the time, you’re not doing your job.  Understanding “why” is, after all, at the heart of management.
The power of why taps into four of these psychological vectors: pride in his work, poor social skills, the desire to not explain his or her self and the belief that they are smarter than you.  So let’s posit a dialog between you and a software engineer:

Manager:  Bob — we need to get that download bug fixed today.  Its a big deal.
Bob: Well that’s actually pretty hard.  I’d need to *nerd level gobbledygook gook speak here*

A lot of times that’s where the dialog ends.  The software engineer throws up a wall and relies on you not understanding this.  Now let’s revisit that and add why to the mix:

Manager:  Bob — we need to get that download bug fixed today.  Its a big deal.
Bob: Well that’s actually pretty hard.  I’d need to *nerd level gobbledygook gook speak here*
Manager: That’s interesting and I don’t understand everything you just said but I do know that our product used to be able to download data and now it can’t.  Can you explain why?

And this is the point where Bob either has to explain himself as a human being or do the work.  And, honestly, he’d rather do the work than talk to you.  Don’t take offense at this.  It is actually both what you and and to you advantage so its a good thing.  Bear in mind that Bob would rather talk to a computer than talk to you.  If he really wanted to talk to people, he’d have your job.  Finally if he really is smarter than you, or at least things he is, shouldn’t he be able to fix it?  

One thing to know about technical people — we hide behind our jargon because it gives a way to make difficult social interactions go away.  If I use words that you don’t understand then it naturally intimidates you because it makes you feel stupid and me feel superior.

The power of why is a very real thing.  I cannot tell you the number of times that simply asking “why” has resulted in the engineer saying “Fine!  I’ll just fix it”.  

The other thing to understand about the power of why is that software can be intimidating as hell  even to the engineer who is working on it.  By forcing them to explain themselves, in a way that you can understand, you are inherently forcing them to work thru the issue at hand in their own head.  And, very often, you force them to realize where they went wrong.  Let me illustrate this with an anecdote.
  
Once upon a time there was a software company that was making CD-ROM indexing software and an indexing operation just would not finish.  Finally I dragged the engineer in question over and made him watch how astonishingly, mind numbingly slow it was.  The very next day a 24 + hour process executed in 8 minutes.  What changed?  To this day I do not know.  All he said was “I was stupid[1]”.

This second example tapped into the engineer’s belief that his code was “fast”.  By confronting him with the real world where his code was not fast, I forced him to either admit that his code was slow (which conflicted with his pride) or just fix it or, worse, explain himself.  And the normal thing happened — he fixed his code.  And, just so you know, the engineer was not me.  I was the lowly user in this case.

[1] This actually operated on several emotional vectors — the desire to not have to explain himself coupled with pride in performance (every engineer thinks his or her code is fast).</description>
        <pubDate>Mon, 26 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/mtu/2019/08/26/the-power-of-why.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mtu/2019/08/26/the-power-of-why.html</guid>
        
        <category>software_engineering</category>
        
        <category>chapter_02</category>
        
        
        <category>mtu</category>
        
      </item>
    
      <item>
        <title>Jumpstart and GoRails Has Outstanding Tech Support - Go Chris Oliver Go - Recommended</title>
        <description>Technical support gets a bad rap in the industry but there are time when technical support proves **just how good a product or service can be** -- and this is one of those times.  

I'm very much a Rails guy and I've been debating trying [Jumpstart Pro](https://jumpstartrails.com/) for some time.  I finally have a crappy little SAAS idea and this is likely the perfect time for me to try it -- because I have written **zero** code so far.  Now JumpStart is a paid product that amounts to a pre-build starting Rails app with all the necessary cruft for a SAAS app already built.  So it really only works when you are in a pure greenfield state.  So this is perfect for JumpStart Pro.  

But since it is a paid product, I wanted to try before I buy which made me try the [free version on Git](https://github.com/excid3/jumpstart).  And that, well, that was a disaster.  I ended up with all kinds of issues, mostly related to railties and Rails 6.  If you've been around a while, you've seen this and, well, you know it sucks.  So after an hour of mucking about, I finally threw my hands up and said &quot;I'll just email Chris&quot;.  Now I am a paid subscriber to Chris' Rails Cast service, [GoRails](https://gorails.com/), which, I will admit, does give me an advantage in talking to him.  I'm not an active user of the service but I do pay for it monthly.

So I sent him an email around 3 in the morning on a Friday night and I had an answer back by 10:50 am on a Saturday:

    Hey Scott,

    The free template has no shared code whatsoever with the Pro template. The Pro 
    template is actually a pre-built Rails 6 app you clone because as you can see 
    here the Rails generators are an absolute mess to maintain.

    We built a bunch of classes with Tailwind to make it much more familiar to any 
    Bootstrap users. It's pretty freeing once you get used to it.

    It looks like it installed Devise 1.5.4 on your first error log. Devise is currently 
    4.6.2 lol. That I'm sure is part of the issue. They updated the git to support 
    Rails 6 a couple months ago, but haven't cut a release for it yet.

    All stems from a stupid nuance in gem versioning. Devise is set to &lt; 6.0 
    and technically 6.0.0.rc2 is less than 6.0 final, so the current gem version 
    worked, but not with the final release.

    Just updated the template to use Devise from git for now. Works like a charm.
    
    Chris
    
Not only did he understand the issue but he perfectly explained it and fixed it.  And in just a few hours on a Saturday.  This is extraordinary support and, to me, it illustrates just how good JumpStart Pro likely is (and how good Go Rails is).  People don't take this much care with products that aren't good.  

So if you are in the startup business and you look at support as a cost center, you are flat out wrong at least some of the time.  In this case support was an absolute win and will get another copy of JumpStart Pro bought (I haven't done it yet because I'm headed out the door shortly; only reason).

Chris Oliver, Go Rails and Jumpstart Pro -- recommended.</description>
        <pubDate>Sat, 17 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/08/17/jumpstart-and-gorails-has-outstanding-tech-support-go-chris-oliver-go-recommended.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/08/17/jumpstart-and-gorails-has-outstanding-tech-support-go-chris-oliver-go-recommended.html</guid>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Installing Rails 2.6 on OSX and a Creating New Application</title>
        <description>So Rails 2.6 has been released and I just went thru the normal update dance.  Here's how I handled upgrading my system.  Let's start with the basics:

* rvm
* OSX Mojave
* Ruby 2.5.1
* MySQL (yeah, yeah I know; not apologizing)

Here is what I did (you should know that I hit issues with older versions of Yarn and Node so I've included updating them as part of this):

1.  Started a terminal session.
2.  Verified my ruby version:

    ruby --version

3. Verified my rails version:

    rails --version
    
4.  Upgrade Yarn:

    curl -o- -L https://yarnpkg.com/install.sh | bash
    
5.  You can either start a new terminal to get your path exports correct (it is set on Terminal start) or simply do this:

    export PATH=&quot;$HOME/.yarn/bin:$HOME/.config/yarn/global/node_modules/.bin:$PATH&quot;

6.  Upgrade node:

    brew upgrade node
    
7.  Upgrade Ruby to 2.6.2 (not required but I figured why not):

    rvm install 2.6.2
    
8. Change to the new ruby:

    rvm use ruby-2.6.2
    
9.  Generate a new application:

    rails new lauck --database=mysql
    
10. If you got anxious and generated a new application prior to installing a new ruby then, you would need to update the Gemfile to reflect the Ruby version and bundle install.

**Note**: I've named this new test application after one of my favorite librarians.  

# References:

* [Rails 6 Announcement](https://weblog.rubyonrails.org/2019/8/15/Rails-6-0-final-release/)
* [Rails 6 Release Notes](https://edgeguides.rubyonrails.org/6_0_release_notes.html)
* [Rails 6 on Hacker News](https://news.ycombinator.com/item?id=20717886)
* [Installing Yarn](https://yarnpkg.com/lang/en/docs/install/#mac-stable)
* [RVM Basics](https://rvm.io/rvm/basics)
* [Installing Ruby 2.6](https://dev.to/grv19/installing-ruby-26-2dch)
* [How to Upgrade to Rails 6](https://selleo.com/blog/how-to-upgrade-to-rails-6)





</description>
        <pubDate>Sat, 17 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/08/17/installing-rails-2-6-on-osx-and-a-creating-new-application.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/08/17/installing-rails-2-6-on-osx-and-a-creating-new-application.html</guid>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Rails Scopes are Elegant</title>
        <description>This post is obvious to any advanced Rails user (hell even any middling Rails user) but I know there are some readers who aren't that sophisticated so here goes. 

A scope is a Rails database query that operates much like a class method only it is much, much simpler to define.  Here's a class method that finds the active elements in a table called habits:

    def active
      Habit.where(active: true)
    end
    
Now that's not bad but a Rails scope looks like this:

    scope :active, -&gt; { where(active: true) }
    
But the real benefit here is that scopes are **chainable** so they can be inserted into an overall &quot;query pipeline&quot; where you can easily extend things.  In my case I had a site listing habits and I realized that habits needed a state of active (if it was true it should be displayed and if it was false then it shouldn't be).  Here was my initial view code:

    &lt;%plan.habits.each do |habit|%&gt;
      &lt;li&gt;&lt;%=link_to(habit.name, habit_path(habit))%&gt;&lt;/li&gt;
    &lt;% end %&gt;
    
Once I had the scope defined then all I had to do was this:

    &lt;%plan.habits.active.each do |habit|%&gt;
      &lt;li&gt;&lt;%=link_to(habit.name, habit_path(habit))%&gt;&lt;/li&gt;
    &lt;% end %&gt;
    
And **blammo** I had an active list of habits.  

Now let's carry the example further and let's say that you want a list of habits sorted by name. You can define a scope called ordered_by_name like this:

    scope :order_by_name, -&gt; { order(&quot;name ASC&quot;)}

and the scope into your view like this:

    &lt;%plan.habits.active.order_by_name.each do |habit|%&gt;
      &lt;li&gt;&lt;%=link_to(habit.name, habit_path(habit))%&gt;&lt;/li&gt;
    &lt;% end %&gt;    

And it will be sorted by name, easy peasy.  Now I've used scopes for **years** (prior to their current syntax actually) but their elegance has never struck me in quite the same way.  Perhaps it is just Monday ...

## More on Rails Scopes:
* [api.rubyonrails.org](https://api.rubyonrails.org/classes/ActiveRecord/Scoping/Named/ClassMethods.html)
* [Scopes or Class Methods](https://www.justinweiss.com/articles/should-you-use-scopes-or-class-methods/)
* [Named Scopes](https://medium.com/le-wagon/what-are-named-scopes-and-how-to-use-them-rails-5-5a0444d8b759)</description>
        <pubDate>Mon, 05 Aug 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/08/05/rails-scopes-are-elegant.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/08/05/rails-scopes-are-elegant.html</guid>
        
        <category>rails</category>
        
        <category>database</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Rails Migration Tips and Tricks</title>
        <description>A migration is the Rails facility which alters a database's structure.  Migrations are an essential aspect of all database development with Rails and they generally work quite well with the default command: 

    bundle exec rake db:migrate

-or-

    bundle exec rails db:migrate

That command will cause Rails to execute all pending migrations.  A pending migration is one where the timestamp on the migration is **not** stored in the table schema_migrations.  

The trick with migrations is understanding how to manipulate them, replay them, etc when they fail.  

## The Hard Way - The schema_migrations Table

Let's start with the hard way - manipulating schema_migrations table directly.  Let's say that you need to re-run the last 2 migrations.  Here's what you need to do:

Start by getting the timestamps of the last two migrations.  You can do this with an ls -ltr db/migrations and pick them out of what might be a giant list of migrations (my current work project has 858 migrations) -- and this can, well, **suck**.  You can also be smart about it and use tail to get just the last few (default 8 on OSX) migrations.

    ls -ltr db/migrate | tail
    -rw-r--r--  1 sjohnson  staff   201 Jul 18 04:44 20190717234458_create_units.rb
    -rw-r--r--  1 sjohnson  staff   120 Jul 18 04:44 20190717235922_add_unit_id_to_habits.rb
    -rw-r--r--@ 1 sjohnson  staff   154 Jul 18 04:44 20190718000145_add_unit_id_to_habit_tasks.rb
    -rw-r--r--@ 1 sjohnson  staff   129 Jul 18 04:44 20190718075945_add_unit_preferences_json_to_users.rb
    -rw-r--r--@ 1 sjohnson  staff   154 Jul 18 04:44 20190718080511_add_unit_type_to_units.rb
    -rw-r--r--@ 1 sjohnson  staff   131 Jul 18 09:28 20190718110444_fix_stupidity_with_float_val.rb
    -rw-r--r--  1 sjohnson  staff   164 Jul 18 13:33 20190718133051_add_options_to_habits.rb
    -rw-r--r--@ 1 sjohnson  staff   151 Jul 25 15:48 20190725100535_add_has_loggable_tasks_to_habits.rb
    -rw-r--r--@ 1 sjohnson  staff   257 Jul 26 15:21 20190726191446_add_plan_id_to_habits.rb
    -rw-r--r--@ 1 sjohnson  staff   523 Jul 27 03:29 20190726133807_create_plans.rb

Now you can delete from schema_migrations using a database console or by using ActiveRecord in the Rails console so either:

    delete from schema_migrations where version in (20190726133807, 20190726191446);
    
-or- in a Rails console:

    ActiveRecord::Base.connection.execute(&quot;delete from schema_migrations where version in (20190726133807, 20190726191446)&quot;)

Now you would also need to undo any changes your migrations might have partially implemented.  If this was table creation then this is a relatively simple &quot;drop table foo&quot; statement but if it was an index creation or something harder, you need to selectively alter individual tables.  And even a hard core SQL guy like myself generally doesn't want to do that.  So let's look at the easier options.

## The Easy Way

Happily Rails provides some additional facilities for this allowing you to rollback the last migration or an individual migration.

    rake db:rollback

gets rid of the last migration.  I do NOT, however, ever recommend that you do this.  I'm currently working on an active side project where pull requests are flowing and even when you might think that you know what the last migration **you created** was, you may not realize that another developer slipped a migration in and *whammo*, &quot;Houston we have a problem&quot;.

My recommendation is to always specify the version with:

    rake db:migrate:down VERSION=20190726133807

This is absolute and will only affect the migration in question.  Given that I'm a big damn fan of always understanding the state of my persistent storage, it isn't surprising that I recommend this.

## See Also

All of this can be easily referenced:

* [Stack Overflow](https://stackoverflow.com/questions/4352848/how-to-rollback-just-one-step-using-rake-dbmigrate/21119193)
* [Core Rails Docs on Migrations](http://guides.rubyonrails.org/migrations.html#running-migrations)</description>
        <pubDate>Wed, 31 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/31/rails-migration-tips-and-tricks.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/31/rails-migration-tips-and-tricks.html</guid>
        
        <category>rails</category>
        
        <category>migration</category>
        
        <category>db</category>
        
        <category>database</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Nerd Humor - When You Blog in Bright Sunlight in a Convertible</title>
        <description>Disclaimer: There is likely only one person on the planet who will find this funny.  Hi!

So I found myself last night in a convertible with my wife driving and I thought to myself, &quot;I know; I'll write something&quot;.  And I began writing a screed on the use of the underlying Rails Migration commands that manipulate the state of your database.  Rails has a number of these commands that, after MORE THAN A DECADE of daily rails programming, I've never bothered to use.  I have known for a long time I need to master these and I finally have.  

I finish up that post this morning and go to find it and publish it and a git status shows me **nothing**.  Gulp.  This just doesn't happen to me.  Scramble around looking for it -- no love.  Confirm that it actually exists in my editor and I didn't just have a surreal &quot;out of keyboard&quot; experience -- yep.  It is there.  And then it hits me -- I have multiple blogs, one for nerd writing and one for recipes.  Yep:

    ➜  recipes git:(gh-pages) ✗ git commit -m &quot;updates to pickle&quot;
    [gh-pages 0e34bce] updates to pickle
     1 file changed, 3 insertions(+), 1 deletion(-)
    ➜  recipes git:(gh-pages) git push origin gh-pages
    Counting objects: 4, done.
    Delta compression using up to 4 threads.
    Compressing objects: 100% (4/4), done.
    Writing objects: 100% (4/4), 568 bytes | 0 bytes/s, done.
    Total 4 (delta 3), reused 0 (delta 0)
    remote: Resolving deltas: 100% (3/3), completed with 3 local objects.
    remote: 
    remote: GitHub found 7 vulnerabilities on fuzzygroup/recipes's default branch (1 critical, 1 high, 4 moderate, 1 low). To find out more, visit:
    remote:      https://github.com/fuzzygroup/recipes/network/alerts
    remote: 
    To github.com:fuzzygroup/recipes
       9fc96ec..0e34bce  gh-pages -&gt; gh-pages
    ➜  recipes git:(gh-pages) jekyll post &quot;Rails Migration Tips and Tricks&quot;
    New post created at _posts/2019-07-30-rails-migration-tips-and-tricks.md.
    ➜  recipes git:(gh-pages) ✗ mate _posts/2019-07-30-rails-migration-tips-and-tricks.md 
    ➜  recipes git:(gh-pages) ✗   

Moral of the story?  Don't tech blog where you food blog.  Sigh.
</description>
        <pubDate>Wed, 31 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/humor/2019/07/31/nerd-humor-when-you-blog-in-bright-sunlight-in-a-convertible.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/humor/2019/07/31/nerd-humor-when-you-blog-in-bright-sunlight-in-a-convertible.html</guid>
        
        <category>humor</category>
        
        <category>blogging</category>
        
        
        <category>humor</category>
        
      </item>
    
      <item>
        <title>Patterns and Anti-Patterns</title>
        <description>There times when I use terms so common to software engineering that I forget that there is a wide audience out there to whom these aren't native concepts.  That's my bad -- using things before you define them is not a good thing.  

Earlier today I referenced &quot;[anti-patterns](https://fuzzyblog.io/blog/anti_patterns/2019/07/26/development-anti-pattern-two-objects-with-almost-the-same-structure.html)&quot; but failed to define it.  The idea of an anti-pattern is intimately tied to the idea of a pattern and let's let Wikipedia do the talking:

&quot;In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. It is not a finished design that can be transformed directly into source or machine code. It is a description or template for how to solve a problem that can be used in many different situations. Design patterns are formalized best practices that the programmer can use to solve common problems when designing an application or system.&quot; [Software Design Pattern](https://en.wikipedia.org/wiki/Software_design_pattern)

So if a pattern is a best practice, something that you do then an anti-pattern is the inverse -- it is something that you **do not do**.  I'll blog about another one soon -- the going dark anti pattern.</description>
        <pubDate>Fri, 26 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/anti_patterns/2019/07/26/patterns-and-anti-patterns.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/anti_patterns/2019/07/26/patterns-and-anti-patterns.html</guid>
        
        <category>software_development</category>
        
        <category>anti_patterns</category>
        
        
        <category>anti_patterns</category>
        
      </item>
    
      <item>
        <title>Development Anti Pattern - Two Objects with Almost the Same Structure</title>
        <description>One of the beautiful things about greenfield development, that joyous process of starting anew on something, is that it lets you see errors that you make much more cleanly.  And this is a combination of a few things:

1.  The Forest for the Trees.  In a big code base, any errors you make aren't that visible because they are drowned out by all the other bits of code.
2. Willingness to Get It Right.  When you start a new code base, it is like starting a new love affair -- this time, you are going to get it **right**.  This time, it will be **different**.  So you are a bit more able to see errors then a lot more willing to rip apart something that is already working to make it right.

The specific example at hand is the anti pattern of two objects with almost the same structure.  Here's an example from my current side project:

    desc habits;
    +-----------------+--------------+------+-----+---------+----------------+
    | Field           | Type         | Null | Key | Default | Extra          |
    +-----------------+--------------+------+-----+---------+----------------+
    | id              | bigint(20)   | NO   | PRI | NULL    | auto_increment |
    | created_at      | datetime     | NO   |     | NULL    |                |
    | updated_at      | datetime     | NO   |     | NULL    |                |
    | name            | varchar(255) | YES  | MUL | NULL    |                |
    | hardness        | int(11)      | YES  |     | NULL    |                |
    | frequency       | int(11)      | YES  |     | NULL    |                |
    | economic_value  | int(11)      | YES  |     | NULL    |                |
    | user_id         | int(11)      | YES  | MUL | NULL    |                |
    | description     | text         | YES  |     | NULL    |                |
    | shareable       | tinyint(1)   | YES  |     | NULL    |                |
    | habit_type_id   | int(11)      | YES  | MUL | NULL    |                |
    | master_habit_id | int(11)      | YES  | MUL | NULL    |                |
    | unit_id         | int(11)      | YES  |     | NULL    |                |
    | options         | text         | YES  |     | NULL    |                |
    | has_tasks       | tinyint(1)   | YES  |     | 0       |                |
    +-----------------+--------------+------+-----+---------+----------------+
    15 rows in set (0.00 sec)

    desc habit_tasks;
    +----------------------------+--------------+------+-----+---------+----------------+
    | Field                      | Type         | Null | Key | Default | Extra          |
    +----------------------------+--------------+------+-----+---------+----------------+
    | id                         | bigint(20)   | NO   | PRI | NULL    | auto_increment |
    | created_at                 | datetime     | NO   |     | NULL    |                |
    | updated_at                 | datetime     | NO   |     | NULL    |                |
    | habit_id                   | int(11)      | YES  | MUL | NULL    |                |
    | name                       | varchar(255) | YES  |     | NULL    |                |
    | hardness                   | int(11)      | YES  |     | NULL    |                |
    | frequency                  | int(11)      | YES  |     | NULL    |                |
    | economic_value             | float        | YES  |     | NULL    |                |
    | user_id                    | int(11)      | YES  |     | NULL    |                |
    | metric_type_id             | int(11)      | YES  |     | NULL    |                |
    | unit_id                    | int(11)      | YES  |     | NULL    |                |
    | best_general_time_for_this | varchar(255) | YES  |     | NULL    |                |
    | options                    | text         | YES  |     | NULL    |                |
    +----------------------------+--------------+------+-----+---------+----------------+
    13 rows in set (0.06 sec)
    
These are pretty close to the same and they represent an attempt to give a 1 level hierarchy (the habit_task) onto the parent object (habit).  

And the simple answer here is that what I'm actually looking for are groups not a sub-object i.e. invent a group concept an then simply reduce all the habit_task objects to habits that are grouped together.  That achieves roughly 95% of what habit_tasks did and dramatically:

* reduces code duplication (the same routine to calculate economic_value is used multiple places)
* reduces complexity overall
* gets rid of a huge wart in the display views where nesting is needed
* improves performance by not having to constantly check if a habit has tasks

The bottom line here is that whenever you have two objects with almost the same structure, start asking yourself is that really the same damn object somehow.  </description>
        <pubDate>Fri, 26 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/anti_patterns/2019/07/26/development-anti-pattern-two-objects-with-almost-the-same-structure.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/anti_patterns/2019/07/26/development-anti-pattern-two-objects-with-almost-the-same-structure.html</guid>
        
        <category>software_development</category>
        
        <category>anti_patterns</category>
        
        
        <category>anti_patterns</category>
        
      </item>
    
      <item>
        <title>A Bootstrap 4 Two Column Example that Just Works</title>
        <description>I remain, alas, CSS Challenged.  I'm from an era where the height of HTML layout was tables and once upon a time I know all the tricks including images in the corners for that rounded corner effect.  But, as is all too often in technology, the tricks that once served you well -- don't.  

Now we exist in a whole new world for layout i.e. CSS and for me that means [bootstrap introduction](https://getbootstrap.com/docs/4.0/getting-started/introduction/) or [bootstrap main page](https://getbootstrap.com/).  Bootstrap isn't perfect but I fully respect what it tries to do and the simple fact that, imho, Bootstrap made the Internet **better**.  So when I want to make two columns and I naturally reach for a table tag, I need to come look this example up from [CodePen](https://codepen.io/SitePoint/pen/WMYOxb)

    &lt;div class=&quot;container&quot;&gt;
      &lt;div class=&quot;row header&quot;&gt;
        &lt;div class=&quot;col-xs-12 text-center&quot;&gt;
          &lt;h1&gt;Bootstrap 4 2-Column Layout &lt;/h1&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-sm-6 first-column&quot;&gt;
          &lt;p&gt;This column takes the full height as its sibling column thanks to Flexbox&lt;/p&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-sm-6 second-column&quot;&gt;
          &lt;p&gt;
            Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum Lorem ipsum 
          &lt;/p&gt;
        &lt;/div&gt; 
      &lt;/div&gt;
      &lt;div class=&quot;row&quot;&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    
And I'm not pretending that this is hard or not but the shift from tables to a css grid isn't exactly simple (or at least it isn't for me).  

I'm noting this particular example because I've tried several and this one just worked right out of the box.  The others should have -- and maybe it was just my ham fisted approach -- but this one did.  Recommended. </description>
        <pubDate>Fri, 26 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/bootstrap/2019/07/26/a-bootstrap-4-two-column-example-that-just-works.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/bootstrap/2019/07/26/a-bootstrap-4-two-column-example-that-just-works.html</guid>
        
        <category>boostrap</category>
        
        <category>css</category>
        
        
        <category>bootstrap</category>
        
      </item>
    
      <item>
        <title>Git Rebasing Old Migrations Easily When SourceTree Fails</title>
        <description>So I've written before about schema.rb changes, SourceTree and migrations and the problem with git rebase before:

* [Schema WTF Take 1](https://fuzzyblog.io/blog/rails/2019/05/09/rails-migrations-multiple-developers-and-the-schema-wtf-moment.html)
* [Schema WTF Take 2](https://fuzzyblog.io/blog/rails/2019/05/10/the-schema-wtf-moment-take-2-an-excursion-into-sourcetree.html)

I just hit this again and my normal pairing partner is offline and while he has the patience to deal with the crazy ass error messages from SourceTree, I do not.  Personally I think Atlassian (the maker of SourceTree) should simply be killed with fire while my pagan friends dance naked around a drum circle but hey -- that's just me.  

Anyway my tooling has let me down.  And, yes I could learn the specific range level commit commands that SourceTree is generating behind the scenes but I think there's an easier way.  Here's what I did:

1. Abort the git rebase with git rebase --abort
2. Change back into my develop branch which is now current.
3. Copy schema.rb to ~ i.e. cp db/schema.rb ~
4. Look at my pull request to identify the specific migration that I ran.
5. Roll back the specific migration with: rake db:migrate:down VERSION=20190610143443
6. Copy in the schema.rb from ~ i.e. cp ~/schema.rb db
7. Run the migration with bundle exec rake db:migrate
8. Do the git add / commit / push dance
9. Get someone to do the pull request review

And that, dear reader, should have worked swimmingly.  Alas, it did not.  And this was one of those software engineering cases where trying to understand something fully likely would have been a waste of time.  This change amounted to a few lines in a model, a migration to add an index and a small update to a spec file -- I simply recreated the branch from the current develop as &quot;-a&quot; (on top of the original branch number) and then re-made the same damn changes.  Yeah that sucks but it worked.  

Today was definitely a day in the trenches.  Sigh.</description>
        <pubDate>Wed, 24 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/24/git-rebasing-old-migrations-easily-when-sourcetree-fails.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/24/git-rebasing-old-migrations-easily-when-sourcetree-fails.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>migrations</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Bullshite rsync Subtleties</title>
        <description>So yesterday or the day before [I blogged about rsync](https://fuzzyblog.io/blog/osx/2019/07/22/backing-mac-to-mac-via-scp-rsync.html) and how good it was.  And that's true but rsync is notoriously tricky and today I became very, very aware of rsync's trickiness.

So my scenario was that I had an old MacBook Air which I was trying to get all the data off of since DropBox seemed to have an incomplete sync.  And I figured that if I just got all the files copied up to my big desktop then I could reformat the machine and give it to my wife as a replacement for her laptop on which the keyboard died.

Note: Some of these things are perhaps known issues / not issues because I'm on OSX.  What I can say is that as a Mac user some of these things make no damn sense.

# Problem the First - Directory Names with Spaces

So here was the first thing that I observed:

    rsync -avh -e ssh Dropbox/&quot;Camera Uploads&quot;/ 192.168.1.25:/Users/sjohnson/backups/macbook_air/Dropbox/&quot;Camera Uploads&quot;
    Password:
    building file list ... done
    created directory /Users/sjohnson/backups/macbook_air/Dropbox/Camera
    ./
    .DS_Store
    .dropbox
    2017-07-03 05.30.42.jpg
    2017-07-03 07.03.03.jpg
    2017-07-09 08.54.25.jpg
    2017-07-09 08.54.31.jpg
    2017-07-09 11.09.16.jpg
    2017-07-09 11.09.23.jpg
    2017-07-10 18.48.37.jpg
    2017-07-10 18.48.43.jpg
    2017-07-10 18.48.47.mov
    ^Crsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at /BuildRoot/Library/Caches/com.apple.xbs/Sources/rsync/rsync-52.200.1/rsync/rsync.c(244) [sender=2.6.9]'

So what you're seeing is that even tho the data came from &quot;Camera Uploads&quot;, it got moved to &quot;Camera&quot;.  The issue is obviously directory names with spaces.  And since I had started with an scp approach this meant that rsync was copying up data that it already had in place.  ARRGHH!

And here's a second pass with a different approach to space encoded filenames:

    rsync -avh -e ssh Dropbox/&quot;Camera Uploads&quot;/ 192.168.1.25:/Users/sjohnson/backups/macbook_air/Dropbox/Camera\ Uploads/
    Password:
    building file list ... done
    ./
    2017-07-14 18.09.19.jpg
    2017-07-14 18.09.37.jpg
    2017-07-14 18.09.39.jpg
    2017-07-14 18.28.18.jpg
    [A bunch of filenames omitted for brevity]
    2017-07-30 09.27.13.jpg
    2017-07-30 09.27.23.jpg
    ^Crsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at /BuildRoot/Library/Caches/com.apple.xbs/Sources/rsync/rsync-52.200.1/rsync/rsync.c(244) [sender=2.6.9] 

Before the second pass:

     ls -ltr Camera | wc -l
          15

After the second pass

    ScottiMac:Dropbox sjohnson$ ls -ltr Camera | wc -l
          36

So even after changing how I handled the destination filename, it still went to &quot;Camera&quot;.  Double ARGH.

So what's the obvious workaround when rsync seems to entirely fsck up OSX filenames with spaces?  I'm sure there are other options but my approach was drop dead simple, rename each folder on source and destination from:

    Camera Uploads

to

    Camera_Uploads

and re-run the sync command:

    rsync -avh -e ssh Dropbox/Camera_Uploads/ 192.168.1.25:/Users/sjohnson/backups/macbook_air/Dropbox/Camera_Uploads/
    Password:
    building file list ... done
    .DS_Store

    sent 18.87K bytes  received 1.64K bytes  8.21K bytes/sec
    total size is 4.57G  speedup is 222553.34

# Problem the Second - Excluded Directories

My next issue was that I wanted to exclude the Library directory from the files being synced.  This would eliminate all the giant Docker files that are stored in Library (and Library mostly isn't needed since it is Application level stuff; not really user data).

Here was my first attempt:

    ➜ rsync -avh --exclude '/Users/sjohnson/Library/' -e ssh ~ 192.168.1.25:/Users/sjohnson/backups/seas
    Password:
    building file list ... rsync: opendir &quot;/Users/sjohnson/Library/Application Support/CallHistoryTransactions&quot; failed: Operation not permitted (1)

So that clearly failed.  Now I've long been aware that Apple has old versions of core open source tools like rsync, so:

    ➜ rsync --version
    rsync  version 2.6.9  protocol version 29
    Copyright (C) 1996-2006 by Andrew Tridgell, Wayne Davison, and others.
    &lt;http://rsync.samba.org/&gt;
    Capabilities: 64-bit files, socketpairs, hard links, symlinks, batchfiles,
                  inplace, IPv6, 64-bit system inums, 64-bit internal inums

    rsync comes with ABSOLUTELY NO WARRANTY.  This is free software, and you
    are welcome to redistribute it under certain conditions.  See the GNU
    General Public Licence for details.

Clearly the 2006 copyright date is a problem.  This calls for brew!

    brew install rsync

And, naturally, brew does [shell magic trickery](https://fuzzyblog.io/blog/ruby/2019/07/10/when-rbenv-well-won-t-rbenv-fixing-shell-extension-madness.html) that messed up for me.  Happily tho rsync is just an executable that I can fully path to (all brew executables always end up in /usr/local/bin):

    /usr/local/bin/rsync --version
    rsync  version 3.1.3  protocol version 31
    Copyright (C) 1996-2018 by Andrew Tridgell, Wayne Davison, and others.
    Web site: http://rsync.samba.org/
    Capabilities:
        64-bit files, 64-bit inums, 64-bit timestamps, 64-bit long ints,
        socketpairs, hardlinks, symlinks, IPv6, batchfiles, inplace,
        append, ACLs, xattrs, iconv, symtimes, no prealloc, file-flags

    rsync comes with ABSOLUTELY NO WARRANTY.  This is free software, and you
    are welcome to redistribute it under certain conditions.  See the GNU
    General Public Licence for details.

and that still failed but here was the magic (and Virginia, yes, the error was **all damn mine**, not the version of rsync):

&quot;Excluding a specific directory is same as excluding a file, just pass the relative path to the directory to the --exclude option&quot; from [Rsync Exclusion](https://linuxize.com/post/how-to-exclude-files-and-directories-with-rsync/)

So that made my rsync command this:

    rsync -avh --exclude 'Library/' -e ssh ~ 192.168.1.25:/Users/sjohnson/backups/seas

And that worked like a charm.  Sigh.  Always read the docs well if at all possible.  </description>
        <pubDate>Wed, 24 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2019/07/24/bullshite-rsync-subtleties.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2019/07/24/bullshite-rsync-subtleties.html</guid>
        
        <category>osx</category>
        
        <category>rsync</category>
        
        <category>mac</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Getting Around the Makara ActiveRecord Proxy</title>
        <description>One of the key approaches to database scalability is the use of a replica server so that read only queries (i.e. SELECT queries) get shunted away from the master database.  When you use a framework based approach to development such as Rails, this is generally implemented as an ActiveRecord connector that sits between your SQL and the server on which it gets executed.  One such example is [Makara](https://github.com/taskrabbit/makara) from TaskRabbit.

The problem here is that not all SELECT queries are created equally and not all select queries can run correctly on a replica.  I recently had the situation where a given query was complex enough (think 500+ lines of select query) that it could only execute on the replica server maybe 3 times out of 10 with a 45 plus second runtime.  I was assured by the query author that this would execute correctly on the production database.  And while I was absolutely dubious about this, I realized that there was literally no way to prove this without getting around Makara.  

I looked at Makara and like a lot of gems, I didn't find the documentation much to my liking (although I had a hint that if I got funky with it, it might work) but then I saw this little snippet:

&quot;Calls inside a transaction will always be sent to the master (otherwise changes from within the transaction could not be read back on most transaction isolation levels)&quot;

And that gave me this example (not the real sql; just something I had open in a db console):

    results = ActiveRecord::Base.transaction do 
      ActiveRecord::Base.connection.select_rows
      (&quot;
      select user_id, date_created_at, sum(int_val) from metrics where habit_id = 2 
      group by date_created_at order by date_created_at desc LIMIT 10;
      &quot;) 
    end
    
And, sure enough, that worked.  </description>
        <pubDate>Tue, 23 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/23/getting-around-the-makara-activerecord-proxy.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/23/getting-around-the-makara-activerecord-proxy.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>So You Need to Write</title>
        <description>I'm a long time blogger and from time to time I get people asking me how I produce as many words as I do.  I know a few people looking to start writing projects and I thought I'd put together a few notes on the topic of how I'm productive.

# My Guidelines for Writing Productively

Here is the quick version of my writing guidelines:

1. Break the tyranny of the file
2. Write in smaller chunks - never chapters
3. Write don't format
4. Look into different writing tools
5. Realize that no one thing you write matters all that much
6. Measure your progress and improve on it

## Break the Tyranny of the File

I haven't primarily written using a classical word processor software now since 1987 and I attribute most of my productivity to this one fact, specifically I think that I haven't had to think about files in almost 30 years.  I refer to this as &quot;breaking the tyranny of the file&quot;.  As stupid as it may sound, the need to decide how to name a file, where to locate a file, etc are all cognitive decisions that pull mental energy away from the actual writing process.  And lest you think that in 2019, files are simpler than ever, consider this dialog box from trying to close an unsaved document in Pages:

[View Pic](https://i.imgur.com/a/CYHtw37.png)

I'm sorry but this doesn't make much sense even to me (the answer here is to click Delete; sigh).

The easiest way for most people to break the tyranny of the file is to use a content management system like a blog where, yes, there are files but their creation and management is hidden.  As an example, here's how I create something in my blogging tool:

    jekyll post &quot;So You Need to Write&quot;

-or- if I want to make a draft that isn't going to go live today:

    jekyll draft &quot;So You Need to Write&quot;

And that one command builds what I need and I can then open it in my editor.  I never have to worry about filename / location / etc.

## Write in Smaller Chunks - Never Chapters

Along with breaking the tyranny of the file comes the idea of working in small chunks - think sections (or posts) not chapters.  The longer the thing that you are &quot;writing&quot; is, the harder it is to hold it all in your head at once.  If you can write a long work it at the section / post level and then composite it together at the end you will find that each section is easier to write.

And if you are a software developer trying to write, consider this analogy.  At one time or another, we've all written functions that were longer than one screen.  Remember how hard it was to keep all of that function in your head.  Well, working at the section level is analogous.

## 3. Write Don't Format

I view formatting as the absolute bane of writing.  Tools like Microsoft Word make formatting so seductive because you feel &quot;productive&quot; but, really, you aren't being productive when you are formatting, you are just &quot;twiddling&quot; with the content.  To me one of the worst advances in modern software for writing productivity is the full integration of formatting tools right into the writing process.  From my perspective, you write and then you format as a final step before output.

Note: Structural formatting like headings, inline emphasis such as bold facing / italics are fine.  I'm more referring to the &quot;let's see what this looks like with a different font / margins / etc&quot; type of twiddling.

## 4. Look Into Different Writing Tools

To paraphrase McLuhan's famous &quot;[The Medium is the Message](https://en.wikipedia.org/wiki/The_medium_is_the_message)&quot;, I would argue that the tool alters the output.  If all you have ever used to write is Microsoft Word, well, there is a whole world out there of writing tools that will dramatically affect how you write.  I cover these in the next section and I would strongly, strongly argue that you at least explore different tools.

Note: If you're addicted to some of the actually excellent tools inside Word such as the grammar checker then you can always do what I do and copy and paste a chunk of writing into Word, let the grammar checker give you changes and then move it back.  I do this regularly. 

## 5. Realize That No One Thing You Write Matters All That Much

When you write a lot, you come to realize that no one thing actually matters all that much.  I tend to view writing as a corpus -- some bits are stronger than others and the value is in the totality.  I no longer fret about if a particular piece is perfect because I have accepted that some will be and some won't be.  

## 6. Measure Your Progress and Improve On It

One of the best changes I've made for being a productive writer in 2019 is that I now benchmark my progress and I use it to motivate myself.  Here's an example

[View Pic](https://imgur.com/a/IJF29FK)

I should note that my numbers here are lower than they would normally be.  A big part of the reason for that is that I wrote the tooling driving the screenshot above (new product coming soon).

# Writing Tools

For the purposes of being transparent, I will tell you what writing tools I use.  But, for the sake of kittens everywhere, DO NOT use what I use.  My tool choices would likely lead to incessant, Tourette's style cursing.  I use idiosyncratic tools because I've been doing this forever and I'm an uber nerd.  These work for me and likely don't work for you.

## My Personal Writing Tools 

I use:

* [TextMate](https://macromates.com/) as an editor
* [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) as a drop dead simple content formatter
* [Jekyll](https://jekyllrb.com/) as a blogging tool 
* [Github](https://github.com/fuzzygroup/blog) for content management (and Jekyll is intimately tied to github)
* [gist.github.com](https://gist.github.com/fuzzygroup) for generating temporary content versions that I can show people a quick draft.  [Nick Janetakis](https://nickjanetakis.com/) taught me this trick.  Thanks Nick!
* [Google Docs](https://docs.google.com) if I need a way for people to edit something but Google Docs WILL NOT re-produce Markdown so it is a suck ass, bullcrap tool. Argh.

To repeat, do not try and use these tools the way I do.  After all, I modeled my writing process after the way I do software development right down to using git ...

### A Note About Github

I thought my comment about using Github / git for managing my writing was my being a typically overly nerdy developer and Mark Bernstein gave me some interesting feedback:

&quot;This might actually be OK advice for writers; I liked the Pragmatic Programmer’s advice, back in the day, to keep all your writing in subversion.  It's possible that Time Machine obviates some of the need for this, but it's a good idea.&quot;

If you are committed to using a plain ASCII format for writing such as MarkDown or HTML then Github / git might actually be something to consider.  Do bear in mind that Git is an industrial strength tool and [not always the easiest thing to use](https://fuzzyblog.io/blog/git/2019/06/27/understanding-a-small-organization-s-git-development-model.html).

## Tools You Should Consider

Here are the writing tools that you should look at:

* [WordPress](https://wordpress.com/)
* [Tinderbox](https://www.eastgate.com/Tinderbox/)
* [Storyspace](https://www.eastgate.com/storyspace/)
* [Scrivener](https://www.literatureandlatte.com/scrivener/overview)

### WordPress

If you are looking to write daily and just build that skill then I strongly recommend that you blog.  I've been blogging now since 2002 and I was one of the authors on the [O'Reilly Essential Blogging book](https://www.amazon.com/Essential-Blogging-Selecting-Doctorow-Paperback/dp/B00ME3RFYA/) (by the way O'Reilly, I've never once gotten a royalty check -- did no copies ever sell?).  Anyway, blogging is a drop dead simple way to write that incorporates a lot of my approaches:

* Break the tyranny of the file 
* Write in smaller chunks 
* Write Don't Format

And if you're going to blog then WordPress is a damn fine way to do it.  And hosted WordPress is both easy and cheap; you don't need your own server.  Recommended.

### Tinderbox / Storyspace

I'm covering these tools together because they both come from the same creative mind and, to me, they are heavily interlinked (despite being different products).  Tinderbox and Storyspace are both more tools for thinking about / planning what you have to write than the writing itself.  Both tools are heavily oriented around graphical views of the structure of your writing.  And Tinderbox is specifically designed for note taking.  

Here is some commentary from the author:

&quot;Storyspace 3 works seamlessly with Tinderbox, with which it shares files. Tinderbox is designed for making, visualizing, and analyzing notes, making it ideal for the early stages of ambitious projects. Storyspace is designed for writing and reading interlinked narrative; many writers will move freely between Tinderbox and Storyspace.&quot;

### Scrivener

Scrivener is the best thought-out tool I've seen for writing long form works (think books / novels) in a damn long time.   With a focus on structural views and working in small chunks, Scrivener addresses my top three concerns (tyranny of the file / smaller chunks / write don't format).  You have to be open to really learning a new tool to use Scrivener, but it is worth it.



### Disclaimers

Matt Mullenweg, the founder of WordPress, is a friend and one of the coolest folks I know online.  But despite that bias, WordPress is still damn awesome.

Tinderbox and Storyspace are both written by Mark Bernstein who is also a friend and has been doing this stuff even longer than I have.  I met Mark back in '87 when we both attended the first ACM Conference on Hypertext at University of North Carolina, Chapel Hill.  And while I have done a lot of different things career wise, Mark has focused exclusively on writing tools and brought a lot of good into the world that way.  I respect him greatly for this.

Mark Bernstein read an early draft of this post and gave me a number of great changes.  Thank you Mark!

All of the techniques above I have used extensively -- for non fiction writing.  I have never tried them in a fiction context.  I suspect that they would work but I can't attest to that.

# In Closing

Someone I used to know once made a wonderful observation on how to be a successful writer.  It was short, succinct and beautiful in its clarity:

“Fingers on keys; ass in chair”</description>
        <pubDate>Mon, 22 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/writing/2019/07/22/so-you-need-to-write.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/writing/2019/07/22/so-you-need-to-write.html</guid>
        
        <category>writing</category>
        
        <category>blogging</category>
        
        <category>heinlein</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>Backing Mac to Mac via scp / rsync</title>
        <description>I found myself turning my old Macbook Air over to my wife to use when the cursed butterfly keyboard on her new MacBook died.  And even tho I had theoretically used Dropbox to sync my data with other machines, a quick check on this machine revealed something like 200 top level directories in ~/Dropbox while a check against my &quot;current&quot; machine revealed only 148 top level directories.

**WHAT THE FERENGI Dropbox !!!!**

And that convinced me that I needed to run a backup on this machine before I turned it over to my wife.  Naturally I found that a Time Machine backup failed and, honestly, I found myself with little desire, inclination or will to fix Time Machine.

This led me to attempt to backup this Mac via an scp command to my big desktop Mac which has plenty of free disc space.  So I used this shell command:

    scp -p -r ~/Downloads/* 192.168.1.25:/Users/sjohnson/backups/macbook_air/Downloads/

as a test command to backup the ~/Downloads directory from one machine to another.  And, annoyingly, I found that after only a few gigabytes, my scp would fail with a networking error.  After trying this again (and again and again), I came to the conclusion that I was fighting an untenable battle that felt a lot like a land war in Asia.  And this led me to Google which, rapidly, led me to rsync, a tool I love but which I never seem to think to use.  Here was the command I finally came up with

    rsync -avh -e ssh Downloads/* 192.168.1.25:/Users/sjohnson/backups/macbook_air/Downloads/

In case you aren't aware, the benefit to [rsync](https://linux.die.net/man/1/rsync) is that it will keep track of your progress to date and resume incomplete copies.  </description>
        <pubDate>Mon, 22 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2019/07/22/backing-mac-to-mac-via-scp-rsync.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2019/07/22/backing-mac-to-mac-via-scp-rsync.html</guid>
        
        <category>osx</category>
        
        <category>backup</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Employee Transitions - Don't Kill Your Organizational Memory</title>
        <description>A lot of companies have moved to organizational chat systems like Slack or the late, unlamented HipChat.  And, from what I have seen, none of those companies have ever dealt with employee transitions in a remotely rational fashion.  Here's what generally happens:

1.  Employee leaves.
2.  Slack / HipChat / whatever chat account is promptly deleted.
3.  I lose all access to ANY and ALL conversations with that previous employee forever.

What.  The.  [Fsck](https://en.wikipedia.org/wiki/Fsck)*?  This makes no actual sense.  Chat has become the organizational memory for what happens in a company.  Now let's move to an actual, specific example.  At my day job we just had a manager depart during a re-org.  These things happen and I really don't know the specifics.  What I do know is that this was my damn manager and he was directly involved in the project I'm on.  If I hadn't, **painstakingly** copied every single slack message he and I ever had, all 20 odd plus pages of them, to a text file, I would have lost much of my knowledge of the project including important details like:

* Who at the client I was supposed to speak with
* The API keys that define connectivity
* The schedule

Now I might have been able to remember the people at the client and I do have a vague memory of the schedule but a high entropy api key like this:

afdjklfdsaj294u324,asdfmafrwae6435%1 (not the real api key; I'm annoyed but not an idiot)

I'll never remember that.  And since I wasn't ready to use it yet, I just left it in my chat client for the time being.  Is this the right place for an API key?   Nope.  But in the real world, chat is often the communications mechanism for delivery of things like API keys.

# In Closing, Email versus Chat

It is always interesting to me that organizations would never delete emails that were sent to me from employees who have left (I just confirmed that I have every email ever from employees who quit).  So why is chat different?

*Yes this is a slightly more polite, much, much more nerdy way of saying &quot;fuck&quot;.  It refers to the fsck utility which checks file system integrity and the fact that when you need to run fsck, you're first word is generally &quot;fuck&quot;.
</description>
        <pubDate>Sat, 20 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2019/07/20/employee-transitions-don-t-kill-your-organizational-memory.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2019/07/20/employee-transitions-don-t-kill-your-organizational-memory.html</guid>
        
        <category>startup</category>
        
        <category>hr</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Using redis-cli on An Encrypted AWS Redis Server</title>
        <description>So I find myself needing to use [redis-cli](https://redis.io/topics/rediscli) to modify the keys on a running Redis instance to deal with a few [Sidekiq issues](https://github.com/mperham/sidekiq/blob/master/Changes.md#413).  And, much to my surprise, I can't simply connect to it and make changes because I keep getting **Error: Connection reset by peer** errors.  A bit of a deep dive taught me this:

* Our redis instance is encrypted both at rest and in transit
* The way to do this is to connect to it via stunnel which builds a secure tunnel
* You have to supply the password on the command line to redis-cli; this surprised me because it leaves the password in the shell history and that's fscking awful for security; grumble, grumble, grumble

Here are some references that I followed:

* [AWS Docs](https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/in-transit-encryption.html)
* [Data Next Docs](https://datanextsolutions.com/blog/how-to-fix-redis-cli-error-connection-reset-by-peer/)

Here are the steps I followed:

1.  I started by installing stunnnel.
2.  I continued by building a mapping for stunnel to the redis server I wanted to mess with.  This required getting the redis server url from our application's settings.
3.  Start the tunnel
4.  Verify that the tunnel is running.
5.  I connected to redis-cli passing the -a password option.
6.  I was able to then verify that redis-cli works correctly by doing a simple set / get:

    set a &quot;hello&quot;
    get a
    
    &quot;hello&quot;
    
And this positioned me for being able to run a redis-cli keys command.  Of course the keys routine I needed to run was error full but that's another story ...</description>
        <pubDate>Thu, 18 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/redis/2019/07/18/using-redis-cli-on-an-encrypted-aws-redis-server.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/redis/2019/07/18/using-redis-cli-on-an-encrypted-aws-redis-server.html</guid>
        
        <category>redis</category>
        
        <category>aws</category>
        
        
        <category>redis</category>
        
      </item>
    
      <item>
        <title>Adding Quick and Dirty JSON Serialization to Database Objects</title>
        <description>So let's say that you're building a system where you need to configure things like measurements and you know that some people like pounds and some people like kilograms.  One option is to hard code a database structure where you have an attribute for each.  And while that works, there are always going to be some more measurement types in the world and this approach leads you to an endless array of sucking database column changes.  

Another approach is to add a text column to your database and store a JSON blob in it.  Rails actually makes this pretty easy with a migration and a declaration:

Migration:

    class AddUnitPreferencesJsonToUsers &lt; ActiveRecord::Migration[5.2]
      def change
        add_column :users, :options, :text
      end
    end

Declaration at the top of your class:

    serialize :options, JSON

This will now add an options column to your table where you can store anything.  And since there is a serialize call, it will even act like an attribute so you can say something like:

    user = User.first
    user.options

and get back:

    {
        &quot;length&quot; =&gt; &quot;inches&quot;,
        &quot;weight&quot; =&gt; &quot;pounds&quot;,
          &quot;time&quot; =&gt; &quot;hours&quot;
    }

And if you really, really like working on a REPL like I do then you can set this globally for all users with this snippet:

    users = User.all
    users.each do |user|
      user.options = options = { &quot;length&quot; =&gt; &quot;inches&quot;, &quot;weight&quot; =&gt; &quot;pounds&quot;, &quot;time&quot; =&gt; &quot;hours&quot; }
      user.save
    end

Easy - no fuss, no muss, no need to write code in an editor for a one time, one off fix.

And just to be clear, you can do a better job with this by really following the instructions.  Here's an [example from CodeBurst](https://codeburst.io/json-serialized-columns-with-rails-a610a410fcdf) which includes custom serializers.  Recommended
</description>
        <pubDate>Thu, 18 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/18/adding-quick-and-dirty-json-serialization-to-database-objects.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/18/adding-quick-and-dirty-json-serialization-to-database-objects.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Never Type Bundle Exec Again</title>
        <description>The command prefix &quot;bundle exec&quot; is something that I've typed probably a thousand, thousand, thousand times.  The bundle exec prefix goes in front of so many damn Rails commands and while I've seen other developers alias it to &quot;be&quot;, that never felt right to me.  And now there's a workaround where I don't even have to type that thanks to the miracle of [Z-Shell](https://en.wikipedia.org/wiki/Z_shell) and [Oh My Zsh](https://ohmyz.sh/).  

Edit your ~/.zshrc and add search for the word plugins and then change that line to this:

    plugins=(git bundler brew gem heroku)

Then you and to do a source on it:

    source ~/.zshrc

And then you can do this magic:

    rails g migration add_frequency_and_economic_value_to_habits
      invoke  active_record
      create    db/migrate/20190717211807_add_frequency_and_economic_value_to_habits.rb
      
And that makes my life better by 13 characters that I'm never going to have to type again.  Honest to the FSM, I'm mildly aroused right now ...

And thank you to [CoderWall](https://coderwall.com/p/weixga/no-more-bundle-exec-with-oh-my-zsh) for publishing this originally.</description>
        <pubDate>Wed, 17 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/17/never-type-bundle-exec-again.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/17/never-type-bundle-exec-again.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Your Daily Remote Worker Management Tip</title>
        <description>Dear Manager of a Remote Employee, 

My apologies in advance that this is bitter and salty like the code brewed coffee that I just swilled in a frazzle but I find myself at the end of my rope. And so I will give you a remote worker management tip.  

When a remote worker, particularly when it is an individual who has worked for you for more than a year, says &quot;we need to work on XYZ in person&quot;, it basically amounts to &quot;Oh my lord, we are so damn screwed&quot;.  AND YOU PAY ATTENTION TO IT AND ARRANGE IT.

Sigh.

Signed, 
Your Remote Worker</description>
        <pubDate>Tue, 16 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/remote_work/2019/07/16/your-daily-remote-worker-management-tip.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/remote_work/2019/07/16/your-daily-remote-worker-management-tip.html</guid>
        
        <category>startup</category>
        
        <category>remote</category>
        
        
        <category>remote_work</category>
        
      </item>
    
      <item>
        <title>Getting Around Spotlight's Madness</title>
        <description>Spotlight is the Apple / OSX search engine and I find it absolutely, blindingly maddening.  The way Spotlight seems to work, when you use it from the OSX search icon, is that it tells you what it found but not **where** it found it.  And I get that filesystems are passe and we are supposed to be in a post file world, blah, blah, blah.  And I call bullshite on that.  I'm a professional software developer and knowing where things actually exist is actually fricking important.  

And here's how to do that with Spotlight -- use mdfind directly.  mdfind is the underlying  command line executable that also executes spotlight searches.  Earlier today I was struggling to remember the name of a Ruby gem and all I knew was that it had &quot;icon&quot; in its name.  So here's what I did:

    mdfind icon -onlyin /Users/sjohnson/Dropbox/fuzzygroup/ | grep Gemfile

And this command looked for the string of characters &quot;icon&quot; but only in the directory where I keep all of my many, many Rails projects.  And then I fed it thru a grep command to find only the references to Gemfile.  And this very quickly led me to [identicon](https://github.com/victorgama/identicon) which is a gem which builds [Stack Overflow like abstract user avatars](https://meta.stackexchange.com/questions/17443/how-is-the-default-user-avatar-generated).


</description>
        <pubDate>Tue, 16 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2019/07/16/getting-around-spotlight-s-madness.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2019/07/16/getting-around-spotlight-s-madness.html</guid>
        
        <category>osx</category>
        
        <category>spotlight</category>
        
        <category>mdfind</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Fixing ByeBug Madness</title>
        <description>I learned an interesting and subtle trick today.  As is all too true lately, my pairing partner, [Sean Kennedy](https://csphere.github.io/), gets the credit for this one.  I had the ruby debugger, byebug, embedded in a view context and I noticed that while this should have brought up a breakpoint, it way, way, way, way overstepped the break point and shot beyond, going into core Rails stuff.  And then Sean told me this trick:

    &lt;%# a = 1; byebug; b = 2; %&gt;
    &lt;%= link_to &quot;Sign in with OAuth 2 provider&quot;, user_doorkeeper_omniauth_authorize_path, class: &quot;btn btn-primary&quot; %&gt;
    
Apparently byebug has a known issue with execution context and wrapping the byebug call inside two assignment statements gives it enough of an execution context for it to stop properly.  

And yes this is a sucky thing but all tools have issues and this one can be trivially worked around. </description>
        <pubDate>Tue, 16 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2019/07/16/fixing-byebug-madness.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2019/07/16/fixing-byebug-madness.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>When RbEnv, well, Won't RbEnv - Fixing Shell Extension Madness</title>
        <description>If you have ever used Ruby's [rbenv](https://github.com/rbenv/rbenv), or [rvm](https://rvm.io/) or [Python's virtual env](https://virtualenv.pypa.io/en/latest/) or any of the different approaches to managing multiple editions of the same language binary on a machine, this post is for you.

All of these tools tend to work the same way -- they inject their own pathing in front of the system path so that the correct version of a binary gets called.  Here's an example from my development system when rbenv wasn't working:

    which ruby
    /usr/bin/ruby
    
    ruby --version
    ruby 2.3.7p456 (2018-03-28 revision 63024) [universal.x86_64-darwin18]
  
And when I made rbenv work, here's what I got:

    which ruby
    /Users/sjohnson/.rbenv/shims/ruby
    
    ruby --version
    ruby 2.6.2p47 (2019-03-13 revision 67232) [x86_64-darwin18]

Technically I should have been able to fix this issue by using this rbenv command:

    rbenv local 2.6.2

And if that didn't work then this should have:

    rbenv rehash

And either of those should have made things work by injecting into my path the correct values.  Sadly, albeit not surprisingly, it did not.  My personal development style seems to have a high entropy value and I suspect that's why but that's a different argument regarding excessive uptime, an abject refusal to reboot my machine, etc.  

Anyway here's the work around:

    cd /to/my/development/directory
    export PATH=/Users/sjohnson/.rbenv/shims:/Users/sjohnson/.rbenv/bin:$PATH

My pairing partner [Sean Kennedy](https;//csphere.github.io/) corrected this to be:

    export PATH=$HOME/.rbenv/shims:$HOME/.rbenv/bin:$PATH

And while he's almost certainly right, I haven't tested his version so both are here.  Pick and choose as you like.

The bottom line here is that shell extensions -- which is what rbenv, rvm and virtualenv actually are -- are fragile.  And if you understand what's going on underneath them, fixing the issue actually is pretty simple.
</description>
        <pubDate>Wed, 10 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2019/07/10/when-rbenv-well-won-t-rbenv-fixing-shell-extension-madness.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2019/07/10/when-rbenv-well-won-t-rbenv-fixing-shell-extension-madness.html</guid>
        
        <category>ruby</category>
        
        <category>rbenv</category>
        
        <category>rails</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>RSpec - Uninitialized Constant Errors on Model Tests</title>
        <description>This one is an easy one but still tripped me up.  I'm working on a project where generators on RSpec stub files are turned off (don't ask; disagreement about the utility of generators between two senior engineers and I lost) and I just got this error:

    bundle exec rspec spec/models/metric_type_spec.rb

    An error occurred while loading ./spec/models/metric_type_spec.rb.
    Failure/Error:
      RSpec.describe MetricType, type: :model do
        describe &quot;#val_col&quot; do
          it &quot;should return int_val for word_count&quot; do
            h = FactoryBot.create(:metric_type_word_count)
            expect(h.val_col).to eq :int_val
          end

          it &quot;should return float_val for weight&quot; do
            h = FactoryBot.create(:metric_type_weight)
            expect(h.val_col).to eq :float_val

    NameError:
      uninitialized constant MetricType
    # ./spec/models/metric_type_spec.rb:1:in `&lt;top (required)&gt;'
    No examples found.

And the error turned out to be that this line was missing:

    require 'rails_helper'

That line is normally inserted by the generator but I built this spec file from scratch and just missed it.  I've stared at that line a thousand, thousand times over the years but I suspect I never realized that what it did was tell RSpec something like this:

&quot;Read the Rails directory structure and autoload all classes in the directories below /app&quot;.  Doh!</description>
        <pubDate>Wed, 10 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/07/10/rspec-uninitialized-constant-errors-on-model-tests.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/07/10/rspec-uninitialized-constant-errors-on-model-tests.html</guid>
        
        <category>rails</category>
        
        <category>rspec</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Bad Ansible Error Message - No handler was ready to authenticate</title>
        <description>Of all the multitude of sins that software engineers wreak upon the world, I have a personal hatred for crappy error messages.  Crappy error messages are an unforgivable sin that condemn all the people that ever get the error message to waste inordinate amounts of time on thinking, tinkering, google and more.  Here is my command:

    python /etc/ansible/inventory/ec2.py --list    

What this is doing is executing the Ansible ec2.py script which talks to AWS and returns a list of resources.  And here is today's favorite crappy error message:

    Traceback (most recent call last):
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 1701, in &lt;module&gt;
        Ec2Inventory()
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 272, in __init__
        self.do_api_calls_update_cache()
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 539, in do_api_calls_update_cache
        self.get_instances_by_region(region)
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 593, in get_instances_by_region
        conn = self.connect(region)
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 557, in connect
        conn = self.connect_to_aws(ec2, region)
      File &quot;/etc/ansible/inventory/ec2.py&quot;, line 582, in connect_to_aws
        conn = module.connect_to_region(region, **connect_args)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/ec2/__init__.py&quot;, line 66, in connect_to_region
        connection_cls=EC2Connection, **kw_params)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/regioninfo.py&quot;, line 220, in connect
        return region.connect(**kw_params)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/regioninfo.py&quot;, line 290, in connect
        return self.connection_cls(region=self, **kw_params)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/ec2/connection.py&quot;, line 103, in __init__
        profile_name=profile_name)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/connection.py&quot;, line 1100, in __init__
        provider=provider)
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/connection.py&quot;, line 569, in __init__
        host, config, self.provider, self._required_auth_capability())
      File &quot;/var/lib/jenkins/.local/lib/python2.7/site-packages/boto/auth.py&quot;, line 1021, in get_auth_handler
        'Check your credentials' % (len(names), str(names)))
    boto.exception.NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV4Handler'] Check your credentials
    jenkins@ip-172-31-19-205:/home/ubuntu$ sudo^C
    jenkins@ip-172-31-19-205:/home/ubuntu$ mkdir ~/.aws
    jenkins@ip-172-31-19-205:/home/ubuntu$ mkdir ~/.aws/credentials
    jenkins@ip-172-31-19-205:/home/ubuntu$ rmdir ~/.aws/credentials
    jenkins@ip-172-31-19-205:/home/ubuntu$ nano ~/.aws/credentials
    jenkins@ip-172-31-19-205:/home/ubuntu$  python /etc/ansible/inventory/ec2.py --list
    {
      &quot;_meta&quot;: {
        &quot;hostvars&quot;: {
          &quot;199.31.18.123&quot;: {
            
The trick here was reading enough of it to find &quot;No handler was ready to authenticate.&quot;  I discovered this while setting up a new [Jenkins](https://jenkins.io/) server and the error was that Ansible's AWS credentials hadn't yet been copied up the server in the directory:

    /home/jenkins/.aws/credentials
    
Once I put them there this problem went away (another problem came up immediately but one down and N thousand to go).</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2019/07/08/bad-ansible-error-message-no-handler-was-ready-to-authenticate.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2019/07/08/bad-ansible-error-message-no-handler-was-ready-to-authenticate.html</guid>
        
        <category>ansible</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Time Bounding</title>
        <description>Time Bounding is one of those brilliant techniques that is so simple that you often don’t even think to use it.  The idea behind time bounding is simple — some things are only worth so much time.  Here’s an example of a ticket that I wrote not 5 minutes ago for one of my engineers:

  Hi, 
  I'd like to put at the bottom of the intranet a list of anyone’s most recent tickets.  Can you learn to use the lighthouse gem to get a list of a given user's tickets?

This should be time bounded to no more than 60 minutes; it is just installing a gem, getting an API token and getting a list of tickets for a given user.

And that’s time bounding.  You’ll also notice that I included a description of why I’m only allocating an hour to it — that this is a pretty simple conceptual task:

1.	Install a component
2.	Get an API token (I.e. Look at a web page)
3.	Figure out how to call the API
4.	Get back an iterable chunk of data

The reason for time bounding is that computing things tend to expand to fill all time available.  Engineers are perfectionists and if you don’t put a limit on things — at their start — you can often find that something you expected to take a few hours is now a few days.

*Takeaway*: The engineer assigned to this knocked it out in about twice the allocated time.  I added a little bit of polish so we ended up about 2.5x the initial 1 hour.  That’s not bad actually.  And the benefit is that everyone in the company can now see, in one place everyone’s tickets.
</description>
        <pubDate>Tue, 02 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/07/02/time-bounding.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/07/02/time-bounding.html</guid>
        
        <category>mtum_chapter4</category>
        
        <category>startup</category>
        
        <category>mtum</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Programming versus Software Engineering or The Parable of Jim and Ed</title>
        <description>Once upon a time there was an American software company on the fast track to going public in an exploding industry.  Let’s call them *D*.  The VP of Engineering was a brilliant engineer named Jim.  Jim is one of those guys who, in a nerd discussion of “what’s your first computer,” always wins because he tells the tale of how he toggled in the boot loader on his KIM-1[1] until his fingers bled.  Jim was, and is, an engineer’s engineer.  All of D’s software was written either in straight C or C + assembly and shipping on DOS and Windows.

Ed was a new hire, 2 years roughly out of school, who had just joined the company off a job doing AI programming[2] in Manhattan mostly using LISP.  As a new hire, Jim gave Ed a task which was fairly generic and didn’t require knowledge of D’s large code base: write a memory manager.  This was a very abstract task[3] and really required knowing your stuff at a pretty low level.  And while Ed, at the time, obviously wasn’t Jim’s equal, Ed was also damn smart.  He would later go on to a number of senior industry roles ultimately ending up at one of the industry’s largest and most successful companies (but that’s years and years later).  So Ed went off and works on this for about a month.  And he prints it ou[4]t and brings it to Jim for approval.

Jim takes it, rifles thru the pages, thinks a bit and then says “Ok.  Explain it to me.”  Ed looks at him and says something to the effect of “Well I know it works but it’s now so complex enough that I can’t actually explain it”.  Jim hands it back to him and says “If you can’t explain it, it is not engineering.  Rewrite it.”  And Ed, to his credit, not only finished it faster than the first version but this version he could explain.  

To me that’s the very essence of programming versus software engineering.  Any idiot can be trained to be a programmer but a software engineer is something entirely different.  All engineering is about fully understood, repeatable processes.  And at the very of software engineering there is an element of professionalism and understanding exactly what’s going on.

I’ve likely told this story to every single engineering team I’ve ever had.  And I’ve had times where I know that an engineer has gone off the reservation and is spiraling in the wrong direction but I felt that it provided this type of teachable moment.  I’m not, yet, the type of engineer that Jim is but one of these days I hope to be.

￼
[1] Yes readers, before there were keyboards, there were toggle switches.  Think back to 1977 or so.

[2] The timeframe here was the early 90s so Ed was doing this in the late 80s when AI was hot as the Internet has been since 1995 or so.

[3] And still is.  Writing a memory manager, 25 years later, is still a damn hard thing.

[4] Pre-internet guys, pre-internet.  Its what you did.
</description>
        <pubDate>Tue, 02 Jul 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/07/02/programming-versus-software-engineering-or-the-parable-of-jim-and-ed.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/07/02/programming-versus-software-engineering-or-the-parable-of-jim-and-ed.html</guid>
        
        <category>mtum_chapter1</category>
        
        <category>startup</category>
        
        <category>mtum</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Understanding a Small Organization's Communications Model - Github Issues vs Google Docs vs Slack</title>
        <description>It is not hyperbole to say that communications is the lifeblood of an organization, even a small organization.  And while there are many types of communications tools that you use in an organization, I find that there are three common types:

* Ticket Tools aka Github Issues or Jira or Trello
* Documentation Tools such as Google Docs or Microsoft Word
* Realtime Tools such as Slack or Gitter or Discord

The obvious question becomes when to use what tool.  

# Ticketing Tools

A ticketing tool like Github Issues, Jira or Trello is really a communications message that amounts to &quot;Do this Task&quot;.  Most good ticketing tools allow you to communicate in a bunch of ways including:

* Do this task (the ticket itself)
* Clarify it by attaching information to the ticket
* Engage in back and forth with people via discussion on the ticket

Think of your tickets as an on going communications forum that is *task oriented* and results in a deliverable.

# Documentation Tools

Back in the old days the only real communications tool was Microsoft Word and a Word document was the be all, end all.  Increasingly it now seems like a full writing environment like Word or Google Docs is what you use when you have to work something out conceptually.  

# Realtime Tools 

Both Slack and Discord are realtime discussion tools.  And what these tools do is give a way for 2 or more people to work out a problem and come to a consensus.  Slack discussions can transpire either synchronously or asynchronously and are a rich medium including attachments, polls, gifs, etc.  

# And There's a Flow

I wrote this originally because someone was asking me &quot;what tool do I use&quot; and I actually had problems parsing that statement.  To me this is a flow as much as anything.  I find that things might start in Slack, proceed into Google Docs and then into 1 or more Github Issues.  Alternatively you often see a Google Doc split into a half dozen Github Issues.

At least from an engineering perspective, the output of all of these communications tools is one or more tickets that represent a unit of work to be done.
</description>
        <pubDate>Fri, 28 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/06/28/understanding-a-small-organization-s-communications-model-github-issues-vs-google-docs-vs-slack.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/06/28/understanding-a-small-organization-s-communications-model-github-issues-vs-google-docs-vs-slack.html</guid>
        
        <category>software_engineering</category>
        
        <category>startup</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Understanding a Small Organization's Agile Model</title>
        <description>Agile is a an approach to software development that replaces traditional top down project management with a working style  that is characterized by the division of tasks into short phases of work and frequent reassessment and adaptation of plans.  This definition was provided courtesy of Google and I'm not certain as to its exact source.  Here is the [Wikipedia description of Agile](https://en.wikipedia.org/wiki/Agile_software_development).

Agile is one of those things that most of us have some version of what it means in our heads but everyone seems to have a different set of working practices.  This is my version of agile for how I approach software development in a fast paced, greenfield context.  I have worked this way for about 20 years now and I have found that it is more than agile enough even though I deliberately ignore a lot of the agile concepts.

# Its All About the Ticket

To me the essence of agile lies in small, fast based units of work.  And that unit of work is generally represented by a ticket whether written in Jira, Github Issues or something else.  The key skill here is decomposing big units of work into small, readily accomplishable chunks that can be done in succession (or in parallel) whether by one individual or several.

# Rapid, Regular Deploys

When you work in an agile context, it is essential that you deploy regularly as this is the only way for your developers to get the real world feedback about their code and whether or not it works.  In 2019 this should ideally be done by a CI server that simply takes submitted code and handles the deploy automatically.

# Regular Refactors

If you're going to do agile then one thing that you are inherently committing to is the idea of regular refactors.  Refactoring is the process of taking code that you have written, often recently having written and restructuring it as you understand the problem more deeply.  In an agile context when your understanding of the problem is constantly changing, one way you address this is by committing to refactoring.  

# Test Coverage

Test coverage on code is pretty much explicitly not an agile requirement but I personally find test coverage to be an essential agile tool.  One of the best ways to keep your speed up and be able to refactor regularly is to have test coverage that presents the speed at which you are trying to operate from becoming an issue.

# Incremental Development of Data Structures

A classic approach to development is that you start with the underlying data structures or database schema -- and you plan it in full.  This is the very essence of &quot;anti-agile&quot; also known as waterfall development but it is still surprisingly comment -- even among &quot;agile&quot; folks.  Now part of the reason for this is that many development environments don't lend themselves to incremental schema development but that isn't an excuse.  One of the best aspects of Ruby on Rails from an agile context is that the underlying Rails migration facilities handle incremental schema development pretty much flawlessly.  

Ideally from an agile schema you should be building the schema ticket by ticket as you develop.  And you should never be doing more schema development than is in the ticket (and I know that this gets violated all the time; including by myself).

# Pair Programming Whenever Possible

Pair Programming, apply two people to the same ticket at the same time, sharing a screen is still a controversial topic.  Some people take the perspective that it improves code quality.  Other people find that one of the people is loafing while only one person codes.  I find pair programming to be a wonderful agile tool in that it tends to improve code quality and keep developers on track 

# What I Don't Believe In About Agile

Here are the things about &quot;agile&quot; that I don't really believe in at all:

* [Scrum / Scrum Master](https://www.cprime.com/resources/what-is-agile-what-is-scrum/).  Generally speaking I don't believe in scrum / scrum master because I work in very small teams and I find that very small teams simply get stuff done and there isn't the need for a formal &quot;scrum master&quot;.  I also dislike the term &quot;master&quot; here for the same reason that I dislike &quot;master bedroom&quot;; it has bad connotations imho.  The definition from google for *scrum* is &quot;a set of practices used in agile project management that emphasize daily communication and the flexible reassessment of plans that are carried out in short, iterative phases of work.&quot; and to me that's just simply agile.  
* Agile Velocity - I find this to be complete hogwash.  An agile velocity is a metric that is typically calculated by your ticketing system and reflects some sort of &quot;velocity&quot; related to ticket completion.  The reason that I don't find this to be a valid concept is that when you have small teams, a single crappy ticket can entirely pork your &quot;velocity&quot; and since this is a metric, managers tend to take it way too seriously.  
* Sprint - this one I kind of believe in, the idea of organizing your development around short lived goals and then executing on them.  I do get this but I find that when you have continuous deployment, the overall sprint concept is less relevant

</description>
        <pubDate>Fri, 28 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/06/28/understanding-a-small-organization-s-agile-model.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/06/28/understanding-a-small-organization-s-agile-model.html</guid>
        
        <category>software_engineering</category>
        
        <category>startup</category>
        
        <category>agile</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Understanding a Small Organization's Git Development Model</title>
        <description>I am actively working on a new codebase for a personal project and I thought it might be instructive to document how I am using Git.  The first thing to understand is that we are using Github at the Organization level which means, yep, paid Github.  Github is pricey but it is absolutely worth it.  Anyway ...

The development style we are using is what I would generally term branch based development.  This means that there are three branches / types of branches: 

* master
* develop
* feature branches which are named for their GitHub issue number and then a short word or phrase i.e. 42-poros

The master branch is ONLY for deployment and should always be kept pristine.  The assumption with any new code base in 2019 is that deployment can happen at any damn time and, ideally, should be automated.

The develop branch is where development kind of happens.  Ideally you shouldn't work directly in develop but, well, shite happens.  And sometimes commits will happen directly on develop.

Ideally all work happens in a feature branch and this brings us to the actual git commands that you will use daily.  

# Git Commands I Actually Use Daily

Git is what I would refer to as the &quot;swiss army chainsaw&quot; of version control.  Git is astonishingly powerful and I intentionally use a limited subset of git commands because these commands are how I understand to use Git.  I deliberately use the atomic git commands i.e. create branch is separate (for me) from change into branch because I like to use Git this way; again this is how I understand it.  

* git clone git@github.com:fuzzygroup/job_seeker_dashboard.git -- pull down the remote repo into your local working directory
* git checkout filepath -- this checks out a single file from the repo and replaces the existing file EVEN IF THERE ARE LOCAL CHANGES
* git branch 42-poros -- this creates the local branch 42-poros
* git co 42-poros -- this changes into the local branch 42-poros
* git status -- this shows you your current status
* git branch -d 42-poros -- this deletes the branch 42-poros
* git add . &amp;&amp; git commit -m &quot;rollup commit&quot; -- this adds all local files and saves them with a commit message of &quot;rollup commit&quot;
* git co develop &amp;&amp; git pull origin develop -- this changes into the branch and updates develop
* git rebase origin/develop -- this rebases the code in your branch from the code in develop
* git pull origin develop -- this pulls the code in develop from the remote
* git diff db/schema.rb -- show me the differences in schema
* git log -- show me a log of the sequential commits
* git log --oneline --graph -- shows you the git structure
* git reset HEAD~1 -- remove the last 1 commit from git and place them back into the working tree
* git reset HEAD~2 -- remove the last 2 commits from git and place them back into the working tree
* git stash -- temporarily store the work you are currently doing in a local branch so you can change to another branch, do something and easily resume without an add / commit cycle
* git stash pop -- get back the work you stashed
* git push origin 42-poros -- this gives you a url you can use to create a pull request for your code to be reviewed

# General Git Workflow

Here is the general git workflow I use:

1. Go to GitHub issues and grab an issue
2. Do a *git status* to make sure I don't have any work to commit.
3. Change into develop with *git co develop*
4. Pull from remote to make sure I'm up to date with: *git pull origin develop*
5. Create a branch with: *git branch 42-poros*
6. Change into the branch *git co 42-poros*
7. Do my work locally and when I'm ready, add with: *git add .*
8. Write my commit message with: *git commit -m &quot;a message that is hopefully better than this&quot;*
9. Push my code to the server for review with: *git push origin 42-poros*  This creates a url you can used to make a &quot;pull request&quot; that another person on the project can review.
10. Change back into develop and lather / rinse / repeat

 # An Important Note

When you get yourself into a git badness state.  STOP.  Don't flail foolishly; swallow your pride and *ask* an expert.  

Also this article was helped with by the usual reviewer, my buddy [Sean Kennedy](https;//csphere.github.io/).  Any errors are mine not his.  His git-fu is substantially greater than mine -- he's my expert.  Thanks buddy!
</description>
        <pubDate>Thu, 27 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/git/2019/06/27/understanding-a-small-organization-s-git-development-model.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/git/2019/06/27/understanding-a-small-organization-s-git-development-model.html</guid>
        
        <category>software_engineering</category>
        
        <category>git</category>
        
        <category>github</category>
        
        
        <category>git</category>
        
      </item>
    
      <item>
        <title>If I Was a Science Fiction Author, I Would Read This</title>
        <description>I'm not a science fiction writer.  Nor do I even play one on the Internet but I do know people who are.  Here are two fantastic books that I suspect could easy provide the &quot;science&quot; in your &quot;science fiction&quot;:

* [How To](https://www.amazon.com/How-Absurd-Scientific-Real-World-Problems/dp/0525537090/ref=sr_1_1?crid=UQX5KJ82O5ML&amp;keywords=randall+munroe&amp;qid=1561645604&amp;s=books&amp;sprefix=randall+munr%2Caps%2C279&amp;sr=1-1)
* [Thing Explainer](https://www.amazon.com/Thing-Explainer-Complicated-Stuff-Simple/dp/0544668251/ref=sr_1_2?crid=UQX5KJ82O5ML&amp;keywords=randall+munroe&amp;qid=1561645604&amp;s=books&amp;sprefix=randall+munr%2Caps%2C279&amp;sr=1-2)
* [What If](https://www.amazon.com/What-If-Scientific-Hypothetical-Questions/dp/0544272994/ref=sr_1_3?crid=UQX5KJ82O5ML&amp;keywords=randall+munroe&amp;qid=1561645604&amp;s=books&amp;sprefix=randall+munr%2Caps%2C279&amp;sr=1-3)

**Note**: How To isn't out yet.  

All of these books are by Randall Munroe of XKCD fame.  What a lot of people don't know is that Randall graduated from MIT and his science is what you'd expect from someone who pulled off graduating from MIT -- **excellent**.  </description>
        <pubDate>Wed, 26 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/writing/2019/06/26/if-i-was-a-science-fiction-author-i-would-read-this.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/writing/2019/06/26/if-i-was-a-science-fiction-author-i-would-read-this.html</guid>
        
        <category>writing</category>
        
        <category>sciencefiction</category>
        
        <category>sf</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>Tools for Publishing an Ebook</title>
        <description>I resisted John Gruber's Markdown for **years**.  I always looked at it as &quot;Damn it.  I know HTML; why the f would I need that?&quot;.  And then I started blogging using Jekyll which really forced immersion and now I damn well adore Markdown.  Markdown is one of those rare technology standards that I can point to and say &quot;Yep.  It actually made my life *better*&quot;.  

I've recently been thinking about building an ebook and I'm not willing to **not use** Markdown as my writing format.  My formatting needs aren't large so, for me, Markdown is just fine for the writing portion of an ebook. The questions come about getting something else out of the markdown: 

* How do you generate the epub file?  
* How do you generate the pdf?  
* How do you get something into whatever format the kindle uses?
* How do you get something into the kindle store?
* How do you get something into the apple store?

I started my research from a Hacker News discussion on tools for [ebook publishing](https://news.ycombinator.com/item?id=15433327) which is a great place to start.  I've then done some additional research and I've summarized each of the tools.

# LeanPub

In the world of tech books, it seems like every few years there is a new company that I find myself regularly buying tech books from.  It used to be O'Reilly.  Then for a lot of years it was Pragmatic and, this year, it is LeanPub.  I have bought more technical books from LeanPub in 2017 than I have from Pragmatic and O'Reilly combined -- in the past two years.   LeanPub takes in Markdown and creates book from this markup.

# Gitbook

[Gitbook](http://www.gitbook.com) is a tool for taking inputs from any of these formats and producing an ebook:

* DocBook V5.x (.xml)
* Word (.docx)
* HTML (.html)
* Open Document (.odt)

Unfortunately the lack of Markdown as an official input source means that Gitbook isn't an option and I find that disappointing.  Their [pricing model](https://www.gitbook.com/pricing) is actually reasonable, free for open source books and $7 / month for up to 5 private ebooks.  They also seem to have good collaboration tools which is obviously useful for the publishing / editing process.

# Softcover

Softcover is a ruby based tool for creating ebooks created by Michael Hartl who is best known for creating documentation for the Ruby world.  Softcover is a Ruby gem which wraps around a large amount of open source and proprietary tools that are combined into a comprehensive ebook publishing tool.  Here's an example of the softcover check command which illustrates the different tools that Softcover relies on:

    Checking Softcover dependencies...
    Checking for LaTeX...         Missing
    Checking for GhostScript...   Missing
    Checking for ImageMagick...   Found
    Checking for Node.js...       Found
    Checking for PhantomJS...     Found
    Checking for Inkscape...      Missing
    Checking for Calibre...       Missing
    Checking for KindleGen...     Missing
    Checking for Java...          Found
    Checking for zip...           Found
    Checking for EpubCheck...     Missing

Even with all the open source tooling that I use constantly, I did not have all of these installed and that was fairly surprising to me.  Happily Softcover does an excellent job of not only telling you what you don't have but also how to get it.  Here's an example:

    Missing dependencies:
      • LaTeX (http://latex-project.org/ftp.html)
          ∟ Huge download—start it now!
      • GhostScript (should come with LaTeX)
      • Inkscape (http://inkscape.org/)
      • Calibre (http://calibre-ebook.com/)
          ∟ Enable Calibre command-line tools (http://manual.calibre-ebook.com/generated/en/cli-index.html)
      • KindleGen (http://www.amazon.com/gp/feature.html?ie=UTF8&amp;docId=1000765211)
          ∟ Put the kindlegen executable on your path, e.g., in /usr/local/bin
      • EpubCheck 4.0.1 (https://github.com/IDPF/epubcheck/releases/download/v4.0.1/epubcheck-4.0.1.zip)
          ∟ Unzip and place epubcheck-4.0.1/ in a directory on your path

About the only change I could make here would be to point out the brew installations for OSX are *brew install caskformula/caskformula/inkscape* but given that not everyone uses brew, that's clearly an edge case.

# UlyssesApp

UlyssesApp is actually interesting.  In a field of mostly open source tools or software as a service options, UlyssesApp is a client side Mac application for creating your electronic book.  I'm honestly not sure about UlyssesApp but I'm willing to look further into it.</description>
        <pubDate>Wed, 05 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ebook/2019/06/05/tools-for-publishing-an-ebook.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ebook/2019/06/05/tools-for-publishing-an-ebook.html</guid>
        
        <category>ebook</category>
        
        
        <category>ebook</category>
        
      </item>
    
      <item>
        <title>Software Can Always Be Better - An Amazon Example</title>
        <description>One of the hardest things to grasp about software is that it can ***always*** be made better. Even a mature, established bit of software with literally tens upon tens of millions of users can still have fundamental issues.  We're in the tenth year of the iPhone and I've been using the Amazon app on my iPhone for at least 8 of those years.  And yet I can still pick up my iPhone, try something from the Amazon app and find a critical issue.

The problem at hand was really, really simple - I just wanted to buy something.  I saw this [tweet from Jason Kottke](https://twitter.com/jkottke/status/811593510746984452) and that's a hilarious gift idea so I figured that I would buy 2, one for my wife and one for her friend (they are wine buddies).  So I picked up my iPhone and searched up the product.  I selected 2 and clicked the Buy Now with 1 Click button.  

So at this point I've told Amazon:

* What I want
* How many I want
* That I want it now

The interesting thing was that this time my Amazon app had some kind of a password glitch and couldn't process the transaction.  I did the login dance and ended up back at the product page.  My initial thought was &quot;Great! The order went through&quot; but then I realized that wasn't the case at all, I was just back on the product page.  I checked the Orders feature in the Amazon app and, sure enough, no order had been created.  Now wouldn't you think that in an application who's **sole purpose** is to make transactions that in the event of a password problem that the order would somehow be persisted?  Password and authentication issues are a constant thing in all kinds of user centric systems.  And saving a user's input, particularly on mobile, where making input is hard should be damn near sacred.</description>
        <pubDate>Wed, 05 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/06/05/software-can-always-be-better-an-amazon-example.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/06/05/software-can-always-be-better-an-amazon-example.html</guid>
        
        <category>software_engineering</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Building My First Node App</title>
        <description>As anyone who reads this blog knows, I am a Ruby / Rails guy.  Yes I flirt with other things from time to but but, at heart, I am a ruby guy and specifically of the Rails flavor.  Part of the reason for that is that I adore the set of choices that Rails as a whole makes for me. I look at other communities, say Python or Node, and I always think &quot;Man do I really want to figure out a collection of libraries for Task X, Task Y and Task Z and then figure out if they work together?&quot;.  I adore that Rails has put together a set of defaults that just plain work together.

Now, that said, sometimes you do have to go farther afield than Rails and this week I found myself needing to put together a basic Node app for [one of my students](http://fuzzyblog.io/blog/how_to_be_a_developer/2017/07/02/how-to-be-a-developer-001.html) to run with.

# Problem Description

What I need to build is effectively a two page web site - a form which a nodejs backend which calls an API and then generates a page of results for the user.  It is effectively a specialized travel calculator.  This isn't very much but when you don't really know anything about a platform, even the tiniest thing can be problematic.

# Things I Read

Here are the articles I looked at:

* [Hackathon Starter][https://github.com/sahat/hackathon-starter]
* [AirPair NodeJS Tutorial](https://www.airpair.com/javascript/node-js-tutorial)
* [Building A Simple NodeJS API] (https://medium.freecodecamp.org/building-a-simple-node-js-api-in-under-30-minutes-a07ea9e390d2)

# A Starting Point

The best starting point I found was [hackathon-starter](https://github.com/sahat/hackathon-starter).  It is absolutely more than necessary and needs to be cut back but what I got with this was:

* a modern looking starter app 
* done in an MVC style fashion
* complete with CSS that doesn't suck 

# Useful Commands

These were useful commands:

To create a bare bones, basic Node app:

&gt; npm init

To install the nodemon package which lets changes be reloaded dynamically without restarting the app server:

&gt; npm install nodemon -g

[Stack Overflow on Installing Nodemon](https://stackoverflow.com/questions/28517494/nodemon-not-found-in-npm)

To run the app via nodemon:

&gt; nodemon app.js

And that's about all I have time for right now but it is a real start on the process.  The next step is to write the application specific code.</description>
        <pubDate>Wed, 05 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/node/2019/06/05/building-my-first-node-app.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/node/2019/06/05/building-my-first-node-app.html</guid>
        
        <category>node</category>
        
        <category>javascrpt</category>
        
        
        <category>node</category>
        
      </item>
    
      <item>
        <title>Welcome to An Alexa House</title>
        <description>Our house is largely powered by Amazon Alexa's devices.  I have even gone so far as to put them into our guest bedroom.  This blog post was written as something I put on the dresser for our first house guests using the room after I changed the lighting over to be Alexa powered.  Yep.  I've just written documentation for the guest room.  Deep Sigh.  On the positive side my house now has a very, very Star Trek feel to it.  This makes me happy.

For people who don't know what an Alexa is, an Alexa is a voice based computing platform that allows you to issue voice commands that can do lots and lots of different things like play music, turn on lights, supply a romantic or dimmed light mood instead of using candles, set timers, etc.  

The next section is examples of what you can do in *our* guest room with its Alexa.

## Alexa Example Commands

## Lighting 
* Alexa turn on guest room
 * Alexa turn on guest room lights
 * Alexa turn off guest room lights
* Alexa set guest room to Savanah Sunset
 * Alexa set guest room to arctic sunrise
 * Alexa turn on guest room
 * Alexa turn on savanah sunset 
 * Alexa set guest room to green
  * Alexa set guest room to red
## Music
 * Alexa play Journey
 * Alexa play Frank Sinatra
 * Alexa play Raise Your Glass by Pink
 * Alexa play Christmas carols
## Clock and Time
 * Alexa what time is it
* Alexa set a 10 minute timer
 * Alexa set a timer named check plex for new movies for 10 minutes
 * Alexa set a 7 am alarm
 * Alexa set an alarm for 7
 * Alexa set a 2 minute timer named brush my teeth
## General Life
 * Alexa what is 43 * 523 
## What Do I Wear 
* Alexa what is the weather
* Alexa what temperature is it?
## For Fun
* Alexa Play Akinator
 * Alexa tell me a joke
 * Alexa why did the chicken cross the road
 * Alexa Mordor thing shelley knows JSJ

And if you want to be kind of an funny jerk you can also say 
 * *Alexa turn off all lights* 

This will turn off all lights in the entire house.  Boom!  Now this seems like an insane feature until you realize that your kids have never found a light they wouldn't prefer to leave on.  Deeper sigh.

## More Details on Alexa

A single Alexa device costs about $50 and can replace your:

 * Alarm Clock
 * Radio
 * Weather Report
 * Stereo
 * Timer
 * Light Switch

I use the following equipment in our guest room:
 * An [Alexa Echo Dot Third Generation](https://www.amazon.com/All-new-Echo-Dot-3rd-Gen/dp/B0792K2BK6/). The new third generation, the one I actually linked to is a much better stereo replacement since the speaker was improved.  
 * [A Wall Mount](https://www.amazon.com/Compatible-Kitchens-Space-Saving-Solution-Management/dp/B07JX5DNX5)
 * 2 Phillips HUE Color Changing Light Bulbs; the [Starter Kit](https://www.amazon.com/Philips-Hue-Equivalent-Assistant-California/dp/B07DPYM57M/) is usually the best deal.

In terms of other smart home technologies, I've used X-10 and Iris and I am so, so, so much happier with Alexa.

## Privacy 

And, yes, I know the argument about not wanting Amazon to listen in on your home.  I don't believe that is a valid concern given that virtually every one reading this generally carries a cell phone that is already doing this (Hey Siri / Hello Google).  Scott McNealy pointed out the loss of our privacy back in the 90s.  This is not news.

*Disclaimer*: Oh hell yes I'm an Amazon shareholder.  I'm also a proud user of as many AWS APIs as I can find.  Go buy an Alexa.  Hell please buy six or seven.  I have at least six of them.  </description>
        <pubDate>Tue, 04 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/alexa/2019/06/04/welcome-to-an-alexa-house.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/alexa/2019/06/04/welcome-to-an-alexa-house.html</guid>
        
        <category>alexa</category>
        
        <category>amazon</category>
        
        
        <category>alexa</category>
        
      </item>
    
      <item>
        <title>So You Want to Start a Consulting Practice ...</title>
        <description>I spent about a decade as a full time consultant focusing on high end Rails development services with an emphasis on big data.  And in that period I was actually quite successful both on the revenue and life flexibility front.  If you have any type of truly specialized expertise then consulting is a possible career path.  I know a number of folks with specialized expertise and I seem to find myself in the &quot;How do you consult?&quot; discussion regularly so here is a write up.

# Understand the Consulting Equation

Here is the basic consulting equation:

Consulting is An Equation Where You Trade Time for Dollars

That's all consulting is.  It really is just that simple, you trade your time for dollars.  And whatever the consulting customer takes away from it is up to them.  If they don't follow your advice, it is on them, not you.  That's a tough thing to accept at times.

# Be Brave In Setting Your Hourly Rate

If you're going to trade time for dollars then you have to have an hourly rate and that hourly rate usually is pretty aggressive.  My general advice on setting your hourly rate is that you almost always need to be higher than you feel comfortable with.  The reason for this is that if you make it too low then:

 * Customers won't treat you as seriously; a higher price sets a mental tone for the customer
 * It is easier to take your hourly rate down (i.e. give a customer a discount) then take it back up
 
One of the ways that I as a software engineer came to grips with my hourly rate is that I looked at the nature of the services I provide and I asked myself &quot;Am I any less talented than my $xyz / hour attorney?&quot;  And when I realized that I wasn't, well, I was much happier with charging a high hourly rate.

# Charge for Every Interaction

When I first started consulting I didn't keep track of the time that I spent texting or im'ing with customers.  I looked at it as &quot;well its just an im&quot;.  Nope!  And the answer to this again lay in my relationship with my attorney -- I can't text him a question and not get billed for it so the same goes for my services.

# Have a Minimum Billing Increment

Along with the concept of charging your customers even when they text you goes the idea of a minimum billing increment.  This can be anything you want it to be as long as you are ethical with it and disclose it to your customers (who will promptly forget about it, at least in my experience).  When I was billing $100 / hour, my minimum billing increment was 15 minutes or $25.  And I know this feels like a lot but if you don't aggressively manage your consulting relationships then your consulting relationships will manage *you*.

# How Do I Replace My Existing Salary?

If you're going to replace your existing salary then the first thing you need to know is what your salary actually is.  And this sounds stupid, right?  I mean your salary is what they pay you on an annualized basis, right?  Actually if you are a full time consultant then your &quot;salary&quot; needs to cover the costs of what you currently get, which is salary + benefits.  Depending on the nature of the benefits you get from a job, the general rule of thumb is to multiply your salary by a percentage which covers:

 * equipment
 * vacation
 * health care
 * random other benefits

The range on this is generally 1.2 to 2.0 times the salary.  For example if you are working at a lightly funded startup and there aren't a lot of benefits then you multiply your salary by 1.2.  And if you are working for a company with great benefits (think Google) then you multiply your salary by 2.0.  So if you are making $100,000 then you might need to replace this as follows:

 * 120,000 (lightly funded startup)
 * 200,000 (google)

in order to have an equivalent standard of living.

# Model Your Damn Numbers Well

[Here is a Google Docs spreadsheet that models all of my normal assumptions for building a consulting practice](https://docs.google.com/spreadsheets/d/1O6T1Qbk3cmHcarHu1t0JaIcWlcfLvLdv-ffrzvHobb4/).  

# Write Your Product Literature Early 

After the financial stuff, my normal advice for consultants is to write their product literature - a web site about themselves - early.  This should be roughly the second thing you do and once you make it, get feedback from people you respect.  Given that this is what people see before the hire you, it should be good.

# Sell Your Time in Blocks that Can Be Pre-Purchased

One final thought is that for certain types of consulting it may be appropriate to sell your time in blocks that can be pre-purchased such as &quot;7 sessions for the price of 5&quot;.  Keep this in mind because you get the revenue up front and that pre-disposes the client to keep going with you.</description>
        <pubDate>Tue, 04 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2019/06/04/so-you-want-to-start-a-consulting-practice.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2019/06/04/so-you-want-to-start-a-consulting-practice.html</guid>
        
        <category>consulting</category>
        
        <category>startup</category>
        
        <category>marketing</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Sales, Marketing, Advertising and Public Relations</title>
        <description>In undergrad I majored in Management but I took enough marketing classes that I can still summon [Ries and Trout](https://en.wikipedia.org/wiki/Positioning_(marketing)) on occasion.  Let's start with some basic definitions.

# Sales Versus Marketing

The distinction between sales and marketing is pretty damn critical.  

 * Marketing is the creation of demand for a product or service
 * Sales is the execution of strategies to fulfill that demand

What this means is that you do marketing BEFORE you do sales.  Marketing leads to sales; not vice versa.

# Advertising versus Public Relations

Advertising and Public Relations are both elements of a marketing strategy in that, when executed well, *create* demand.  Now the best description of the difference between advertising and public relations cast them in light of traditional, male-female relationships.  

 * When *you*, the male, tell a girl how good you are in bed, that is advertising.  It is inherently untrusted.
 * When another girl tells the girl you're interested in how good you are in bed, that's public relations.  It is generally trusted.

The rule of thumb is that if you have an advertising based strategy, you're going to have to have a lot more &quot;impressions&quot; or &quot;views&quot; of the advertising for it to have the same impact as public relations but public relations is more hands on / takes more work. Whereas with advertising you simply spend money. 

</description>
        <pubDate>Tue, 04 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2019/06/04/sales-marketing-advertising-and-public-relations.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2019/06/04/sales-marketing-advertising-and-public-relations.html</guid>
        
        <category>startup</category>
        
        <category>marketing</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Production of Cocktails at Scale for a Fan Run Event</title>
        <description>I recently  had the opportunity to produce Jello Shots and pre-mixed cocktails for an event and I found the process surprisingly interesting -- as so many things are actually interesting when you explore them.  The truly interesting thing for me was the parallel between efficient production of beverage things and the learnings I had in college in a manufacturing class.  Pretty much everything I learned regarding an efficient manufacturing workflow proved true.  

And I will admit that a big part of why I am writing this down is to document the assembly line process -- for the next damn time ...

## Issue: Taster Needed

I am not a drinker so all of this was done in conjunction with my Shiny wife, Shelley.  Many thanks to her since I could not have done this without her.  As with any type of cooking, tasting is crucial to the final product.  She nobly bore up to this challenge.

## This Is Chemistry / Engineering Not Craft Cocktails

One of the things that I realized when I began this was that this was essentially chemistry and engineering not craft cocktails.  While I made the joke online that I was making &quot;Small Batch Artisan Jello Shots&quot;, this was mostly for the lulz.  When you are producing 180 in a morning, well, you aren't artisan.

## Inclusiveness 

Given that we are an inclusive organization, I found it necessary to also produce sugar free jello cocktails.  The crazy thing about sugar free jello is that while a standard jello packet is 3 ounces, the sugar free is about 0.3 ounces -- small enough that you may actually think that the sugar free isn't sweetened.  Rest assured that is sweet.  It also apparently has a distinctly chemical after taste so your desire to taste them may vary.  

## The Importance of the Assembly Line

The since most important thing is that this is an assembly line and you have to get that line and flow exactly correct.  Here is a picture of the final line:

### Instructions

Here are the things you need:

1. The small solo cups, with lids, for making the shots.  Here's how to calculate what you need - each 3 oz pack of jello makes roughly 15 decently sized Jell-O shots.  So if you want to make 180 like I did that means 180 / 15 or 12 packs of jello. Some cups will break or not seal well so you want to have some extra.  Whatever you do, if you can avoid it, DO NOT buy multiple brands.  If you buy multiple brands then you will find that there are stacking issues due to there being size issues and that the list won't be interchangeable.  Here is the pro tip if you buy multiple brands -- put the lids on the sheet tray with the shots.  [Crude Analytics such as it was](https://i.imgur.com/GyfB05El.jpg).  [Storing Lids with the Shots](https://i.imgur.com/TS1wErUl.jpg)
2. A sharpie marker for labeling S or SF (sugared or sugar free). 
3. Flat sheet trays or rimmed cookie sheets; you will generally get about 15 to 17 shots per batch so you need sheets large enough for them.  [Picture](https://i.imgur.com/i8kaebgl.jpg)
4. A stand mixer with the whisk attachment 
5. A large pot for boiling water
6. A large bowl for storing cold water; what I did was make a big bowl of ice water.
7. Jello of various colors
8. Vodka 
9. A good 4 cup measuring cup preferably with a look down scale to make measuring easier.  Oxco is excellent.  You use this for the hot water.
10. A good two cup measuring cup for the cold water.  Again one with the look down scale.  And again Oxco.  
11. A dipper for pouring from the boiling water into the 4 cup measuring cup; A one cup measuring cup is excellent for this.
12. A dipper for pouring from the cold water into the 2 cup measuring cup.  A one cup measuring cup is excellent for this.

Here is the overall flow:

1. Bring the water to a boil.
2. Make the ice water.
3. Add the Jello to the stand mixer with the whisk attachment.
4. Add the boiling water to the mixer and turn it on.
5. Add the cold water and turn it on. 
6. Add the vodka and turn it on.
7. Fill a measuring cup with the mix.
8. Fill the Jell-O shots.
9. Chill the Jell-O shots.

Here are the specific instructions that you need to follow:

1. Clear space in the fridge for as many trays as you plan to make.  In my case, since this was first time in a while, I had to scramble to make space since I didn't know how many sheets I would end up with.  Don't be me -- do the math!  Or realize that you only own N sheet trays and that's the maximum amount of space you need to clear.
2. Add a 3 oz packet of jello to the bowl of the stand mixer and attach the whisk attachment.
3. Make a large bowl of ice water. Put your dipper next to it.  Put your four cup measuring cup next to it.
4. Start a large pot of water boiling.  Put your dipper next to it.  Put your two cup measuring cup next to it.  By the time the water boils the ice water will be cold.
5. Set the vodka next to the cold water.
6. Set out a sheet tray with 17 cups laid out on it.  Place the lids also on the sheet tray.
7. Use the dipper to add 2 cups of water to the 4 cup measuring cup.
8. Add 2 cups of hot water to the mixer and set it on low.  You want to mix it for 2 minutes and the easiest way to do this is to use an Alexa i.e. &quot;Alexa set a 2 minute timer&quot;.  Or just use your phone.
9. Use the cold water dipper to add 1/2 cup of cold water to the 2 cup measuring cup.
10. Add 1/2 cup of vodka to the 2 cup measuring cup.
11. Add the water plus vodka to the mixer and let it mix briefly.
12. Pour from the stand mixer bowl into the measuring cup you have with the best spout -- for me this was the 4 cup oxco.  This is how you fill the individual Jell-O shots so pourability really, really matters.
13. Fill each cup 1/2 to 3/4 full.  If this is a sugar free tray and you are making both then label at least one lid so you know what is on the tray.
14. Move the tray to the fridge.  Repeat the process.

And here is the assembly line about to swing into action:

![jello_shot_assembly_line](https://i.imgur.com/yasnga2l.jpg)

## Storage

180 Jell-O shots can be stored in one medium and two small coolers. 

![cooler](https://i.imgur.com/hUvGLkFl.jpg)

## Premix Cocktails

A pre-mix cocktail is when you make a large batch of cocktails for an event so you can simply fill a glass from a cooler instead of mixing them one at a time.

### Premix Margaritas

My wife loves Margaritas and here is the individual recipe:

 * 1 shot tequila
 * 1 shot triple sec or other orange liquor (based on price generally)
 * 1/2 shot lime juice, kroger brand

And here is a recipe that fills a standard 1800 Tequila bottle:

 * 1 1/2 cups lime juice
 * 3 cups Tequila
 * 3 cups Triple Sec

This almost exactly fills the bottle and makes what I am told is a lovely Margarita.  I use 1800 bottles because they seal excellently and we have a number of empties (&quot;Dead Soldoier; Salute!&quot;).

Here is the assembly line for premix margaritas.  Note the towel on the bar to capture drips and the left to right order of ingredients along with 1800 bottles to store the output.

![premix_margaritas](https://i.imgur.com/EenxYc0l.jpg)

### Premix Giggle Juice aka Lemon Drop Martinis 

Another cocktail that I like to mix at scale are Lemon Drop Martinis aka &quot;Giggle Juice&quot;.  Here's the individual recipe:
 
 * 2 oz vodka
 * 1 oz simple syrup
 * 1 1/2 tbsp lemon juice

The wrinkle in this recipe is figuring out that 1 1/2 tbsp lemon juice maps to 1 oz lemon juice.  So here's a recipe for 8 Lemon Drops (I can't remember of this fills an 1800 bottle or if it needed to be doubled to do that; experiment in the interests of Science!).

 * 16 oz vodka
 * 8 oz simple syrup
 * 8 oz lemon juice, Kroger brand

Note: Wash the 1800 bottle first to get rid of the tequila taste.

## Premix Assembly Line

Shown below is the premix assembly line for marg
 
## Credits

My thank you goes out to:

 * Shelley Johnson - Lead Taster, Encouragement and All the Love
 * Lisa Meese - Concept validation and Giggle Juice consumption
 * Lexi Taylor - Teaching me about sugar free jello shots
 * Tami Coxen - For illustrating the quality of craft cocktails when I briefly got my wife drinks from Tami once upon a time
 * Deanna Sjolander - For writing encouragement
</description>
        <pubDate>Tue, 04 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/cocktails/2019/06/04/production-of-cocktails-at-scale-for-a-fan-run-event.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/cocktails/2019/06/04/production-of-cocktails-at-scale-for-a-fan-run-event.html</guid>
        
        <category>cocktails</category>
        
        <category>event</category>
        
        
        <category>cocktails</category>
        
      </item>
    
      <item>
        <title>Zsh and RbEnv Madness</title>
        <description>I'm a Mac user and I recently switched over from Bash to Zsh.  I then got massive weirdness when I switched from Terminal Window 1, Tab 1 to say Terminal Window 3, Tab 9 (yes I have a problem; I'm sure you have your own problems; now shoo!).  Specifically I would see that in the first Terminal Window, I would get the right version of Ruby and then in the second Terminal Window, I would get the &quot;system ruby&quot;.  

This requires a bit of an explanation for the less than Ruby familiar:

 * All (or effectively all; go away you) *nix machines include a version of Ruby aka &quot;System Ruby&quot;
 * The system ruby is generally old and poorly maintained
 * The system ruby is rarely the version of Ruby your application will use
 * You need a &quot;Ruby Version Manager&quot; to set the version of Ruby you need for your application.  Two of the leading contenders are RVM and RbEnv.
 * Both RVM and RbEnv work by shell level fuckery.  Sorry for the swearing but this is how they get under the environment and essentially insert a version of an executable before your standard path.  I also think of both of these as &quot;Dancing Bear&quot; software -- its not how well they dance but that they dance at all.
 * On a directory where you want to use a specific version of ruby you have to create a file called .ruby-version which contains a value like 2.5.1.  This tells RVM or RbEnv to load that version of Ruby when you enter this directory.

Given my allusion to how many terminals and terminal tabs I might have open at any one time, you may have already figured out my issue -- it was my own damn fault.  

The issue was this - if you ever have one terminal reporting a different version from another and both have the .ruby-version file contents then it HAS to be an issue of sourcing .zshrc.  There is likely no other way that this error can occur.  And, sure enough, I realized that this was my first time in this directory since I had changed over from Bash to Zsh.  Here are the suspect lines from .zshrc:

    # rbenv
    export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;
    eval &quot;$(rbenv init -)&quot;

These lines add the current path before the system path and then evaluate the rbenv init function.  And this causes any calls for ruby, irb, bundle, gem, etc to go from the right place and not from system ruby.

The solution was to go thru every Terminal Window and Terminal Tab and execute:

    source ~/.zshrc

Note 1: This same type of thing exists for Python (virtual env) and many other languages.  And it is always the same type of fuckery.

Note 2: As always kudos to my pairing partner, [Sean Kennedy](https;//csphere.github.io/), who helping with this.  </description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2019/06/03/zsh-and-rbenv-madness.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2019/06/03/zsh-and-rbenv-madness.html</guid>
        
        <category>ruby</category>
        
        <category>zsh</category>
        
        <category>rbenv </category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>The Most Horrible, Awful Aspect of Startups and Remote Employees</title>
        <description>I'm a firm believer in remote work -- I have spent the past 20 years as a remote employee for many different organizations and there's no way that you could ever convince me that remote work isn't the best way to structure an engineering organization.  Now, that said, there is one horrible, awful aspect of remote workers for startups -- the damn state.

Any U.S. startup is generally structured as a C, an LLC or a sub chapter S corporation. And that gives the company the correct legal structure to operate as a business.  The problem is that while the company is a federal level entity, employment is fundamentally a state level thing.  And every damn state has some screwy accounting, legal or training thing that will reach out and smack you in the ass -- hard.  I've seen this repeatedly and I've even heard my accountant say things like &quot;Damn Boston; I hate Massachusetts&quot;.  

Let's say that you have a startup with three founding employees who live in:

 * Indiana
 * Maryland
 * Oklahoma

This is three sets of state level paperwork with which you have to comply.  And even if you use a payroll company like PayChex, I guarantee that there will be some level of state paperwork that you, the founder, will be find annoying / incomprehensible / ass maddening. 

I have read that Stripe is trying to make creating startups easier with the [Stripe Atlas](https://stripe.com/atlas) program and I hope so but I am honestly dubious. 


Bah!  States!  A pox upon them.  Begone!</description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/remote_work/2019/06/03/the-most-horrible-awful-aspect-of-startups-and-remote-employees.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/remote_work/2019/06/03/the-most-horrible-awful-aspect-of-startups-and-remote-employees.html</guid>
        
        <category>remote</category>
        
        <category>startup</category>
        
        
        <category>remote_work</category>
        
      </item>
    
      <item>
        <title>Tabaholics of the Chrome World Unite for the Great Suspender!</title>
        <description>My wife thinks I have a problem.  She thinks that my approach to computing with respect to web browsers should be accompanied by a disclaimer like this:

    Me: Hi I'm Scott and I have a problem.
    Support Group: Hi Scott!!!
    
    Me: I'm a **muted voice** *tabaholic*
    Support Group: Oh no!

Now from a rationalization perspective, I view urls on the Internet as pieces of paper.  And so if I have a browser window open with 30 tabs, well, that's just 30 sheets of paper, right?  And my computer can certainly handle 30 pieces of paper, right?

But if I have 10 browser windows open, each with 30 tabs, that's 300 pieces of paper.  And that's just Chrome.  What about Safari, Chromium, FireFox, etc.  Perhaps I do have a problem...

The issue, as with a big fraction of most computer issues, is memory.  From a technical perspective although the HTML that makes up a web page is generally tiny, web pages are frighteningly complex data representations and they are actually pretty damn big.  I regularly see Chrome hitting 11 plus gigs of memory usage (on a 16 gig box).

And this brings us to, wait for it, *The Great Suspender*.  The Great Suspender is a Chrome extension that automatically suspends tabs that haven't been active recently (yes this period is configurable).  This essentially kills the memory footprint for tabs you have open but aren't using.  It took my Chrome memory usage from 11 gigs down to a fraction of that.  I can't recommend The Great Suspender strongly enough.

[View The Great Suspender on the Chrome Extension Store](https://chrome.google.com/webstore/detail/the-great-suspender/klbibkeccnjlkjkiokjodocebajanakg?hl=en)

Kudos and thanks to my good buddy [Sean Kennedy](https;//csphere.github.io/) who clued me into this.  </description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/chrome/2019/06/03/tabaholics-of-the-chrome-world-unite-for-the-great-suspender.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/chrome/2019/06/03/tabaholics-of-the-chrome-world-unite-for-the-great-suspender.html</guid>
        
        <category>chrome</category>
        
        <category>browser</category>
        
        
        <category>chrome</category>
        
      </item>
    
      <item>
        <title>Road Warrior 2019 Edition</title>
        <description>It has been an age and a half since I did the road warrior gig.  Due to some family health issues with respect to elder care, I suspect that I'm going to travel quite a bit in 2019.  Here are some of the things that I've learned about doing the Road Warrior as of late:

 * *Disease*.  Don't ever touch the tray table and seat pocket.  I commented to a friend at the FDA about how I always seem to get sick on planes and I blamed the air.  Her response was that the air is just fine but that the seat pockets and tray tables are &quot;Mos Eisley for germs&quot; (ok I may have rewritten her description to make it more cool but that's my take on it).  My reaction was to stop touching them entirely -- and I no longer get sick after flights.  I no longer get drinks on board; I bring a resealable water bottle with me so I don't have to worry about a drink needing a place to rest.
 * *iPads Rule*.  An iPad is a hell of a computing device for lightweight computing -- if you add a keyboard.  I use a Belkin Ultimate Keyboard for my old school iPad Air and it works great (using it now, lightly balanced on my knees).  Given the airline's move to minimal levels of personal space, a smaller computing device than a traditional laptop is highly desirable.
 * *Netflix Downloads*.  Whether or not you like Netflix shows, I don't think you can dispute that Netflix's Download feature works brilliantly.  Before I ever board a plane, I always download something.  
 * *USB Battery*. I use an Anker PowerCore 10000 and its the first USB battery I've ever had luck with.  Happily they come in multiple colors so if you travel with your wife or kid, you know who owns what.
 * *Cables*.  I travel with a lightning cable for each iOS device which means iPad, iPhone and AirPods.  I used to travel with less but this lets everything charge at once.
 * *Food*.  Always, always stuff something to eat in your bag.  My personal preference is to buy a chocolate croissant at the StarBucks before I leave but that isn't always available so I make sure to have nuts, granola bar, etc.  You never know when your flight will be delayed and you will find yourself in a cess pool of an airport and all food with be closed.</description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2019/06/03/road-warrior-2019-edition.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2019/06/03/road-warrior-2019-edition.html</guid>
        
        <category>road_warrior</category>
        
        <category>startup</category>
        
        <category>travel</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Summer Games - An Overview for Noobs by a Noob</title>
        <description>by Wagner the Impaler

I just returned from 5 days at Summer Games with my wife and we found the experience unbelievably outstanding. I am writing this overview to decompress from Summer Games and avoid the intense con drop from this event.

It should be noted that although I have written some criticisms of Summer Games in this document, I absolutely loved the experience. Summer Games 2019, for me, was magical and I would describe it as one of the best weekends of my adult life.


## What is Barfleet?

Barfleet is a social organization that is really a LARP on the theme of &quot;Starfleet exists and we throw the parties in Ten Forward&quot;. Barfleet exists to throw parties that enhance the fan run con experience. 

One of the really interesting things about Barfleet is that you can’t just join Barfleet -- membership is only by sponsorship.  A member of a certain rank has to vouch for you and basically say “this person isn’t toxic” and be willing to take responsibility for your actions.  This level of accountability makes everyone feel comfortable and free.  For example: 



*   If you drink too much then someone will take care of you
*   If something triggers you then someone will talk it out

And since Barfleet is a non profit organization, it needs to hold an annual meeting and why not hold it at a camping trip?


## What is Summer Games?

At its heart Summer Games is just a camping trip -- for adults -- although I would more describe it as:



*   a 5 day experience in lightweight communal living
*   a giant party with 175 of your closest friends
*   a safe, consent-based environment where fleet is your family and will take care of you
*   an inclusive environment right down to people knowing your dietary restrictions and making sure that your medical needs and religious beliefs are respected (I helped with breakfast and watched this first hand)
*   a place where you can not only use an authentic Japanese Katana to battle frisbees (DiscHack) but also find a martial artist who would offer to restore your Katana to pristine condition

DiscHack pointed out to me the difference between Barfleet and the real world.  After playing DiscHack, someone walked up to me, the member DeadPool, and first asked to see my sword and then offered to restore it.  *Blink*.  When do you find people this talented and random in the real world?  Disclaimer - I took the gold in DiscHack where my fleet name was spontaneously amended from Wagner to Wagner the Impaler.

At Summer Games you are likely to encounter alcohol, sex, nudity and more all in a consent-driven, safe environment. This is not your grandparents camping trip.


## Give Back Don't Take

One of the very different aspects of Summer Games is that it truly is communal living. This runs counter to all of our normal cultural programming. Here are some examples:



*   Food is very often shared (I brought 180 jello shots to give out; someone else gave me a homemade apple pie and fresh plum jam; thank you anonymous wonderful person),
*   If you need help - just ask; someone will have salt, pepper, a beer, whatever
*   If you have help to give - give it (for people without a wagon, I moved a ton of totes)
*   One of the best parts of Summer Games, if not the best, is the communal living. Planning your time as an exercise in communal living is smart as hell. Example - if you are bringing a camp stove, another person in your camp doesn't need one, right?
*   Fire can be a shared resource; I kept the campfire alive part of the last night because I knew someone would want to use it and that's how I got great, great S'mores


## Your Fleet Name

Throughout this document you will see references to people but not their actual name.  Instead you will find “fleet names”.  Examples are Shiny, Nymph, Wagner, etc.  The reason for this is that Barfleet is an environment where the personal wheels sometimes come off and having an abstraction layer (the fleet name) between yourself and the real world can be useful. 


## Planning Your Summer Games Adventure

When you have kids, getting out of town without them is a huge, huge deal -- child care had to be arranged, food had to be prepped, etc.  Our planning process was the single worst mistake I made. I had my list of tasks and my wife had her own list of tasks. Because there was no single view, we each had a different perspective on what needed to be done and that led to conflict.  Remember -- there can be only one list!

Finally when you plan for Summer Games, you really may want to take a day off on the return side of the trip.  Summer Games is fairly intense and all the happy people with jobs that I know are taking it easy today.  Me?  I’m a noob and I’m at my day job -- grumble, grumble, grumble.


## Hail Hydrate

One of the more perplexing aspects of Barfleet for a noob are the constant calls for “Hail Hydrate”.  This means nothing more than “it’s time to drink some water”. This is simply valid advice whenever you are outside in the hot sun and it is particularly valid when you are playing games all day in summer heat.  And while there is always water available at Barfleet events, a good idea is either a Camelbak or portable water container.  Remember there’s no dehydration in Barfleet (apologies to the Nymph).


## This is Glamping Not Camping

I grew up camping and I thought Summer Games was camping -- no. Summer Games is glamping (Glamour Camping). When your context for camping is minimizing weight, it is hard to adapt your brain to camping with electricity (kudos to Ignitable Shot Therapy for running a functional electrical grid; thanks!), air mattresses, ready access to a Walmart and tents big enough to park a motorcycle in. And yes there is one attendee who sleeps in an authentic yurt.

The underlying inverted camping assumption driving glamping is that weight doesn't matter but volume does (since glamping uses cars for travel to the camping location and wagons for travel within the camp site). A car can only hold so much volume but even a prius can haul a crap ton of weight.


### This is Tote Based Camping

The best way I found to wrap my head around the model of Summer Games is by thinking of it as &quot;Tote Based Camping&quot;. Totes are plastic storage containers that all your gear goes in. You get your totes to your campsite with a camping wagon. I found that you could effectively get by with the following categories of totes:



*   Clothing (one tote for all clothing and bathroom stuff for each 2 people in your campsite; most of your clothing will actually go unused)
*   Kitchen Gear / Camping Gear.
*   Food.
*   Misc / Entertainment.
*   Booze. This is Barfleet!  It should be noted that there is a full bar at SummerGames.  I chose to bring alcohol so that it could be in our campsite but this actually wasn’t needed.


### Your Camping Wagon

You need a folding camping wagon or a folding camping hand truck. This needs to be big enough to move one or more totes and you need bungee cords on it so that when the wagon falls over you don't dump your wife's (or boyfriend's) clothes in the dirt. My second biggest Summer Games mistake was not realizing the need for a wagon and thankfully my Shiny wife saved me by purchasing one.

_Pro Tip_: Wagons can be enhanced and customized. Creativity is important.


### Tote 1 - Clothing

Here's what you need at a minimum for each person in the clothing tote (assuming you are staying for the full 5 days, Thursday to Monday; adjust based on days on the ground):



*   10 pairs of underwear (you get wet and muddy)
*   10 pairs of socks if you wear such things; I don't
*   4 pairs of shorts
*   5 t shirts
*   2 jackets
*   Shoes (see below)
*   1 pair of jeans or pants
*   Pajama Pants / Sweats because at times it can be cold
*   Dresses / Skirts / Kilts for the evening dance party as per your preference


### Tote 2 - Kitchen Gear / Camping Gear

This is the heaviest of the totes. I'm a pretty serious cook so my needs are likely stronger than yours. Here's what goes in ours:



*   Propane Torch for fire starting even in the damn rain
*   Extra tarps if you have them
*   Paper Coffee Cups
*   Red Solo Cups
*   Classic Boy Scout Mess Kit
*   Small Propane Stove
*   Trash Bags - you need these for daily use and for hauling out dirty clothes / stuff at the end.  You need at least one for each day.
*   Citronella Candles
*   Plastic Knives and Forks and Spoons
*   Paper Towels
*   AeroPress Coffee Maker
*   Extra pocket knives for giving to someone; I lost my first one before I left the house when another car in our caravan was missing one
*   Toilet paper in case the bathrooms run out; just a roll or two for personal use
*   Paper Plates
*   Kitchen Tongs
*   Spatula
*   Plunge Whisk
*   Cooking Skewers
*   Long Matches
*   Canola Oil cooking spray both pump and aerosol (for a flamethrower in case the zombies attack)
*   Thick Leather gloves for pulling things off hot fires
*   Flashlight - Camping Lantern (a lantern is a flashlight with a handle and a base). It stands up on its own and can be hung as well.
*   Flashlight - Head Lamp
*   Flashlight - Small, personal, pocketable
*   Flashlight - Large enough to blind a zombie
*   Sunscreen
*   Duct Tape
*   Double A and Triple A batteries (LED lights last so much longer than older styles that this may be personal paranoia)
*   Kitchen Knives
*   Frying Pan
*   Tin Foil
*   Pot Holder
*   Measuring spoons
*   Kitchen shears
*   Light Sticks
*   Butane lighter stick
*   Sharpie
*   Rope for an ad hoc popup / tent level sun shield to reduce the damn heat

_Pro Tip_: The best trick I came up with for this tote was that I could upgrade my old crappy kitchen gear to Summer Games stuff by moving it to the camping gear tote and giving my wife nice new kitchen gear.  This may actually be cheating...


### Tote 3 - Food

Bring food for the requisite number of days. Food is very dietary specific so it's hard to describe. For example my wife has Celiac so our food issues are quite different from yours. You can put all the non perishable food in a single tote. Here are the baseline items that will always be in our food tote:



*   Granola Bars - 3 boxes - for quick energy
*   Coffee
*   English Breakfast Tea
*   Tortillas, Corn and Flour
*   Summer Sausage
*   Nuts
*   Crackers
*   Canned beans - because camping and eating beans just feels authentic; I favor Bush's Southern Style White Beans but your preference obviously
*   Tuna, 2 cans
*   An onion or two
*   Bread
*   Salt
*   Pepper
*   Crushed Red Pepper
*   Jalapenos because spicy
*   Olives
*   Marshmallows
*   Graham Crackers
*   Ketchup
*   Mustard
*   Chocolate Bars, fully sealed

**Funny Games Moment**: Chocolate bars melt in summer heat and ours actually did.  Then, in a true fleet moment, the phrase _Chocolate Bukkake_ was immediately coined by Ignitable Shot Therapy. A joke was made that we could find someone in camp with this fetish and and then they spoke up ...


#### Sidebar: Ultimate S'mores

On the last night around the campfire, Adorable Cannon Fodder shared outstandingly excellent S'Mores with me, honestly the best I’ve ever eaten. Thank you! Here is her recipe:



*   Chocolate Graham Crackers
*   Square Marshmallows
*   Double Stuff Oreos
*   Hershey's Bar

She also had the coolest wagon of all complete with hand sanitizer dispenser.


### Tote 4 - Misc / Entertainment.



*   Power strip and 20’ Extension Cord -- if you get an electric tent.
*   USB Batteries for phone charging
*   USB Laser lights for tent coolness; a single USB battery can drive these for multiple nights
*   Two editions of Flux card game, Firefly and Drinking
*   FoxTail LED edition for night time catch
*   Frisbee
*   Wireties assortment
*   Popup Trash Can
*   Extra Sunglasses
*   Nespresso Machine / Milk Steamer (Coffee is Life).  Again if you get an electric tent.  


### Tote 5 - Booze

This can be beer, wine, spirits, mixers, bar tools, etc. This varies for every person.


### Cooler

For food you also need to have a cooler to hold the cold stuff. There are two included meals at Summer Games (Friday dinner; taco night and Sunday breakfast; pancakes, eggs, bacon, goetta). You need a cooler to hold whatever other perishable items you want to eat. This varies per person but here are the essentials that HAVE to be in our cooler:



*   Butter
*   Sea Salt Caramel Milk for Coffee; this is the coffee creamer of the gods; available from Kroger
*   Half and Half; this is the coffee creamer for mere mortals
*   Proteins
*   Cheese
*   Smoked Salmon
*   Hard Boiled Eggs
*   Eggs cracked into a tupperware and beaten for scrambling


## Big Items

A camp site is basically a small bit of land where you live for N days while you try and enjoy yourself. Given that you may be rained on, invite friend over to your campsite, etc, the big things actually matter.  Again since this is glamping not camping, weight doesn’t matter but volume does.  For example, our tent was heavy as hell but it had 3 rooms which meant we had a lot of space when it rained.


### Do I Tent?

Even though Summer Games is a camping trip and tents are kind of a thing, you do not have to stay in a tent, there are cabins and bunk houses.  These accommodate people with medical needs like CPAP machines, people without tents / equipment and people who just want to sleep in a real bed.  


### Tent

A tent needs to be big enough that if you were stuck inside it for N days of rain you would truthfully still be able to say &quot;that was fun&quot;. This varies per person.


### Camping Table

A camping table is a simple plastic table, sometimes folding, sometimes not that gives a decent workspace in camp. Unlike the Boy Scout camping I grew up with where you never had a place to work, a camping table solves this.


### Camping Chairs

Folding camping chairs make it happy to sit down and talk with people. You should have least the number of chairs as you have people on your trip plus a few.


### Heat Source

You need a heat source that you can cluster around because cooking over wood or charcoal is fun. We wanted a fire pit but my wife found the [Char Griller Table Top Charcoal Grill](https://www.amazon.com/Char-Griller-2-2424-Table-Charcoal-Grill/dp/B003HFFVJK/) and everyone was very impressed by it. Even Ignitable Shot Therapy, Summer Games Fire Demigod, thought it was a great idea.

![grill](https://cdn.shopify.com/s/files/1/1960/2909/products/22424_1_STUDIO_Front.jpg?v=1557240466)


### Popup

A popup is just that, a popup canopy which goes over your camping table and chairs to shield you from the rain. There will be times when you are sitting under your popup while water runs underneath you and are still having a damn great time.

![popup](https://www.leaderaccessories.com/public/uploads/goods/image/goods_img/e26f6981bd07d97ea6b26098a9d09089_thumb.jpg)


Note: The best trick I saw at Summer Games was the person who pitched a pop up over their tent.  That one trick likely trimmed 10 degrees from the heat in their tent.  


## Vital - Your Pocket Knife

Ever since 9/11 I no longer carry a pocket knife everywhere I go but going camping one time pointed out to me just how important a pocket knife can be. I used mine constantly. Your choice will vary best on preference. I used a Smith and Wesson Special Tactical and it worked brilliantly.


## Disconcerting Aspects

There were a few disconcerting aspects to Summer Games, specifically:



*   Connectivity. We use AT&amp;T and there was next to no signal. I was personally glad to be disconnected but we had a kid crisis on the way to Summer Games and having to drive to the Walmart to talk to your kid, well, sucks. This is apparently fixable if you use T-Mobile as a provider since that one provider works at Summer Games. Perhaps a [mobile hotspot linked to T-Mobile](https://support.t-mobile.com/docs/DOC-36974) would work.
*   Bathrooms. Oy.


## Useful Advice for the Noob

Here are a series of bullet points that address different things where I had specific issues.  Some of these are repeats from above and the fact that they are repeated indicates their importance.



*   _Totes_ - Get these totes from [Menards]([https://www.menards.com/main/storage-organization/storage-totes-bins/storage-totes/27-gallon-black-storage-tote/27gblkyw/p-1459474982373.htm](https://www.menards.com/main/storage-organization/storage-totes-bins/storage-totes/27-gallon-black-storage-tote/27gblkyw/p-1459474982373.htm)). They have the advantage of being easily made raccoon proof. There is a stripper raccoon named Cinnamon that roams the campsite.
*   _Trash_ - Your trash needs to go up every single night. You cannot leave food around.
*   _Bathrooms_ - The water will go on and off every single day. This is not a big deal and it will be fine. The port-a-potties on site are not as scary as you think they will be.
*   _Car Bag_ - Pack a car bag with emergency clothing for when you get soaked. Make CERTAIN to leave this bag in the car or when your tent gets flooded you will lose your last dry clothes. My wife used the car bag to find a clean shirt on the way home when her clothing was uncomfortable.
*   _Tent_ - Whatever you do get a tent with a functional door such as the [Coleman Tenaya](https://www.coleman.com/tenaya-lake-6p-fast-pitch-cabin-with-cabinets/2000018142.html?cgid=coleman-tentsandshelters#pmax=5%2C000.00&amp;prefn1=numberOfSleepers&amp;pmin=250.00&amp;prefv1=4-Person%7C6-Person&amp;start=1). You will go in and out of your tent a huge number of times and a door is a dramatic improvement in quality of life. Trust me.
*   _Air Mattress_. You need a decent air mattress. You want one with a built in inflater which has Never Flat technology or you will end up adding air in the middle of the night while cursing life, the universe and your choices. It should be noted that this option only works if you have an electric campsite or if you have a UPS plugged into your air mattress.
*   _Zip Up_. Keep your tent zipped at all times or get used to bugs going in and out. I had a spider run over my face on the last night. Disconcerting to say the least.
*   _Tent Pull_.  Attach a lighted pull to your tent zipper. Thanks to NerdOn for this tip using a glow in the dark LED &quot;ice cube&quot;.
*   _Stake down your tent_. A tent with an external frame is largely self supporting so you may not choose to stake it down. This can lead to issues with your tent floating away. [[These are the best tent stakes I've ever used](https://www.walmart.com/ip/Universal-Camping-Steel-Tent-Stakes-Heavy-Duty-Replacement-Pegs/890381996)](https://www.walmart.com/ip/Universal-Camping-Steel-Tent-Stakes-Heavy-Duty-Replacement-Pegs/890381996). Thank you Han Solo Cup for loaning them to me when I needed them!
*   _Shoes_.  You will need more shoes than you think. Camping is absolutely miserable when you don't have decent footwear.
*   _Label your crap_.  Lots and lots of things look identical; it makes it easier when you know that say this green chair is really yours.
*   _Your Damn Car_. If you get really hot and you need a place to sleep or make love, well, go to your car with your sleeping bag and teenager it
*   _Your Car Keys_. Attach your car keys to your belt with a carabiner and keep them in a known place other than that. Losing your car keys means you can't go home and that's a big deal.  I never lose anything and I lost stuff at Summer Games.
*   _Cheap Wedding Ring_. My wife and I went to Amazon for cheap wedding rings (think $10 for titanium) instead of wearing our good stuff. 
*   _Cheap Watch_. You lose track of time camping and a cheap watch helps keep you connected to the real.


## See Also

There are other documents on Summer Games on Facebook that you should read, specifically packing lists.  I read them but still wrote this one because I didn’t feel that the packing lists gave me a feel for the “Zen of Summer Games”; the packing lists had the how but not the why.


## Recovering from Summer Games

Among people who go to science fiction cons, there is the phrase Con Drop: 

Con Drop is a physiological reaction that often has emotional or psychological symptoms. Essentially, it's the endorphins and other happy chemicals your body has been spewing out the last 3-5 days drying up. It's the crash after the high. Con Drop generally happens two to four days after the end of the convention.  [Source](http://speculativechic.com/2017/05/11/convention-life-con-prep-con-crud-con-drop/)

And I would add that Summer Games isn’t a normal con -- it is an event dialed up to 11 in every damn way.  This makes the con drop even worse.  So just as you feel wonderful during Summer Games, I can assure you that you will feel pretty damn bad afterwards -- I know because I am living it right damn now.  Here are some suggestions:



1. Take the day off from work after Summer Games.  I didn’t do that and I profoundly regret it.
2. Aggressively practice self care -- if you like movies then watch movies; if you like to read comics then get out a run and devour it; if you like ice cream, buy the gallon container.  
3. Sleep as much as possible.  This has been the biggest issue for me.  I went from sleeping two to three hours a night at Summer Games to sleeping … three to four hours a night at home.  
4. Eat well if you can.  At Summer Games you aren’t really eating a balanced meal.  Better nutrition is helpful.  The one exception to this is that I’ve found that sugar helps.
5. Help someone else.  I’ve always found that when times are bad, helping someone out makes them better.  Honestly that’s why I wrote this document -- it got me past the worst of the con drop.  So find a friend suffering from this and get together with them; feed them dinner, etc.

And you know what?  Summer Games was absolutely, unquestionably worth it.


## Credits

This document was reviewed and edited by Shiny, Nymph, Dr. Shake  N Bake, Uisge T'Ango Foxtrot and D’zzy L’zzy.  Thank you for turning these somewhat incoherent scribblings into actual prose.
</description>
        <pubDate>Thu, 30 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/barfleet/2019/05/30/summer-games-an-overview-for-noobs-by-a-noob.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/barfleet/2019/05/30/summer-games-an-overview-for-noobs-by-a-noob.html</guid>
        
        <category>barfleet</category>
        
        
        <category>barfleet</category>
        
      </item>
    
      <item>
        <title>And Now for Something New</title>
        <description>Io, Io, its off to code I go.  From bit to byte I build my stack. Io, Io its off to code I go....  

    cd development_project_path
    rails new slumgullion --database=mysql   --skip-spring --skip-listen --skip-test

And then a shift to [RSpec](https://fuzzyblog.io/blog/rails/2017/02/26/setting-up-rails-with-rspec-from-the-start.html).  And then a:

    cd slumgullion
    # mate is my source code editor
    # edit Gemfile (core package file for external components aka gems)
    mate Gemfile
    # Change gemfile version number to 5.2.3
    # Follow directions in blog post above
    bundle update
    mate config/database.yml
    #update database configuration file with development credentials
    bundle exec rails db:create
    bundle exec rails g controller Dashboard
    # set the default route
    mate config/routes.rb
    # write a trivial def index i.e. def index end
    mate app/controllers/dashboard_controller.rb
    # start the server
    bundle exec rails s -p3000
    # open in chrome this url 
    http://localhost:3000/
    # get errors; remember to create a blank index template to force html rendering
    touch app/views/dashboard/index.html.erb
    # mate app/views/dashboard/index.html.erb
    # get a blank page without any errors
    # get it into version control
    git add . 
    git commit -m &quot;Canonical initial commit message&quot;
    # goto git and create a private repo
    # add the origin
    # initial push
    # create first branch paralleling first ticket id
    git branch feature/1
    git co feature/1
    
And now its time for real work.  I personally believe that Rails development is always, always, always, always, always, always best done with N terminal windows where the beginning of them is as follows: Server, Database Terminal, Rails Console, General Command Line usage (add / commit / run tests / etc).  Your opinion may differ but that's how I roll -- keep all your tooling available at the same time.  And yes it is a Zorkian experience where you can be lost in a twisty maze of terminal windows all alike.

    # onto Bootstrap configuration for some appearance
    mate Gemfile
    # add bootsnap
    # add bootstrap-saas
    # stop server with command + c / ctrl c
    bundle update
    # port 3000 is default so omit
    bundle exec rails s 
    # start second terminal tab - db window
    # cd development root
    cd slumgullion
    # start third terminal tab - console window
    # cd development root
    cd slumgullion
    bundle exec rails c
    # start fourth terminal window
    cd slumgullion

Realize that after N late nights, even with examples you wrote yourself using Bootstrap, you do not know how to setup Bootstrap from scratch so sacrifice a small goat in the dark of the night whilst chanting around a firepit.  When that fails to produce results, grumble and then resume with Google while chanting [Save Me FreeCodeCamp.  Save Me FreeCodeCamp](https://medium.freecodecamp.org/add-bootstrap-to-your-ruby-on-rails-project-8d76d70d0e3b).  And like magic you channel this set of commands:

    git mv app/assets/stylesheets/application.css app/assets/stylesheets/application.scss
    # fix application.scss like url above directs
    mate app/assets/stylesheets/application.scss
    # fix application.js like url above directs
    mate app/assets/javascripts/application.js
    mate app/views/application.html.erb
    # add a jumbotron element to see if bootstrap works
    mate app/views/dashboard/index.html.erb
    # restart server as per above

Success !

With thanks to Pete Jenney / Dataware for giving me the basis to make up that lyrical riff.
</description>
        <pubDate>Tue, 21 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/slumgullion/2019/05/21/and-now-for-something-new.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/slumgullion/2019/05/21/and-now-for-something-new.html</guid>
        
        <category>rails</category>
        
        <category>slumgullion</category>
        
        
        <category>slumgullion</category>
        
      </item>
    
      <item>
        <title>Using Ansible ec2.py Directly</title>
        <description>This is a shorty but it is incredibly useful when you need to diagnose [Ansible](https://www.ansible.com/) weirdness.  

*Note:* I know this is obvious but when you have used Ansible and AWS extensively, you get used to ec2.py just being a component to Ansible and you don’t even think about calling it directly.

We recently had a situation where all of our Ansible driven deploys were failing.  And the reason for this wasn’t easy to diagnose because all of our deploys were wrapped up in fairly complex shell logic.  What helped us sort it out was breaking the underlying Ansible execution pipeline apart and calling [ec2.py](https://github.com/ansible/ansible/blob/devel/contrib/inventory/ec2.py) directly.  ec2.py is the open source bit that talks to an AWS environment and returns a massive data structure that represents the &quot;inventory&quot; of servers, volumes, network connections, etc.

Here’s the way to call ec2.py directly

    AWS_ACCESS_KEY_ID=some_access_key AWS_SECRET_ACCESS_KEY=some_secret_key /etc/ansible/inventory/ec2.py
    
As long as your ec2.py is executable (and it damn well should be), this will return the normal giant JSON hash of data that represents your AWS environment.  And if it doesn't return a giant JSON hash, well, then you know you have a problem.</description>
        <pubDate>Sat, 18 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2019/05/18/using-ansible-ec2-py-directly.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2019/05/18/using-ansible-ec2-py-directly.html</guid>
        
        <category>ansible</category>
        
        <category>devops</category>
        
        <category>python</category>
        
        <category>aws</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Camel Camel Camel - A Way to Get Amazon Stuff Cheaper</title>
        <description>The vast breadth of things on the Internet never fails to amaze me.  And when I discover something new, I find that writing about it helps me internalize them.  [Camel Camel Camel](https://www.camelcamelcamel.com), hence forth referred to as Camel^3, is one that I am absolutely late to the party  on but I thought that someone out there might not know about it.  

Camel^3 is an Amazon price watcher that lets you supply either an Amazon link or simply a search term as well as a price target.  Camel^3 will then persistently monitor that product and alert you when the price drops.  The advantage to this is that even though you think that Amazon’s prices are fixed, they are actually fairly variable and when you have a **non immediate** purchase need, Camel^3 gets you what you need at a cheaper price.  

In my case I’m currently lusting after a [Synology NAS Disc array](https://www.amazon.com/Synology-bay-DiskStation-DS218j-Diskless/dp/B076G6YKWZ/ref=sr_1_3?keywords=Synology&amp;qid=1558199113&amp;s=gateway&amp;sr=8-3) which presently has a price of $175.  So I added it to my Camel^3 page with a price target of $130.  And since this is going to require drives with it, I can also set up Camel^3 price watches on [Western Digital RED Raid Drives](https://www.amazon.com/Red-Pro-10TB-Hard-Drive/dp/B072F422FW/ref=sr_1_fkmrnull_3?keywords=western+digital+raid+red+10tb&amp;qid=1558199233&amp;s=gateway&amp;sr=8-3-fkmrnull).    So if my price watches get fulfilled between the disc array and the drives, I should save about $200.

Camel^3 makes money by you returning to their page and using their affiliate links to Amazon.  There doesn’t seem to be any privacy violations, advertising or any other crap as all too often there are.  Oh and massive props to Camel^3 for allowing use of its service without even creating an account and password.  That’s hugely awesome.

Strongly Recommended.  </description>
        <pubDate>Sat, 18 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/amazon/2019/05/18/camel-camel-camel-a-way-to-get-amazon-stuff-cheaper.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/amazon/2019/05/18/camel-camel-camel-a-way-to-get-amazon-stuff-cheaper.html</guid>
        
        <category>amazon</category>
        
        
        <category>amazon</category>
        
      </item>
    
      <item>
        <title>A Mobile First Blogging Strategy</title>
        <description>Not that anyone’s watching but if they were, they would notice that I’ve started blogging again both here and on my [recipes](https://www.fuzzyblog.io/recipes) site.  There were a few things that really made the difference in my starting again but the biggest was realizing that I could actually employ mobile devices in my blogging strategy.  

The interesting thing here is that I made zero changes in my blogging tool.  I'm still using Jekyll hosted on Github Pages.  These changes were purely in how I approached content.  You could use this with Jekyll, WordPress or any blogging tool.

Here's what I did:

1. The biggest change was my adoption of Apple Notes as a core text editor.  Yes it isn't perfect by any means but the fact that Apple Notes is a standard thing across my iPhone, my iPad and my OSX laptops means that I have a simple way to always create a blog post no matter where I am.  The Sync on Apple Notes seems to be flawless and it is fast enough that I now use Apple Notes as a &quot;paste buffer&quot; between my boxes.
2. I [turned off Smart Punctuation](https://www.howtogeek.com/344310/how-to-turn-off-smart-punctuation-on-your-iphone-and-ipad/) on all my iOS devices so I don't have issues with a curly quote in a MarkDown file.
3. I turned off Smart Punctuation on my OSX device also, Apple Menu, System Preferences, Keyboard, Text, uncheck Use Smart Quotes and Dashes.
4. I copied into my Apple Notes Blogging folder a markdown template for how I like to create a post.  When I need to start something I just copy it in at the top of the post.
5. I added a Belkin keyboard to my iPad converting it from a tablet to a lightweight, long lasting laptop.

And this is what my blogging strategy now looks like:

1. Go into notes, wherever I am, in whatever computing device I have handy.
2. Open my template post and copy all of it.
3. Create a new note and paste in my template.
4. Write.  Lather / Rinse / Repeat as needed.
5. When I’m ready to publish, go to my actual laptop and copy the title off the post.  Then I execute a “jekyll post (paste in title)” and press enter.
6. I go back into the note and copy all of it with command + a, command + c
7. I open the resulting post in my MarkDown editor of choice, TextMate.
8. I paste in the text and make any residual edits / adjustments.
9. If I don’t have the Jekyll Server running, I start it with jekyll s to generate the correct feed and such.
10. In the command line, I do a git add / commit / push cycle. I also wrapped this into a shell command so I can just type ./postentries.sh &quot;some description of what i'm posting&quot; and have all of this handled for me (including step 9).
11. And then everything is online almost instantly.

Writing this out does make it seem convoluted but it is actually pretty damn seamless and the ability to use any computing device I have at hand for content creation makes it *wonderful*.

</description>
        <pubDate>Sat, 18 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/blogging/2019/05/18/a-mobile-first-blogging-strategy.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/blogging/2019/05/18/a-mobile-first-blogging-strategy.html</guid>
        
        <category>blogging</category>
        
        <category>writing</category>
        
        
        <category>blogging</category>
        
      </item>
    
      <item>
        <title>When libreadline Goes Whack</title>
        <description>Well shite.  I’ve seen this before and I just plain hate it.  This is the last error you want to get on a Friday afternoon after five.  Taking the time to fix it now generally means that your computer won’t boot on a Monday due to something going wrong in the fixing process and you forgetting what you did.  

Every single time libreadline goes to hell, it is always a pain. Here’s how I fixed it this time.

## The Environment / Context

I'm running a MacBook on OSX (Sierra; yes I'm a luddite on OS upgrades) with [HomeBrew](https://brew.sh/) 1.7.0.

## The Error

Here is the error:

    ```dyld: Library not loaded: /usr/local/opt/readline/lib/libreadline.7.dylib
    Referenced from: /usr/local/Cellar/postgresql@9.5/9.5.12/bin/psql
    Reason: image not found
    sh: line 1: 36440 Abort trap: 6           
    /usr/local/Cellar/postgresql@9.5/9.5.12/bin/psql seasthree_test /Users/sjohnson/fuzzygroup/seas/cas-seas3-attain-admin/spec/fixtures/schools.sql```

libreadline is a core library used by almost any open source tool that ends up reading input from the standard in.  In this case I'm trying to feed a sql file into the pg (postgres) executable.

This is one of those errors that I've seen a dozen or so times and I always fix it with some crazy combination of jiggery-pokery / goat sacrificing.  This time I thought I'd document it for, well, the next damn time.

## The Solution

The answer appears, courtesy of Google via a [Github Issue](https://github.com/Homebrew/homebrew-core/issues/5799), appears to be an update of bash.  Sigh.  While I love the power of command shells, I truly, truly hate messing with them.  

Here's the process I followed:

    brew update
  
That upgrades HomeBrew to the latest.  I then check the process again to see if it has magically gone away and no such luck.  The next step is to upgrade bash itself:

    brew upgrade bash 

On an OSX box there is the version of bash that comes with the system and then there is the bash that comes from HomeBrew.  The reason you upgrade bash is that what Apple ships is woefully obsolete:

    bash --version
    GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin16)
    Copyright (C) 2007 Free Software Foundation, Inc.

Bash after it has been upgraded is now north of version 5. 

The way that you make your new bash usable is that you first have to add it to /etc/shells by editing this file:

    sudo nano /etc/shells

This prompts you for your password.  Then at the very end of the file you need to add this line:

    /usr/local/bin/bash

After adding this line, do the CTRL+W and then CTRL+Q dance to exit the editor.  That essentially tells OSX &quot;Ok this is a valid shell that someone can now tell us they want to use&quot;.  The next step is to use the chsh command to make this permanent.

    chsh -s /usr/local/bin/bash $USER

**Note:** Much of what I know about changing an OSX shell comes from this [Stack Exchange post](https://apple.stackexchange.com/questions/224511/how-to-use-bash-as-default-shell).     

At this point you have set your system up to use the new shell but it won't take effect until you restart your terminal.  Given that I use iTerm and I have about 70 terminals open at any point, restarting a terminal is a big deal.  But what I can do is validate this by using the Apple Terminal.  So I start up Apple Terminal and do a:

    echo $BASH_VERSION
    5.0.7(1)-release

And, unfortunately, I still get the error when I try again.  Sigh.  At this point with the new Bash in place, I'm able to try:

    brew upgrade libreadline

And that begins the process.  Now since libreadline is used by almost everyone that HomeBrew has installed for me, it is a long, long process.  I can watch it upgrade Postgres, Maple, etc.  When it is finally done, I try again only to find that my pg executable isn't there anymore.  This takes a moment of thought but I realize that it is simply moved since the previous directory included a version number.  When I finally reconstruct a path to the correct version of pg, it now executes flawlessly:

    /usr/local/Cellar/postgresql@9.5/9.5.15/bin/psq

Success!  And, in closing, yes I do actually know that I shouldn't have to specify the exact pathing.  A big part of homebrew's advantage is that this isn't needed.  But for some reason, on this particular box, it is needed.  And I've just never bothered to figure out why.</description>
        <pubDate>Sat, 11 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/brew/2019/05/11/when-libreadline-goes-whack.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/brew/2019/05/11/when-libreadline-goes-whack.html</guid>
        
        <category>brew</category>
        
        <category>osx</category>
        
        <category>libreadline</category>
        
        
        <category>brew</category>
        
      </item>
    
      <item>
        <title>The Schema WTF Moment Take 2 - An Excursion into SourceTree</title>
        <description>I recently wrote about the [Rails Schema WTF](https://fuzzyblog.io/blog/rails/2019/05/09/rails-migrations-multiple-developers-and-the-schema-wtf-moment.html) moment, that moment when you’re making changes to your system and you have issues with what to commit.  The problem I wrote about there was what happens when you get a migration from another developer.  This time I’m writing about what to do when you create a migration.

The primary idea here isn’t much different from what I wrote about the other day but the core idea is the same — you only want to commit to version control the changes to the schema.rb file that your migration caused.  Let’s start with a sample migration:

    class AddUniqueIndexToThingss &lt; ActiveRecord::Migration[5.0]
      def up
        execute &quot;CREATE UNIQUE INDEX unique_index_things_on_id ON public.things USING btree (id, project_id) where deleted_at is null;&quot;
      end

      def down
        execute &quot;drop index unique_index_things_on_id&quot;
      end
    end
    
If you think thru this migration, it is going to introduce two changes to schema.rb:

 * A new date stamp on the schema.rb file will be listed at the top
 * A single line for the index creation will be added
 
So conceptually what you want to do is do a git add and commit of only these things:

 * The migration file itself
 * The date stamp line
 * The index creation line

And therein lies the problem because while Git is a fantastic tool, from the command line, you never want to have to commit a range of lines because it is just plain ugly.  Even after a decade plus of using git from the command line, I don’t know how to do that.  

And this brings us to SourceTree.  SourceTree is a Git user interface which makes this particular task actually easy.  Here are the steps:

1. Run SourceTree.  
2. If SourceTree doesn’t open the right git project automatically, navigate to the correct project with File menu, Open command.
3. You should set your view to *Flat list (single column)* and also *Split view staging* if it isn’t that way.  This is done with the hamburger icon to the right of *Pending Files, sorted by path*.
4.  In the panel on the lower left, select the checkbox next to the migration itself.  This will move the migration file up to the Staged Files area.  This is equivalent to a *git add* operation.
5. In the panel on the lower left click on the schema file.  This gives you a text view of the whole schema file on the right.  Start by finding BOTH lines for the date stamp (one will be red and one will be green).  Select BOTH of these and click the *Stage lines* button.  This is equivalent to a git add command for only these lines.  
6. In the text view of the whole schema file, scroll down until you find the exact changes from your migration.  In this case we are looking for an index creation line on the things table.  This line was an addition to your schema.rb file so it should be in green.  When you find it, select that line (or lines), highlighting them, and then click *Stage lines*.
7. In the left most bottom pane, right click the overall schema.rb file and click &quot;Reset&quot; from the context menu.
8. Press Command + Shift + C and write a commit message and then click the Commit message.

## Thank You

As always, thank you [Sean Kennedy](https;//csphere.github.io/) for teaching me this particular trick and also for editing this post for clarity.  Appreciated.
</description>
        <pubDate>Fri, 10 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/05/10/the-schema-wtf-moment-take-2-an-excursion-into-sourcetree.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/05/10/the-schema-wtf-moment-take-2-an-excursion-into-sourcetree.html</guid>
        
        <category>rails</category>
        
        <category>db</category>
        
        <category>schema</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>So You Want to Delete Temp Files - An Excursion into Software Engineering Suckitude</title>
        <description>So today was a day of epic software engineering failure.  Let's consider, just for a second, this string of commit messages:

    feature/ATT-1551-f - And goodbye ... it all goes away like the bad dream that it was
    feature/ATT-1551-e - It is called a constant; sigh; oh and an entire rewrite
    feature/ATT-1551-d - And let's go again - there are still hours in the day to get this wrong ...
    feature/ATT-1551-c - hopefully, possibly the last iteration on this humiliating bit of work
    feature/ATT-1551-b - Argggh !!!!
    feature/ATT-1551-a - Debugging hooks for understanding this bit of madness

Any experienced engineer can read between the lines of that level of *gallows humor* in these commit messages and know, I mean *know*, what's coming.  This whole blog post is an illustration of how something that is drop dead simple can really, really bite you in the ass.  And you can take away from this either a smug feeling of &quot;I'm smarter than that&quot; or you are raising a glass to me somewhere and saying &quot;you poor damn bastard, there, there&quot;.

## The Problem

This all started with the git branch *feature/ATT-1551* which added a feature to our architecture which cleaned up straggling temp files that had been left behind by a variety of different processes that generated data locally that was then queued up to S3.  Yes, yes, yes I know fix the cause not the symptom but that was out of the scope of the defined work; you know the drill.

We are a Rails application and I made the decision to implement this as a Capistrano post deploy hook that executed a Capistrano rake task.  That was, unbeknownst to me, my *first* mistake.  The specification on the ticket said that the need was to clean up any files in shared/tmp that were older than 10 days.  My initial pass on this ended up being about maybe 25 lines of ruby implemented in lib/capistrano/tasks/tmp_files.rake. 

## And Here's Where It All Began

My code went thru the pull review process just fine until there was a comment that said something to the effect of &quot;Wouldn't it be better if the 10 days was not fixed in the code but could be adjustable from outside so we didn't have to redeploy if we needed a different amount of days&quot;.  That sounds logical, right?  The pull request discussion thread diverged between passing in a parameter or using an environment variable and it ultimately converged on an environment variable because a parameter didn't work since this was already buried inside an Ansible based deploy engine that executed a Capistrano task (that then called a post deploy hook).  So with that decision, the descent into madness truly began.  And if my description of Ansible spawning Capistrano spawning a post deploy hook feels virtually Cthulian to you, well, *yep*.

*Sidebar:* I'd love to tell you that all of this was easily resolved but that wouldn't be fun, now would it?  Instead, while the initial code was written by me, getting this actually debugged and solved took two senior software engineers most of a day. And it is an excellent illustration of just how complex even something simple can be in today's world.  The second engineer got pulled in right about here, after the pull review process centered around environment variables.

## Nothing is Actually Simple Anymore

We have a file of environment variables that are automatically defined by our application using the [Figaro gem](https://github.com/laserlemon/figaro).  Normally I'm not an environment var fan but this really works well so I followed our normal process for this:

* Go into our Ansible stack
* Decrypt the environment variable files (these are YAML files and that becomes relevant later on)
* Add the new variable to each of our environments (production, training, staging)
* Encrypt the environment variables
* Move the environment variable files up to our Jenkins deployment box

The reason for the convoluted process is that we don't have these files as part of our Capistrano deployment since they expose serious credentials.  Instead our Ansible wrapper around the deployment handles moving these up and in place.  And this uses Ansible's transparent encrypting / decrypting as well.

After a few go arounds of &quot;what should this variable be named&quot;, I added to our Ansible stack two shell files:

 * decrypt_all.sh
 * encrypt_all.sh 

So at least that makes this process suck less given that each command line for this is about 112 characters long and I have to do it three times every single time.  I try and follow a boy scout's approach to code where you leave things better than you found them whenever possible.  And this did make things better.

## Using the Environment Variables 

Once this was done, that led to modifying the code to pull the value out of the environment variable.  Now this should have been nothing more than reading ENV['DELETE_TMP_FILE_DAYS'] for the value TRUE, couldn't be easier, right?  Ha!  *Shudder*. Given that this was implemented as a Capistrano post deploy hook, getting any bit of this tested boiled down to this process:

* Create a new branch for every single debugging attempt (because we have to go thru a pull review process)
* Write code
* Git commit / add / push
* Go thru pull review process
* Deploy
* Wait thru a roughly 11 minute deploy cycle for something to fail
* Read logs to analyze failure
* Lather / Rinse / Repeat

Given that this is a post deploy hook executed Rake task there is no way to debug this easily.  Yes you can check the code.  And you can execute it manually in Rails console but that's not quite the same as it actually running in the environment … or is it?

## Cutting to the Chase Or 11 Steps Into Madness

This is getting kind of long already so I'm going to cut to the chase and bullet point the issues as we hit them:

1. The first issue was that we kept defaulting to the if / else case where our environment variable wasn't being set.  And since I was casting this to a constant for safety (remember this code deletes files on a file system so it is wise to be damn careful with it), this meant that the constant kept failing a defined? check.  We had put in a raise &quot;Constant not defined&quot; unless defined?(CONSTANT_NAME) check before anything got deleted as a safe guard and that one line of code entirely saved us.  Kudos to my coding partner, [Sean Kennedy](https;//csphere.github.io/), for that.
2. This first issue ultimately ended up being tied to recognizing that a Capistrano post deploy hook *DOES NOT RUN UNDER RAILS*!  And that means that anything loaded by a Rails initializer isn't available — meaning the Figaro gem never executed so the values in our environment variable file weren't even being seen.  But since our environment variable file syntax was read from using ENV['VARIABLE_NAME'] we weren't getting failures due to ENV not existing (ENV always exists, even in an IRB environment).  We would just get a nil value instead of the expected string value of TRUE or FALSE.
3. Figuring out that we weren't in a Rails environment was one of those face palm moments — you've seen the meme, the one with Picard.  I'm more than a bit embarrassed to point out how long that took so we're just going to move on past that issue.
4. Once we recognized that Figaro wasn't available and that we would never get a value for our environment variable key, it wasn't hard to write our own YAML reading routine.  That went thru a deploy cycle and we were fairly certain that it would be solved any minute now — what a crock!
5. What we found was that even though our YAML file had a structure of KEY_NAME=TRUE, we weren't getting back TRUE, we were getting back true.  So, of course, our if test which matched on 'TRUE' would fail.  
6. At this point we started testing using the IRB (remember you aren't using Rails) console and YAML parsing.  And this led us to realize that YAML has automatic type coercion for a range of possible true / false values. The first clue was when my editor showed color coding on the YAML value we had, but not when we changed it to something odd like &quot;ASDF&quot;.  And we ultimately found the [YAML spec](https://yaml.org/type/bool.html) which takes about booleans.  Who even knew there was a spec for YAML???  Any of these in a YAML file will automatically get coerced into a boolean: *y, Y, yes, Yes, YES, n, N, no, No, NO, true, True, TRUE, false, False, FALSE, on, On, ON, off, Off, OFF*
7. I don't know about any other developer but to hell with that noise!  I changed our value from TRUE to 1.  And because I wasn't sure if that would come to me as a value or a string, I ended up with a logical check like this: [1,&quot;1&quot;].include?(ENV['DELETE_TMP_FILE_DAYS']).
8. The next hurdle was that we realized that it was ambiguous where we should be looking for the YAML file.  The backtrace indicated that the YAML file should be found on the box where Capistrano was executing but logically because the post deploy hook executes each node being deployed, it should be on that machine.  And given that there was an 11 minute minimum cycle time per attempt to figure this out, we simply said to heck with it and tested for each directory and read from whichever existed.  And don't even bring up the fact that the exception Ruby throws on an invalid directory isn't Errno::ENOENT but instead a system call exception.  Sheesh.
9. Once we finally got past nothing more than the boolean test of whether or not to run this (remember that's all we've been trying to do so far, isolate a boolean from inside the code base to an environment variable), we found the code failing entirely.  And this time the failure was weird and unclear.  It had the feeling that Capistrano itself was swallowing at least one exception.  Doing a read of the code line by line made is realize that the date time math was using a Rails-ism, 10.days.  And, remember, we aren't in Rails now (even though this is a Rails app), this is a Capistrano post deploy hook so we are really running an an IRB context without ActiveSupport.
10. And that brought in the need to treat this not as logical days but instead as old school Unix epoch math.  So it become a matter of comparing a File.ctime(path_to_file).to_i (for the epoch conversion) against Time.now.to_i - (10 * 86400).  My pairing partner wanted to bring in ActiveSupport for the clarity of the 10.days expression but I was having none of that, ActiveSupport is a beast.
11. With each of these changes, we would make the code better and better, each time and we were finally at a point where the code was actually what even I would call good.  It was well structured, well written and clean.  

## Sidebar: What About Test Coverage

Someone out there reading this is pounding their desk and shouting &quot;YOU SHOULDA WRITTEN TESTS DUMB ASS!!!&quot;.  Now normally I would agree with you but there are a few problems with that:

 * Rake tasks aren't well testable; God bless Jim Weirich but rake tasks have always been an aside to testing
 * There isn't, to my knowledge, a way to test a rake task that is executing under Capistrano and isn't actually part of Rails

## And that Brings Us to The Final Solution 

And with the code finally rewritten and being actually implemented in a very service object pattern, my co worker said &quot;WAIT!  Why are we doing this as a Capistrano post deploy hook anyway?  Why isn't this just a CRON job calling a rake task&quot;  This led to a discussion of the issues around CRON and [RbEnv](https://github.com/rbenv/rbenv) and the need for the DevOps around getting a CRON job installed on N boxes.  And then he said the magic incantation: &quot;Why don't we just use [Ansible](https://www.ansible.com/) for this and execute it regularly via [Jenkins](https://jenkins.io/)?&quot;  Ansible is an outstandingly good DevOps environment and one that I actually taught him.  My initial argument to him against Ansible was because this was fundamentally an imperative task and Ansible was declarative.  His counter was why not just use Ansible to solve this via a [bash shell expression](https://fuzzyblog.io/blog/linux/2019/05/09/linux-tip-find-all-files-older-than-10-days-and-delete-them.html).  I thought about it for a few, and, son of a bitch, he was right.  In about 40 minutes:

* We tossed out all of the Ruby
* We tossed out the entire Capistrano post deploy task
* We tossed out the invocation of the call back (ok I remembered this right now as I wrote this section of the blog post) 
* We spun up an Ansible Playbook and Role that executed a shell one liner and had it running on a dozen plus boxes.  This included researching and testing how to do this with shell.
* We defined a Jenkins scheduled task that ran this every 7 days at an off time using a simple CRON syntax

Here is the before / after on the final Ansible implementation showing the change in our disc space utilization:

### BEFORE

    ok: [194.55.22.38] =&gt;
      msg: 'Actual free disc space: 5.3G'
    ok: [194.55.19.230] =&gt;
      msg: 'Actual free disc space: 5.6G'
    ok: [194.55.23.18] =&gt;
      msg: 'Actual free disc space: 5.6G'
    ok: [194.55.23.10] =&gt;
      msg: 'Actual free disc space: 8.5G'
    ok: [194.55.30.164] =&gt;
      msg: 'Actual free disc space: 8.5G'
    ok: [194.55.20.214] =&gt;
      msg: 'Actual free disc space: 11G'
    ok: [194.55.30.119] =&gt;
      msg: 'Actual free disc space: 8.2G'
    ok: [194.55.29.248] =&gt;
      msg: 'Actual free disc space: 8.7G'
    ok: [194.55.26.185] =&gt;
      msg: 'Actual free disc space: 6.7G'
    ok: [194.55.24.235] =&gt;
      msg: 'Actual free disc space: 6.6G'
    ok: [194.55.30.37] =&gt;
      msg: 'Actual free disc space: 2.3G'
    ok: [194.55.31.216] =&gt;
      msg: 'Actual free disc space: 2.4G'
    ok: [194.55.29.170] =&gt;
      msg: 'Actual free disc space: 11G'
    ok: [194.55.29.107] =&gt;
      msg: 'Actual free disc space: 11G'
  
### AFTER

    ok: [194.55.22.38] =&gt;
      msg: 'Actual free disc space: 5.3G'
    ok: [194.55.19.230] =&gt;
      msg: 'Actual free disc space: 5.8G'
    ok: [194.55.23.18] =&gt;
      msg: 'Actual free disc space: 5.8G'
    ok: [194.55.23.10] =&gt;
      msg: 'Actual free disc space: 8.5G'
    ok: [194.55.30.164] =&gt;
      msg: 'Actual free disc space: 8.5G'
    ok: [194.55.20.214] =&gt;
      msg: 'Actual free disc space: 11G'
    ok: [194.55.30.119] =&gt;
      msg: 'Actual free disc space: 8.3G'
    ok: [194.55.29.248] =&gt;
      msg: 'Actual free disc space: 8.7G'
    ok: [194.55.26.185] =&gt;
      msg: 'Actual free disc space: 6.7G'
    ok: [194.55.24.235] =&gt;
      msg: 'Actual free disc space: 6.6G'
    ok: [194.55.30.37] =&gt;
      msg: 'Actual free disc space: 7.2G'
    ok: [194.55.31.216] =&gt;
      msg: 'Actual free disc space: 7.0G'
    ok: [194.55.29.170] =&gt;
      msg: 'Actual free disc space: 11G'
    ok: [194.55.29.107] =&gt;
      msg: 'Actual free disc space: 11G'

You'll notice that boxes 194.55.30.37 and 194.55.31.216 went from 2 odd gigs of free disc space to 7 odd gigs of disc space.  That's a huge win.  

## Conclusion

The hardest part about debugging anything is when your mental model doesn't match what's actually going on under the hood.  The actual epiphany moment was when I realized that this wasn't running under Rails itself.  After that things started to fall into place.

## Thank Yous

A big shout out to [Sean Kennedy](https;//csphere.github.io/) who did a fantastic job debugging this with me and came up with a great final solution.  Kudos also to various friends who im'd with me over the course of the day and provided moral support on an extremely frustrating day. </description>
        <pubDate>Thu, 09 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2019/05/09/so-you-want-to-delete-temp-files-an-excursion-into-software-engineering-suckitude.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2019/05/09/so-you-want-to-delete-temp-files-an-excursion-into-software-engineering-suckitude.html</guid>
        
        <category>software_engineering</category>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Rails, Migrations, Multiple Developers and the Schema WTF Moment</title>
        <description>I’ve been doing Rails now since 2007.  However for much of that time I was a project lead and one of the tasks that I reserved for myself was *all* schema changes.  One of the benefits of that was that because I did all the primary schema changes, I rarely, if ever, had schema conflicts.  I am now on a project where, while I am a Senior Software Engineer, I am not the lead and the schema changes are done by everyone on the team, all the time.  

The consequence of this is that while Rails should handle any schema conflicts correctly, well, *should* is the operative term.  In practice we have found that depending on how you do your git pull and git branching, you often (like every damn branch you make) end up with a schema conflict.  And since our schema file is enormous (think over 3,000 lines in schema.rb), figuring this out can be brutal.  This moment where you realized that your schema is borked is what I call the *Schema WTF Moment*.

While I’ve struggled with this for a while and never found a decent solution, today, I was speaking with another senior guy (my buddy [Sean Kennedy](https;//csphere.github.io/)) and he very cogently explained the issue in a way that made sense.  This is my write up of his description.  He described it thusly:

“If you do a git pull, you should already have the latest schema, affected by any migrations that came in via the pull, but your database tables may not be updated. So, you do need to run the migrations after pulling, but this will often change db/schema.rb. If all you've done is pull and migrate, there's no reason you should be responsible for committing any of the resultant schema changes as they don't technically belong to you, and they may end up being extraneous/incorrect, so resetting the schema diff makes the most sense.”

To help you understand our approach to git, we use:

1. All work is done in a branch.
2. All branches are created from develop.
3. Once a branch is approved, it is merged into develop.
4. The develop branch is automatically deployed to staging with every merge.
5. A production deploy automatically merges develop into master.
6. The master branch is only deployed to production.

Here is my step by step version of what to do before beginning a branch.

1. Switch your code base over to the develop branch.
2. Pull the latest code.
3. Run bundle exec rake db:migrate.  This updates your schema.rb file locally but you want to throw these changes away before beginning any work.
4. Do a git checkout db/schema.rb to throw away the changes
5. Create your new branch.
6. Change to your new branch and begin work
</description>
        <pubDate>Thu, 09 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2019/05/09/rails-migrations-multiple-developers-and-the-schema-wtf-moment.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2019/05/09/rails-migrations-multiple-developers-and-the-schema-wtf-moment.html</guid>
        
        <category>rails</category>
        
        <category>schema</category>
        
        <category>migration</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Linux Tip - Find All Files Older than 10 Days and Delete Them</title>
        <description>It is easy to forget at times just how powerful simple Linux shell commands here.  Here’s one I used recently that finds all files in a single directory that are older than 25 days and deletes them:

    find /apps/foo/shared/tmp/ -mtime +25 -maxdepth 1 -exec rm {} \;

The -mtime param takes a number of days.  The maxdepth takes the number of directories below this to look (1 means the specified directory).  The exec takes the list of files produced and runs rm on each of them.  For reference, this was discovered from [ServerFault](https://serverfault.com/questions/122824/linux-using-find-to-locate-files-older-than-date/122854#122854).

Just in case you’re curious, I tested this on a system with more than 11,000 files in a single directory and it executed flawlessly — no problems at all and it cleaned up more than 5 gigs of disc space.

And you don’t just have to delete files of course.  If you wanted to say find all the files older than 25 days and list them out with the date and time stamp then you could use:

    find /apps/cas-seas3/shared/tmp/ -mtime +10 -maxdepth 1 -exec ls -al {} \;

This was how we debugged this courtesy of a [Stack Overflow post](https://stackoverflow.com/questions/20893022/how-to-display-modified-date-time-with-find-command/20893429#20893429).
</description>
        <pubDate>Thu, 09 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2019/05/09/linux-tip-find-all-files-older-than-10-days-and-delete-them.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2019/05/09/linux-tip-find-all-files-older-than-10-days-and-delete-them.html</guid>
        
        <category>linux</category>
        
        
        <category>Linux</category>
        
      </item>
    
      <item>
        <title>So You Get Ideas In the Car And Don't Want to Die</title>
        <description>One of my very best friends has a great saying “There’s no dying in xyz” and I firmly agree with her. The topic today is “I get ideas in the car and I forget them / I don’t want to die trying to write them down”.  A friend recently referenced this and this is my approach to this.  I also have a teenage son who is working on his license.  And while I will tell him &quot;phone in glove box while driving&quot;, I'm also realistic enough to know that he might not follow that.  And perhaps this will help him also.

1.  The safest approach is to use your phone's voice recorder.  But the key thing is to move the voice recorder app to your home screen.  It can't be buried on the fifth page of icons. 
2. Put your phone's notepad on your home page.  And then trigger voice to text when you get in the car.  This way if you don't talk, it doesn't type anything.  But it can sit there, waiting patiently for you.
3.  Get a phone mount which attaches to your car's dash.  I used to think these things were absolute crap -- until someone made me get one and then it was like &quot;how the hell did I live without this?&quot;.  Happily the [simplest, cheapest phone mount](https://www.amazon.com/dp/B00WP2A39E?ref=ppx_pop_mob_ap_share) I've seen is also fantastic.  Even if you're not actually typing, you'd be surprised at how much this can help you.
4.  Put a pen and paper in the car.  Technology sometimes fails.  Or we're on a call and we can't switch contexts to use an app.  Often just a jotted down keyword or two is sufficient to capture the memory.
</description>
        <pubDate>Wed, 08 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/phone/2019/05/08/so-you-get-ideas-in-the-car-and-don-t-want-to-die.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/phone/2019/05/08/so-you-get-ideas-in-the-car-and-don-t-want-to-die.html</guid>
        
        <category>phone</category>
        
        <category>writing</category>
        
        
        <category>phone</category>
        
      </item>
    
      <item>
        <title>HTTPS And Your Custom Domain on Github Pages</title>
        <description>Github for sometime now has supported https for *.github.io (aka GitHub pages).  And they have allowed it for custom domains as well.  However I was never able to figure this out until yesterday.  Here was the magic process and well as some references.

My issue was that I had three “sites” hosted on GitHub pages:

* [http://www.fuzzyblog.io/](http://www.fuzzyblog.io/) - my personal home page and resume
* [http://www.fuzzyblog.io/blog/](http://www.fuzzyblog.io/blog/) - my blog
* [http://www.fuzzyblog.io/recipes/](http://www.fuzzyblog.io/recipes/)

I tend to look at my online presence mostly as my blog / recipes.  I had actually forgotten that the root personal home page site even existed.  And the issues preventing me from having https working amounted to this:

 * Changing the A records on my DNS provider (Amazon’s Route 53)
 * Waiting 24 hours for a certificate to be generated
 * Clicking the Enforce Https option on the GitHub Settings for the repo that drives fuzzyblog.io.  This has to be done first
 * Clicking the Enforce Https option on the GitHub Settings for all other GitHub Pages repos (for me this was blog and recipes)

*Note*: If you can’t remember which git repo drives your GitHub Pages, look in the directory and file .git/config. It will be right there; look for a url / .git filename.

*Note*: In case you were wondering why you want https, there are lots of reasons but the best is that it should give your content at least a small boost in page rank.

## Recommended Reading

Here are the recommended tech notes from Github on all this:

* [Supported Custom Domains](https://help.github.com/en/articles/about-supported-custom-domains)
* [Adding Custom Domains](https://help.github.com/en/articles/adding-or-removing-a-custom-domain-for-your-github-pages-site)
* [Troubleshooting Custom Domains](https://help.github.com/en/articles/troubleshooting-custom-domains#https-errors)
* [Press Release](https://github.blog/2018-05-01-github-pages-custom-domains-https/)</description>
        <pubDate>Tue, 07 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/https/2019/05/07/https-and-your-custom-domain-on-github-pages.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/https/2019/05/07/https-and-your-custom-domain-on-github-pages.html</guid>
        
        <category>https</category>
        
        <category>blogging</category>
        
        <category>GitHub</category>
        
        <category>git</category>
        
        
        <category>https</category>
        
      </item>
    
      <item>
        <title>Managing An Attack of Hives</title>
        <description>*Disclaimer:* I am not a doctor.  I am not a nurse.  I do not even play a doctor or a nurse on the Internet.  But I have been unlucky enough to suffer from poison ivy on N (N rounds up pretty high; it involves the number of fingers on both hands) occasions so I know a lot about suffering through itchiness.  And I’ve had hives myself.

I recently watched a dear, dear friend come down with hives out of the clear blue.  One day they were fine and the next day there were “an itchy, splotchy mess”.  Hives are one of those things that generally are an allergic reaction that you never quite understand.  They come over you, your life sucks beyond belief and then they go away.  As another friend would say, “How Rude!”.  

We ultimately figured out a good way to help deal with the itchiness but it took about a day so I thought I’d write down some tips and recommendations.  I know this is not the sort of content I usually produce but it made at least one person’s life better so …

## Managing Hives or Poison Ivy

Here are some techniques for managing hives or poison ivy:

1.  Cool water; never hot.  Getting under a hot shower with an itchy condition actually feels good but it is a complete mistake. It seems to reactivate the underlying cause.
2. [Benedryl](https://www.amazon.com/Benadryl-Ultratabs-Antihistamine-Allergy-Diphenhydramine/dp/B001F1DV72/ref=sr_1_2_sspa?keywords=benedryl&amp;qid=1556977968&amp;s=gateway&amp;sr=8-2-spons&amp;psc=1).  If you’re looking for a medicine and don’t mind the associated drowsiness, Benedryl really does help.
3. [Colloidal Oatmeal Bath](https://www.amazon.com/Aveeno-Soothing-Treatment-Itchy-Irritated/dp/B000UEAARO/ref=sr_1_1?crid=3GVFYJIK6BNCE&amp;keywords=aveeno+oatmeal+bath&amp;qid=1556977411&amp;s=gateway&amp;sprefix=aveeno+ot%2Caps%2C138&amp;sr=8-1).  If you have the ability to soak in a tub, a cool tub filled with the Aveeno Oatmeal works well.  Or you can make your [own](https://www.verywellfamily.com/how-to-make-your-own-oatmeal-bath-289466).
4. [Spray Benadryl](https://www.amazon.com/Benadryl-Relief-Spray-Extra-Strength/dp/B00MHYLOMA/ref=sr_1_1_sspa?crid=FG56ZRO8EATG&amp;keywords=spray+benedryl&amp;qid=1556977467&amp;s=gateway&amp;sprefix=spray+bene%2Caps%2C139&amp;sr=8-1-spons&amp;psc=1&amp;smid=ASEVS99O6FS73).  This is a topical spray and while it makes you sticky, it provides solid relief.
5. [Gold Bond Ultimate Eczema Relief](https://www.amazon.com/Gold-Bond-Ultimate-Eczema-Protectant/dp/B00GIHMOGW/ref=sr_1_5?keywords=gold+bond+eczema+ultimate+relief&amp;qid=1556979092&amp;s=gateway&amp;sr=8-5).  This is a colloidal oatmeal lotion and it has specific ingredients (since its an eczema solution) to “deal with the itches”.
6. [Zanfel](https://www.amazon.com/ZANFEL-Poison-Ivy-Sumac-Wash/dp/B000GCPWUU/ref=sr_1_1_sspa?keywords=zanfel&amp;qid=1556977531&amp;s=gateway&amp;sr=8-1-spons&amp;smid=A188ED3JB77RWM&amp;th=1).  I listed this last but it is ABSOLUTELY THE BEST THING EVER for hives or poison ivy.  It is ridiculously expensive ($50 for a tube in downtown Chicago) but it is completely worth it.  Follow the directions exactly and it provides fantastic relief.

Of the recommendations above, #6, Zanfel, is the best.  You won’t want to spend the money but it really is worth it. #5 also comes very, very highly recommended.
</description>
        <pubDate>Sat, 04 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/health/2019/05/04/managing-an-attack-of-hives.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/health/2019/05/04/managing-an-attack-of-hives.html</guid>
        
        <category>health</category>
        
        <category>hives</category>
        
        
        <category>health</category>
        
      </item>
    
      <item>
        <title>Getting AWS Costs Under Control</title>
        <description>*Note:* AWS is an abbreviation for Amazon Web Services, the Amazon cloud computing platform.

I am presently responsible for a monthly AWS budget that is running about $20,000 per month.  And that is up from about $11,000 per month only eight months ago.  I was recently tasked with bringing that down through *reserved instances* and I thought that writing up the learnings from doing a deep dive into reserved instances and AWS Cost Control was worth writing up.  Even though I’ve worked with AWS now since 2008, I found the results of my research pretty damn surprising.

I should note that when I say “responsible”, I mean that I have technical / implementation responsibility in my role as a Senior Software Engineer. I’m not actually a cost control / accounting person in any way.

## What are Reserved Instances

The first thing to understand is that Reserved Instances are poorly named, very poorly named.  Reserved instances are nothing more than *coupons* for the use of an instance of the type of the coupon.  You actually aren’t reserving a specific instance but instead saying to Amazon “Ok I’ll buy this much of instance type X”.  And it is important to understand that reserved instances aren’t a simple volume discount.  They are:

 * Massively complex and confusing
 * Expensive, requiring an up front payment to get the maximum discount
 * Available in multiple flavors such as convertible and non convertible 
 * Complex enough that after researching them in depth you will want to drink and drink heavily 
 * Such a big deal that there actually is a secondary market where you can resell and purchase reserved instances 

When I looked into how much reserved instances could save us, I generally found between a 15% to 20% savings with our current approach to things.  Given our current spending rate, that could easily amount to nearly $40,000 per year.   And while that’s nothing to sneer at, my research into reserved instances told me to keep digging.

The most interesting thing about reserved instances with respect  to AWS cost control is that universally the recommendation I found was this: *Don’t start with reserved instances*.  Even though reserved instances directly reduce your costs, you should never start with reserved instances.  And that brings us to the next section, AWS Cost Control Strategy.
￼
## AWS Cost Control Strategy

After a considerable amount of research, I came up with the following approach for reducing AWS costs. Please note that where I say “we”, you should substitute your own organization.

1. We need to designate someone as the person responsible for managing / monitoring AWS costs.  Right now, for example, we have a quarter of a million dollar budgetary line item that *no one* is responsible for.  This isn’t a technical problem; this is an accounting problem.
2. We need to correctly tag all instances by name, by product family, by application and more so that there is the metadata to properly examine and drill into costs using the AWS or third party accounting tools.
3. We need to implement a lights on / lights off strategy where you turn off resources when they aren’t in use.  As an example, our staging and training environments clusters of machines only need to be available some of the time (staging needs to be available Mon - Friday, 9 to 6 EST; training needs to be available only when specifically scheduled).  Right now all those environments are powered up and available 24x7x365.  Admittedly these aren’t as many boxes as our production environment but if you don’t have something turned on, you’re not paying for it.
4. We need to implement a Right Sizing strategy where we dig into machine utilization and figure out if we’re actually using the correctly sized instances or if we are over spending by using machines that are too large.
5. We should only implement reserved instances when *all* of the above steps are implemented.  Otherwise you are over buying capacity that you likely don’t need.  Also it is not widely understood that you can buy reserved instances not only for your EC2 boxes but also for your RDS boxes; keep that firmly in mind when you plan your reserved instance purchases.
6. Finally we need to realize that AWS Cost Management is a task you need to do regularly whether that is monthly, quarterly or annually.  It needs to be not only someone’s actual job (or portion of their job) but as an activity, it needs to be done consistently.

## Tools and Recommended Reading

Here are some tools to investigate:

* [AWS Instance Scheduler](https://aws.amazon.com/solutions/instance-scheduler/) - This is a code based tool to configure turn on / turn off of your cloud instances
* [Park My Cloud](http://www.parkmycloud.com) - This is a tool which can turn on / off your cloud instances automatically with an easy user interface
* [CloudHealth](https://www.cloudhealthtech.com) - this is an AWS management and analytics tool with specific features for reserved instance analysis
* [CloudAbility](https://www.cloudability.com) - this is an AWS management and analytics tool with specific features for reserved instance analysis

The CloudAbility and CloudHealth tools are seriously pricey, they are generally priced as a percentage of your AWS bill, but they have outstanding analytics features and the ability  to analyze your reserved instance needs more powerfully than the built in AWS tools for this.

Here are some of the better research materials to investigate:

 * [AWS Cost Optimization Levers, Tools and Strategies](https://www.youtube.com/watch?v=3jhKM1rgpEw).  This is an excellent video from the AWS Australia branch.
 * [Saving at Scale with Reserved Instances by Cloudability](https://www.youtube.com/watch?v=L5rlqaC6uNs).  This is also excellent and it illustrates two key points - a) that reserved instances can actually increase your bill and b) that you really need to have someone responsible for AWS cost control on an ongoing basis 
 * [Reserved Instance Ebook](http://get.cloudability.com/rs/064-CAC-529/images/the-complete-guide-to-saving-with-aws-reserved-instances-ebook.pdf).  A great read.
</description>
        <pubDate>Sat, 04 May 2019 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2019/05/04/getting-aws-costs-under-control.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2019/05/04/getting-aws-costs-under-control.html</guid>
        
        <category>aws</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>New Job Hound Feature - Create Cover Letters More Easily</title>
        <description>When you are a product creator, there is something wonderful about building a product that people are really using and implementing a new feature that you **know** will make their lives easier.  I am very, very please to announce the new Job Hound feature Cover Letters More Easily.  Yes I know that's an absolutely crap name but it does describe what this does.  Let me walk you through how this works.

At the heart of Job Hound is an underlying database of the jobs you apply for and how successful you are with them.  Success is measured by your interactions with the job and Job Hound understands that the type of interaction that you have matters.  Every job has a number of status fields like &quot;got_hr_screen&quot; or &quot;passed_technical_interview&quot;.  As each job gains additional status fields, the underlying &quot;score&quot; associated with how successful you are *for this job* grows.  And that is how Job Hound can do things like generate a report of cover letters organized by most successful to least successful.  

Let's walk through the cover letter creation process as it has been and then let's walk through it with the new features.

## Before

If you want to create a cover letter, you start at your list of Jobs to Apply For:


![cover_letter_all_jobs.png](https://jobhound.io/assets/cover_letter_all_jobs.png)

Let's say that you want to apply for the Eezy job so you click on it giving you the details of the job:

The obvious next step is to click on the Add Cover Letter button:

![https://jobhound.io/assets/cover_letter_eezy.png](https://jobhound.io/assets/cover_letter_eezy.png)

Clicking that Add Cover Letter button takes you into the Cover Letter writer where you can see the job description but you start with a totally blank screen.

![https://jobhound.io/assets/cover_letter_blank.png](https://jobhound.io/assets/cover_letter_blank.png)

**Note**: The Job Description above is something that Job Hound always tries to provide so that you can write your cover letter to respond to the requirements of the job.

Writing anything from a blank state is always harder than starting from a template so let's see how this has been improved.

## With the New Feature

With the new feature you can start creating your cover letter for a job you want to apply for by going to the Successful Cover Letter report (click Report in the menu bar):


![https://jobhound.io/assets/cover_letter_all_reports.png](https://jobhound.io/assets/cover_letter_all_reports.png)


Select the Successful Cover Letters report from the links and you'll see the report of all your cover letters organized from most successful to least successful.  In this case we want to use the Wedding Wire cover letter as a template for the job we want to apply to -- Eezy.  


![https://jobhound.io/assets/cover_letter_select_from_report.png](https://jobhound.io/assets/cover_letter_select_from_report.png)


Just select the Eezy job from the drop down list (yes I realize now that I didn't show Eezy in the drop down; apologies; it is there) and click the Create Cover Letter from this button.  That will take you to the Cover Letter Writer with the selected template loaded in.  You can now adjust the the cover letter for your new job.  You'll notice an alert message at the top that reminds you to make adjustments like the company name and job title.


![https://jobhound.io/assets/cover_letter_create_from_template.pngg](https://jobhound.io/assets/cover_letter_create_from_template.png)

## Conclusion

Now you can easily start from your list of cover letters, choose the one that works best for the job you want to apply to and instantly create a new cover letter from it.  Enjoy!

</description>
        <pubDate>Thu, 03 May 2018 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jobhound/2018/05/03/new-job-hound-feature-cover-letters-from-templates.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jobhound/2018/05/03/new-job-hound-feature-cover-letters-from-templates.html</guid>
        
        <category>jobhound</category>
        
        
        <category>jobhound</category>
        
      </item>
    
      <item>
        <title>Anatomy of an Accidental Product Launch with Metrics - Day 1</title>
        <description>I'm a software engineer and I am decidedly **not** a marketing guy (I play one on the Internet from time to time but I really just write code).  I recently launched a side project, [Job Hound](https://www.jobhound.io), that had an unexpectedly successful first day and I thought documenting it might help someone out there (and certainly help me; that's usually why I blog anything; it helps me understand things).

# What is Job Hound?

Job Hound is a tool that I wrote recently as a way to &quot;make getting a tech job suck less&quot;.  I recently went out to get a full time gig and I found the process [just so plain awful](http://fuzzyblog.io/blog/jobhound/2018/04/24/ten-things-i-learned-from-a-job-hunt-for-a-senior-engineering-role.html) so I wrote software as a way to put myself in control of it.  After finding it incredibly useful for my own needs, and then showing it a few friends to positive feedback, I figured that I'd throw it on a cheap Digital Ocean box and see what happened.

# Timing for Metrics

I'm writing this after the first 16 hours the product was live.  All marketing activities were done around 8 am EST.  There's a bit in the Metrics section that gives the numbers after a full 24 hours but this was initially written at the 16 hour mark and then published a few days later.

# My Launch &quot;Plan&quot;

Last week an article on [Quartz talking about the rise in homework for job seekers](https://work.qz.com/1254663/job-interviews-for-programmers-now-often-come-with-days-of-unpaid-homework/) took off on [Hacker News](https://news.ycombinator.com/item?id=16874015) with 989 comments. 

Here was my launch plan:

**My friend [Nick](https://nickjanetakis.com/blog/)**:  You should document your story on your struggle to find a job and then post it on Hacker News

**Me**: Ok.

Honestly there wasn't more to it than that.  This isn't a humblebrag by any means.  Someone I respect suggested it and, well, that is how it started.

I had about 500 words of a very crufty blog post in draft form right in my coding editor so I polished that up from 5 am to 7 am and that became my [Ten Things I Learned from a Job Hunt for a Senior Engineering Role](http://fuzzyblog.io/blog/jobhound/2018/04/24/ten-things-i-learned-from-a-job-hunt-for-a-senior-engineering-role.html) blog post.  I also submitted it to [Hacker News](https://news.ycombinator.com/item?id=16912546#16913834) where over the course of the next 16 hours, it got 638 comments.  

I should point out that this is the first thing I've ever submitted to Hacker News that has gotten **any** traction at all.

I did tweet my blog post on [my personal twitter](https://twiiter.com/fuzzygroup/) and I also made a Job Hound specific tweet and then pinned the Job Hound tweet so it stayed at the top of my twitter feed.

I honestly never even thought to put my blog post on Facebook.  I did that 24 hours later so I don't have any metrics on that.

My final launch action, one I had no hope for at all, was that I also tossed Job Hound up on [Product Hunt](https://www.producthunt.com/posts/job-hound) where it got 56 up votes during roughly the same period (it may have been on Product Hunt 1 day longer; the whole day is kind of a blur right now).

# Fine Tuning

I did relatively little tuning of the content.  My buddy Nick suggested that I emphasize the free nature of Job Hound in the blog post and that I add the Product Hunt link into the blog post.  Both of these were good ideas and done around 2 pm EST.

# The Metrics

Since roughly 8 am this morning (sixteen hours so far), Job Hound has had:

* 204 user accounts created (email / password)
* 98 jobs added
* An average of 0.48 jobs per user

After a full 24 hours, the numbers were:

* 236 user accounts created (email / password)
* 119 jobs added 
* An average of 0.50 jobs per user 

In terms of social media metrics, I started the day with 163 twitter followers and I ended it a full 24 hours later with 187.

Are these numbers good?  I think that absolutely depends on your perspective.  For a side project that I publicly launched with absolutely zero planning -- I'm absolutely delighted.

**Followup after 4 days**: The numbers have continued to steadily grow and I've gotten quite a few emails, linked in messages, tweets and other feedback that all indicates to me that I have hit on something.  My plan is that keep working Job Hound as a side project and just making it better and better.  Maybe something comes of this; maybe not but helping people get jobs is a **good** thing.

# Other

I received a number of emails and messages via LinkedIn and I tried to respond to everyone on the same day.  I've always found that talking to users is the best damn marketing you can do.  This is the root of the [Job Hound Thank You](https://www.jobhound.io/pages/thank_you) page.  

You're probably wondering &quot;what about the web traffic?&quot;  I'm sorry to say that my Google Analytics skills just aren't all that good so I haven't been able to dig into them with any success as of yet.  I'm sure the data is there but I'm too brain dead to figure it out.

# Conclusion - Content Does Work

I think my overall conclusions from all this are:

* Creating content on the Internet as a marketing tool really does work but the content has to have value of its own.  You can't simply shill your product; embed it in something that is interesting all on its own.
* Telling a personal story works.
* Lucky is better than smart.  I was extremely lucky in getting coverage from Hacker News.
* Timing matters.  I was lucky to be able to ride a wave of hiring interest from the Quartz article.  This didn't occur to me and it took the pushing of a good friend for me to put this out.  Thanks Nick.

</description>
        <pubDate>Fri, 27 Apr 2018 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jobhound/2018/04/27/anatomy-of-an-accidental-product-launch-with-metrics-day-1.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jobhound/2018/04/27/anatomy-of-an-accidental-product-launch-with-metrics-day-1.html</guid>
        
        <category>jobhound</category>
        
        <category>startup</category>
        
        <category>metrics</category>
        
        
        <category>jobhound</category>
        
      </item>
    
      <item>
        <title>Ten Things I Learned from a Job Hunt for a Senior Engineering Role</title>
        <description>I recently went on a search for a new full time gig and I learned that today's job search is **not** what it used to be.  Here's the quick summary of what I learned:

1.  The job search takes much, much longer than it used to.
2.  No one believes that anyone can actually code.
3.  Coding Tests Can Trip Up Even Good Engineers
4.  Extensive homework is now normal.
5.  Every company's &quot;process&quot; is different
6.  Outsourced hiring &quot;services&quot; are very much in vogue
7.  Companies Really Want to Know Your Salary; Don't tell Them
8.  Interviews Matter Much, Much More to You Than to the Company
9.  Age discrimination really exists.
10. You'll never really know why you weren't hired.


In terms of full disclosure, at the end of this article, I'm going to pitch you on a [new tool](https://www.jobhound.io) I built for managing your personal job hunt, but don't worry, it's **free** to use.  I found the job hunt process so **absolutely distasteful** that the only way I could get through it was to actually create some software based on what I learned.

# Who Am I?

Here's just a bit about myself so you can understand my job search.  I'm a senior software guy who's still writing code after all these years.  My platform is generally Ruby / Rails and my [resume](http://fuzzyblog.io/blog/resume.html) is here in case you care.  I've been a consultant for the last few years and I wanted to get back to a full time gig in a senior engineering role. 

Here are the stats from my job search:

* Total Jobs Applied For: **82**
* Total Jobs Where You Got an HR Interview	**25**
* Total Jobs Where You Got a Technical Interview:	**15**
* Total Jobs Where You Got an Onsite Interview: **2**
* Total Job Offers: **1**

# Number 1: It Takes Longer than Ever to Get Hired

I've been in high tech a long time and other than the few really bad times, the general rule in high tech hiring has been &quot;fill an engineering slot as fast as possible&quot;.  The industry may talk a good game about culture fit but other than a few firms, it always has been about quickly putting bodies in seats.  That is absolutely no longer the case.  HR departments now seem to be focused on:

1.  Initially getting a large number of candidates to review as the first step (and nothing, even when you seem to be a perfect requirement match, happens until they get that large group of candidates).
2.  Marching the entire group of candidates, step by step, through their entire process.
3.  Putting as many barriers in place before hiring anyone.  I discuss this more below under process.

Just so you're aware, the HR departments will not own up to the process taking longer than ever.  They will spin you a song and a dance about how quickly things are going to progress and it just isn't true.  My two observations in this area are:

* Companies today, despite what they might say, are pickier about candidates than ever
* Companies are less willing to hire than I've ever seen

One helpful thing I did during my job hunt was track everything so I actually had dates all along.  Here's an example of what I commonly saw.

* 2/5 was my initial application to Company W
* 3/1 was when I completed my homework project 
* 3/2 was when I got scheduled for an onsite interview
* 3/23 was when I found out that I didn't get it (two weeks after my onsite)

That's almost two months start to finish!  And today that job remains on the company's web site -- they still haven't hired anyone.

# Number 2: No One Believes Anyone Can Actually Code

The general thing that I saw from everyone that I talked to is that no one actually believes that anyone can code.  At every stage in the interview process, you're going to need to **prove** that your resume isn't a lie and that you can actually do what you say you can.  I find this ludicrous because it is like interviewing an attorney and then saying to him &quot;Please prove that you can cite a Supreme Court precedent&quot;.  When did an entire industry of people get pre-judged as lying?   Don't we exist in a country where the default assumption is innocence not guilt?

# Number 3: Coding Tests Can Trip Up Even Good Engineers

An extremely common thing that you're going to find as part of a job search are coding tests and I'd urge you NOT to take them lightly.  These coding tests tend to bear **no resemblance** to anything you'll do in the work place and even good engineers can get tripped up by them.  I found that part way through my job search I had to buckle down, hit the books and start practicing on different coding tests to get my scores up.  Once upon a time I would have said that any engineer should be able to pass a coding test but that's just not true.  The longer you are in industry, I would argue the less likely you may be to pass a coding test.  I suspect I could have done much better on a coding test fresh out of college when my &quot;test taking skills&quot; were actually something I used.  Now?  The last test I passed was for a driver's license.

**Tip:** You should note that since most tests come from an outsourced service like Codility, you can google for practice questions, sample exams and more.  And I'd strongly encourage you to do that.  

**Tip:** Don't underestimate coding tests at all.  I did and it was to my disadvantage.  You really, really need to practice.

# Number 4: Extensive Homework is Now Normal

As a hiring manager, I've given homework assignments to weed out candidates from the pack but I always kept it reasonable, just a few hours in length.  I've now regularly seen homework assignments ranging from 8 hours to &quot;spend a week on this and really show us what you can do&quot;.  Over on [QZ](https://work.qz.com/1254663/job-interviews-for-programmers-now-often-come-with-days-of-unpaid-homework/) there is an excellent article about this along with [984 comments on Hacker News](https://news.ycombinator.com/item?id=16874015).  And yes that's a huge amount of comments on any one topic since the current amount of comments on the top 30 items on Hacker News is **53** comments.

You should be aware that your coding submission will be judged all kinds of ways, often unfairly.  For one application I built, I was told &quot;you didn't allow for proper performance given a 500x increase in data&quot;. This came after being specifically told &quot;use no technologies other than the database itself&quot; (it was a search application). And what good engineer allows for an arbitrary 500x increase?  That is massive, unnecessary over engineering.

**Tip:** One thing that I did do to stand out from the crowd for almost all my homework assignments was to record a screencast talking through my work and illustrating what I thought were the best points.  I had several interviewers tell me that &quot;thank you; I've never seen that before&quot;.  This gets the interviewer past any issues with getting code installed / setup, etc.

I should comment that even though I don't like the trend towards homework, I vastly prefer it to coding tests.  You shouldn't think, however, that just because you passed the homework assignment, you won't face a whiteboard challenge - every place I did a home assignment for, I also got a whiteboard challenge, sometimes multiple ones.

# Number 5: Every Company's Process Is Different

It used to be that getting a coding job boiled down to:

* Send in resume
* HR Screen
* Technical Interview
* Team Interview

This was a pretty standard process and I saw (and hired) a lot of people with this approach.  Apparently though times change and now the &quot;process&quot; is all over the map.  One of the most useful things I learned to ask companies early on was &quot;Please explain your hiring process to me&quot; and then I wrote it down for each company.  This let me know &quot;for company X, the process is&quot;:

* HR Screen
* Coding Test
* Coding Project
* Technical Interview
* Pair Programming
* Technical Interview with the Team
* Onsite Interview

What you're going to find is that while there are common elements (currently homework) in the hiring process, everyone's process is different.  If you know what a company cares about then you can start to get ready before it happens.  For example, if you know in advance that you have a coding test in front of you, this lets you know that you may need to study.  If an HR person says to me &quot;Oh and we administer a coding test&quot; then my first response is &quot;I've taken a bunch of those, which one do you use?&quot;.  

# Number 6: Outsourced Hiring Assessment Services Are Very Much in Vogue

One thing that really surprised me was how many companies rely on outsourced hiring services.  I saw this all over the place from companies that sent me a link to a coding test, often [Codility](https://www.codility.com/), to &quot;personality assessment&quot; tools, often [Predictive Index](www.predictiveindex.com).  I suppose this is the natural evolution of SAAS companies into the hiring space.  When I was recruiting, in years past, my HR department always strongly warned me against administering coding tests on the grounds of legal issues / fairness.  Now that these are outsourced, I suspect that HR departments aren't concerned at all about legal issues since they come from a vendor not themselves.  

One thing that you'll find is that these outsourced assessment services are being stuck into the hiring process at **ANY** point in the process.  I've seen things ranging from HR screen to initial technical interview to coding test to send in a resume and get a personality test fired back at you before you ever talk to a human being.

# Number 7: Companies Really Want to Know Your Salary; Don't tell Them

Before I started my job search, I was lucky enough to read Josh Doody's fabulous book [Fearless Salary Negotiation](https://fearlesssalarynegotiation.com/) and the best thing I got from it was &quot;Never disclose your current salary or expectations&quot;.  I never disclosed it but I found HR people absolutely relentless on this point.  I finally said to one California company &quot;You do realize that legally you can't ask me that&quot; and they shamelessly replied &quot;Yes I can; you aren't in California&quot;.  And you'll find that there are web forms which require you to fill it in (I always put in a minimal, crazy number like 1234).

Oh and if you haven't read Josh's book then I strongly encourage you to do so.  It is a short but fantastic read.  [Here's an Amazon link](https://www.amazon.com/Fearless-Salary-Negotiation-step-step/dp/0692568689/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1524566767&amp;sr=1-1&amp;keywords=fearless+salary+negotiation).

# Number 8: Interviews Matter Much, Much More to You Than to the Company

Unfortunately it is entirely true that any given interview matters far more **to you** than it does **to the company** that is interviewing you.  Any candidate has only a relatively small number of jobs they can even apply for where as the company has all of the Internet applying (this is particularly true if the company is any good).  I couldn't even count how many interviews companies missed, were late on, etc.  

# Number 9: Age Discrimination is Real

**Disclaimer**: I recently turned 50 and I'm still a hands on engineer who writes code just about every single day.

One very surprising thing to me was just how much I discovered that age discrimination in high tech hiring is real.  There were all too many times when jobs that I was an absolute perfect match for:

* I never got an interview at all
* I was mysteriously ghosted midway though the process 
* I got all the way to the end, nailed everything perfectly and then never found out why I didn't get it

This is one of those things that you never can prove and no one will ever own up to I do think that is very, very real.  

# Number 10: You'll Never Really Know Why You Weren't Hired

I know that there are lots of actually good reasons why companies don't want to tell you why you weren't hired but this remains my single biggest issue with today's job hunt.  I can understand that you can't tell everyone why they didn't get hired but when you get to the end point and pass all these hurdles:

* HR Screen
* Coding Test
* Coding Project
* Technical Interview
* Pair Programming
* Technical Interview with the Team
* Onsite Interview
 
You would think that more information than &quot;We all really liked you but we went in a different direction&quot; could be said.  I saw this repeatedly on jobs that I got all the way to the end point on.  How is a candidate ever supposed to improve themselves when the company will not tell you why you didn't get the job?

# And Here's the Pitch for Job Hound

The end result of all this was that while I did end up with a full time job, I also ended up creating a new **free** product, [Job Hound](https://www.jobhound.io) to make applying for tech jobs (generally programming) easier.  Job Hound is a job application tracking system and it puts the job application firmly in charge of the process.  I built it because after I had applied for about 15 jobs, I had no idea where I was in the process and I couldn't give my wife a decent answer about my progress because I simply had no idea.  Job Hound isn't a SAAS product, it is 100% free and I'd encourage you to check it out.

[My new free product is Job Hound: Make applying for tech jobs suck less!  Sign Up Today.](https://www.jobhound.io)

Also here is [Job Hound on Product Hunt](https://www.producthunt.com/posts/job-hound) in case you want to toss me an upvote.  Thanks in advance.

# Conclusion and What this Means for the Industry and the Job Seeker

I wouldn't underestimate these changes to the hiring process and their impact on the industry as a whole.  Here's how I see things changing:

* **Decreased job mobility**.  If jobs are harder to get then engineers will be less willing to change roles than they have been in the past.
* **Lower salaries**.  Generally speaking the easiest way to get a raise as an engineer is to leave your job.  If jobs are harder to get then less people will leave and that will drive salaries down.
* **Less Cross Pollination of Ideas**.  While companies may not like job mobility, I would argue that on the whole it has been good for high tech in general.  By engineers moving freely across companies, we've seen a great dissemination of engineering techniques and approaches.  If engineers lose job mobility then that will absolutely change.

Given how high tech has been such a tool for job creation over the past few decades, I really don't like the implication of these trends but I do see them as very, very real.

</description>
        <pubDate>Tue, 24 Apr 2018 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jobhound/2018/04/24/ten-things-i-learned-from-a-job-hunt-for-a-senior-engineering-role.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jobhound/2018/04/24/ten-things-i-learned-from-a-job-hunt-for-a-senior-engineering-role.html</guid>
        
        <category>jobhound</category>
        
        <category>jobs</category>
        
        
        <category>jobhound</category>
        
      </item>
    
      <item>
        <title>Using Elastic Search and Rails for a Compound Query with Text Strings and User ID</title>
        <description>I'm writing this blog post, my first in quite a while, because I recently had to implement [ElasticSearch for Rails](https://github.com/elastic/elasticsearch-rails) for a [new application I'm building](https://www.jobhound.io) and I find life in the Elastic world a bit different than I had expected.  If you're an Elastic Search veteran then you should definitely move along because:

* This is fairly basic
* I'm writing this mostly to cement this in my own brain

Curiously I found very few examples about how to do this type of compound query in Rails and that's also part of my motivation for writing it.  The closest example I found was in a [four year old Stack Overflow post](https://stackoverflow.com/questions/19381199/how-to-perform-elasticsearch-query-on-records-from-only-a-certain-user-rails-ti/49900343#49900343).

# The Problem: Everyone Should See Only Their Own Data

I have a series of ActiveRecord models that I want to be able to search using Elastic Search.  This can easily be done with this code fragment:

    Job.search_user(params[:q])

Given that I was initially the only user on this code base, I didn't even notice the issue until I thought about deployment.  At which point there was the obligatory light bulb / I'm an idiot moment.  The problem here is that this code searches everyone's jobs, not just the jobs that **you** created.  Now since every bit of data encompasses a user_id attribute, this should boil down to two problems:

1.  Getting user_id into the index
2.  Constructing a JSON query for ElasticSearch to execute

# Sidebar: But What About SearchKick

I'm sure a number of people are shouting out &quot;Use [SearchKick](https://github.com/ankane/searchkick) (dummy)&quot; but whenever I tried to use SearchKick, I got odd errors and I eventually just pulled it from Gemfile.  Given that I'm a long time believer in Ankane's ChartKick, I'm sure it is me but I still couldn't make use of it.

# Step 1: Getting user_id into the Index

I've opted to setup each of my searchable models as follows:

    include Elasticsearch::Model
    include Elasticsearch::Model::Callbacks
    
    settings do
      mappings dynamic: false do
        indexes :company^5, type: :text, analyzer: :english
        indexes :title^5, type: :text, analyzer: :english
        indexes :why_rejected, type: :text, analyzer: :english
        indexes :location, type: :text, analyzer: :english
        indexes :name, type: :text, analyzer: :english
        indexes :domain, type: :text, analyzer: :english
        indexes :created_at, type: :date
        indexes :user_id, type: :text
      end
    end

The :title^5 bumps the ranking of search results in the title or company attributes by a factor of 5.  

# Step 2: Constructing the JSON query

I ended up with a class method on each of my searchable objects like this:

    def self.search_user(query, user)
      self.search({
        query: {
          bool: {
            must: [
            {
              multi_match: {
                query: query,
                fields: [:company, :title, :description, :name]
              }
            },
            {
              match: {
                user_id: user.id.to_s
              }
            }]
          }
        }
      })
    end

I can call this from my search controller like this:

    @jobs = Job.search_user(params[:q], current_user)

and get results bound only to the logged in user -- exactly what I was looking for.

# Step 3: Re-indexing Everything

After making changes to your settings / mappings, you need to re-index everything.  I handle this with a simple rake task like this:

    namespace :search do
      # bundle exec rake search:index_all --trace
      task :index_all =&gt; :environment do
        klasses = [Job, Note, CoverLetter, Task]
        klasses.each do |klass|
          klass.send(:import, :force =&gt; true)
        end
      end
    end

This re-indexes every model in full.  You don't always need this but it is very convenient to have for development when you're playing with schema changes and the like.

# Recommended Reading

I made heavy use of [Iridakos's excellent tutorial](https://iridakos.com/tutorials/2017/12/03/elasticsearch-and-rails-tutorial.html) and I recommend you do too.</description>
        <pubDate>Thu, 19 Apr 2018 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2018/04/19/using-elastic-search-and-rails-for-a-compound-query-with-text-strings-and-user-id.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2018/04/19/using-elastic-search-and-rails-for-a-compound-query-with-text-strings-and-user-id.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>elastic_search</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Converting from FactoryGirl to FactoryBot</title>
        <description>This blog post talks about the renaming of FactoryGirl to FactoryBoy  and the necessary refactoring that you have to do to a current Rails app to support it.  The official guide to converting your Rails from [FactoryGirl to FactoryBot](https://github.com/thoughtbot/factory_bot/blob/4-9-0-stable/UPGRADE_FROM_FACTORY_GIRL.md) is here.  Their solution is a search and replace approach and while that works, I wanted to go thru this step by step.

## Step 0: Update Your Gemfile

You need to change your Gemfile from FactoryGirl to FactoryBot:

    gem &quot;factory_bot_rails&quot;

## Step 1: spec/support/factory_girl.rb

This file needs to be renamed from spec/support/factory_girl.rb to spec/support/factory_bot.rb and then the contents changed to reflect it:

    RSpec.configure do |config|
      config.include FactoryBot::Syntax::Methods
    end
    
## Step 2: spec/rails_helper.rb

Depending on your RSpec configuration you may not need to adjust this file.  In my case I was using FactoryGirl to create an @user variable so I had to adjust this as:

    @user = FactoryBot.create(:user, options)    

## Step 3: spec/factories/*.rb

Each of your factories needs to be rewritten to call FactoryBot not FactoryGirl so you'll have a structure something like this:

    FactoryBot.define do
      factory :user do
        email { Faker::Internet.email }
        username Faker::Name.first_name.downcase
        password &quot;Sample:1&quot;
        password_confirmation &quot;Sample:1&quot;
        first_name Faker::Name.first_name
        last_name Faker::Name.last_name
        active true
        confirmed true
      end
    end    

## Step 4: Your Actual Spec Files 

In each of your spec files, you'll need to change calls from FactoryGirl.create to FactoryBot.create like this:

    @job = FactoryBot.create(:job, :user_id =&gt; User.first.id)

## Step 5: Also Check spec/spec_helper.rb

You should also check spec/spec_helper.rb as this is a common place where you may have located some Factory* type of stuff.

## Conclusion

Overall this was a simpler conversion than I had expected.  Admittedly this was done in a relatively small [Rails MVP](https://www.jobhound.io) but it really was pretty painless.</description>
        <pubDate>Thu, 19 Apr 2018 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2018/04/19/converting-from-factorygirl-to-factorybot.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2018/04/19/converting-from-factorygirl-to-factorybot.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>testing</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Enabling Github Security Alerts on Your Private Repositories</title>
        <description>Github recently announced that they would be providing [security alerts for github repositories](https://github.com/blog/2470-introducing-security-alerts-on-github).  These alerts let you know when a dependency that you're relying on has a critical vulnerability.  Given the importance of security, this is an excellent idea.  Thank you Github!  These alerts are turned on automatically for public repositories but they require you to opt in for your private repositories.  And while this isn't hard, Github doesn't make it clear exactly how to set this option for your private repositories.  I just spent an hour or so turning this on for all my private repositories and here's the step by step approach:

1.  Go to [github](https://www.github.com).
2.  Login if you're not logged in.
3.  Navigate to your dashboard.
4.  Go to your repositories list by clicking on the repositories link in the header.
5.  Select a private repository by clicking on it.
6.  Select the Settings link.
7.  Scroll down to **Data services**.
8.  Turn on &quot;Allow Github to perform read-only analysis of this repository&quot;.
9.  Turn on Dependency graph.  
10. Turn on Vulnerability alerts.  You should note that Github saves the status of each of these via ajax as you check them off so there's no Save button you need to click.

Now you need to navigate back to repositories and turn this on for any other private repositories that you have. Given the number of private Rails apps I have repositories for, I'm really, really glad I turned this on.

Note: Its unclear exactly how long it takes to build the Dependency graph and for the Vulnerability alert scan to be completed.  I don't think it is immediate so you might want to keep an eye on the home page for your repos over the next few days.
</description>
        <pubDate>Fri, 17 Nov 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/github/2017/11/17/enabling-github-security-alerts-on-your-private-repositories.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/github/2017/11/17/enabling-github-security-alerts-on-your-private-repositories.html</guid>
        
        <category>github</category>
        
        <category>git</category>
        
        <category>security</category>
        
        <category>rails</category>
        
        
        <category>github</category>
        
      </item>
    
      <item>
        <title>Forensic Computing 2 - Kernel Panics and Kexts</title>
        <description>As I've noted in [other places](http://fuzzyblog.io/blog/text/2017/10/08/forensic-computing-1-finding-textmate-untitled-documents.html), my Mac crashes a lot.  For some time now I've just sort of accepted this as a consequence of how I do things.  I'm a software developer and I tend to push the limits of everything.  Part of the reason that I accept it is that my computing usage is more than a bit non standard and I suspect that I'm running up against nothing more than resource limits.  Just as an example, at the start of my day, my terminal count looks something like this: 

    ps auwwx | grep bash | wc -l
          50
          
That's at the start of my computing day (I open a lot of terminals and generally leave them running).  Just as a side note, my desktop Hackintosh, with double the RAM, also running OSX is pretty much rock solid so I tend to view this as a resource issue.  But what if it is not?  What if this is a solvable software issue?

# Analyzing Kernel Panics

In order to figure this out, I took a number of my kernel panics and put them online as gists:

* [October 8th](https://gist.github.com/fuzzygroup/ab3a4c81edb3fa80910e25fc12e49993).  Yes this one happened while I was writing this all down.  Damn it.  
* [October 7th](https://gist.github.com/fuzzygroup/97bc867094294a2b724f5860d04d09ab)
* [September 30th](https://gist.github.com/fuzzygroup/30ee32761f72d55d1fe8b870dbdc04af)
* [August 27](https://gist.github.com/fuzzygroup/137e695b9ed5cc2d2ebcda8c983616a4)
* [September 11](https://gist.github.com/fuzzygroup/5e41b8ccdcdf6c8bf95bbe002aa158

As I've been reading these crash by crash, I always attributed it to randomness since there was no consistent application that seemed to cause the crash (WindowServer was the most common but by no means the sole thing).  But there is a lot of information in these reports and the application when it crashed perhaps isn't the only thing I should be caring about.

I noticed one interesting thing in going thru the above kernel panics (oh and by the way, you can find many of these logs in /Library/Logs/DiagnosticReports/).  They are all named something like Kernel_2017-09-16-132730_FuzzygroupMacbookPro2016.panic.  [Thanks Apple](https://support.apple.com/en-us/TS3742).  What I noticed was that the kext (Kernel Extension) that always is last loaded is com.github.kbfuse.filesystems.kbfuse which belongs to KeyBase.  This is a part of [Keybase](https://keybase.io/) which is a very cool crypto app.  This extension is apparently part of the new [Keybase Filesystem](https://keybase.io/docs/kbfs).

A kext is a kernel extension and that basically means &quot;code that extends the lowest level of the operating system (the kernel)&quot;.  Fuse is a toolkit that lets you write filesystems in user space and what I do remember about the Fuse project is that I've seen a lot of flakiness / crashes over the years from Fuse related stuff.  

A quick google for the term [osx kernel panic fuse](https://www.google.com/search?tbs=li:1&amp;q=osx+fuse+kernel+panic) gave me 49,100 results so I'm not the only one thinking this.  Googling instead for (com.github.kbfuse.filesystems.kbfuse)[https://www.google.com/search?tbs=li:1&amp;q=com.github.kbfuse.filesystems.kbfuse] took me to the github page for the [keybase fuse project](https://github.com/keybase/client/tree/master/osx/Fuse) where I found the following uninstallation directions:

    // Check for any mounts (if there are you need to umount)
    mount -t kbfuse

    sudo kextunload -b com.github.kbfuse.filesystems.kbfuse
    sudo rm -rf /Library/Filesystems/kbfuse.fs
    
# What Next?

I've done these steps and now I need to just wait and see if my system stability improves.  I also deleted the Keybase application entirely because its unclear to me what happens to Keybase if I delete a kext that it is using.

In closing, I'd like to add that I am not the only person who is reporting an issue with this kext and kernel panics.  Here's an [open Github thread](https://github.com/keybase/client/issues/5190) on it.

# Update

As of 24 hours later, my Mac has crashed two more times so the kext apparently had nothing to do with it, alas.  </description>
        <pubDate>Sun, 08 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/10/08/forensic-computing-2-kernel-panics-and-kexts.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/10/08/forensic-computing-2-kernel-panics-and-kexts.html</guid>
        
        <category>osx</category>
        
        <category>kernel_panics</category>
        
        <category>kexts</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Forensic Computing 1 - Finding TextMate Untitled Documents</title>
        <description>So my Mac crashed again last night.  This is now roughly a two to three times per week occurrence.  Once upon a time I would have sworn like a sailor about this but I now, sadly, accept it.  If you're curious about OSX crashes and kernel panics, I dig into [them here](http://fuzzyblog.io/blog/osx/2017/10/08/forensic-computing-2-kernel-panics-and-kexts.html).

[TextMate](http://macromates.com/) is the programming editor and writing tool I use and normally after a crash, it automatically opens all the documents I previously had open.  And when I say &quot;all&quot;, I mean everything, even the things that weren't ever formally saved i.e. &quot;untitled 107&quot;, &quot;untitled 132&quot;, etc.  Because TextMate doesn't force met to assign a file, I'm able to treat TextMate windows as virtual pieces of paper, virtual post-its if you will and just drop stuff into them.  I only bother to save a file when I actually need to make it long term persistent.  And an awful lot of the time, that never happens.  Sometimes these the contents of these windows flows into a [gist](https://gist.github.com/) which then gets emailed to someone.

Now this bizarre &quot;don't bother to save the file&quot; approach is well and good until it **breaks**.  Last night, just before my machine crashed, I know for a fact that I was in **untitled 30**.  But when I restarted my machine, I noticed that untitled 30 wasn't in my Window list:

![/blog/textmate_window_list.png](/blog/textmate_window_list.png)

This literally sent a chill down my spine -- if untitled 30 isn't present then:

1. Where did it go?
2. How often has this happened?
3. What other untitled xyz files have I unknowingly lost?
4. Where on the blankety, blank computer is untitled 30 stored so I can bloody well find it?

Questions 1 and 2 simply cannot be answered but questions 3 and 4 are actually answerable.  If I can simply find out where TextMate stores its untitled files then I can browse the directory and find them.  I should also then be able to find untitled 30.  

# Step 1: Google

Whenever you are trying to understand something like this Google is generally your first place to look.  Unfortunately these googles didn't really yield anything:

* [where does textmate store untitled files](https://www.google.com/search?tbs=li:1&amp;q=where+does+textmate+store+untitled+files)
* [textmate untitled](https://www.google.com/search?tbs=li:1&amp;q=where+does+textmate+store+untitled+files)

# Step 2: Spotlight

OSX's Spotlight, the system level search engine was next.  This too yielded nothing, squat, [bupkus](http://www.urbandictionary.com/define.php?term=bupkus).   

# Step 3: Alfred

The filename search engine in [Alfred](https://www.alfredapp.com/) which you trigger with your Alfred hotkey plus the keyword open and then what you want to search yielded nothing.

# Step 4: mdfind

Another way to use Spotlight is to trigger the mdfind utility from the command line and look for the filename untitled 30 directly by typing:

&gt; mdfind &quot;untitled&quot;

The output of mdfind is a list of filenames.  But, unfortunately, mdfind isn't, by default, for filenames which means that it finds the word untitled in the *body of documents* where as we want it in the filename itself. Just to be sure we can feed the mdfind's output thru grep to winnow it down:

&gt; mdfind untitled | grep untitled

There is a -name option to mdfind but that also returned nothing:

&gt; mdfind -name untitled

**Cool as Hell Sidenote**: In looking up [details on mdfind](http://osxdaily.com/2017/08/24/find-all-screenshots-mac/), I found this very, very cool command line:

&gt; mdfind kMDItemIsScreenCapture:1

What that does is find every screenshot you've taken on your Mac.  And I don't mean files named &quot;Screen Shot xyz&quot; but any screenshot you've ever taken, even ones you've moved to other directories, renamed, etc.  Wow.

# Step 5: Look Elsewhere

Now the absence of results in Steps 2, 3 and 4 gave me a strong indication that what I was looking for wasn't in an indexed location and that gave me a clue.  Spotlight  and mdfind are both generally looking in the files for the user but NOT in the application level files.  OSX has a key directory structure for users that generally isn't indexed: 

&gt; /Users/sjohnson/Library/Application Support/

And I know for a fact there is a TextMate directory below it:

&gt; /Users/sjohnson/Library/Application Support/TextMate/

What I didn't know was what was below the TextMate directory:

    ls -l &quot;/Users/sjohnson/Library/Application Support/TextMate/&quot;
    total 6488
    drwxr-xr-x   10 sjohnson  staff   340B Jan  1  2017 Bundles/
    -rw-r--r--    1 sjohnson  staff   732K Oct  8 05:40 ClipboardHistory.db
    -rw-r--r--    1 sjohnson  staff    32K Oct  8 05:40 ClipboardHistory.db-shm
    -rw-r--r--    1 sjohnson  staff   1.7M Oct  8 07:12 ClipboardHistory.db-wal
    -rw-r--r--    1 sjohnson  staff   1.1K Sep 30 08:46 Global.tmProperties
    drwxr-xr-x    5 sjohnson  staff   170B Oct  7 06:31 Managed/
    -rw-r--r--    1 sjohnson  staff   688K Oct  8 05:51 RecentProjects.db
    drwxr-xr-x    4 sjohnson  staff   136B Nov 20  2016 Ruby/
    drwxr-xr-x  156 sjohnson  staff   5.2K Oct  8 07:12 Session/
    
And, yep, **bingo**, *Session/*.  A quick look there showed me all my untitled documents including: 

    ls -l /Users/sjohnson/Library/Application\ Support/TextMate/Session/untitled\ 30*
    
    -rw-r--r--@ 1 sjohnson  staff   5.3K Sep 13 04:23 /Users/sjohnson/Library/Application Support/TextMate/Session/untitled 30.txt
    
And now that I have a path I can just type open &quot;/Users/sjohnson/Library/Application\ Support/TextMate/Session&quot; and get a Finder window of all my untitled documents like this:

![text_mate_session.png](/blog/assets/text_mate_session.png)</description>
        <pubDate>Sun, 08 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/text/2017/10/08/forensic-computing-1-finding-textmate-untitled-documents.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/text/2017/10/08/forensic-computing-1-finding-textmate-untitled-documents.html</guid>
        
        <category>textmate</category>
        
        <category>computer_forensics</category>
        
        
        <category>text</category>
        
      </item>
    
      <item>
        <title>The Game Nanny Story Or When Life Gives You Lemons</title>
        <description>I have built a lot of products in my career:

* search engines
* blogging tools
* podcasting tools
* online learning systems
* hypertext authoring tools
* email marketing aka spamming tools
* Even a web UI to calculate radiation doses for prostate cancer

But the one thing that I've stayed resolutely way from is *gaming*.  And, despite that, three days ago, I launched a product named **Game Nanny** so what happened?

The short answer is **life happened** or as I refer to it with my wife, *When Life Gives You Lemons*.  Here's a snippet from the home page that tells the tale:

&lt;div markdown = &quot;0&quot; style=&quot;margin-left: 50px; margin-right: 50px; background-color: #f6f8fa; padding: 16px;overflow: auto;font-size: 85%;line-height: 1.45;background-color: #f6f8fa;border-radius: 3px;&quot;&gt; 
&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;My name is Scott and I'm a parent, I bet a lot like you. I recently discovered that one of my children has been using my credit card to purchase things in the XBox store -- a lot of things. That $347.82 bill I mentioned above, well, that's true -- and it was just the tip of the iceberg.&lt;/p&gt;

&lt;p&gt;I'm also a software developer and I'll admit that while I realized immediately that I had a parenting problem, this was also an opportunity -- and the very next morning I started building Game Nanny.&lt;/p&gt;

&lt;p&gt;As parents, even if we don't like video games, we should admit that our kids love them -- but they can be addictive and lead to crazy bad behavior. I'm not the first parent to be surprised by his kid's gaming spending and I won't be the last.&lt;/p&gt;

&lt;p&gt;Use Game Nanny to take control of your kid's XBox bad habits. Don't be surprised again!&lt;/p&gt;
&lt;/div&gt;

&lt;p markdown=&quot;0&quot;&gt;&lt;/p&gt;

And that's the honest to god truth as how Game Nanny came into being.  To summarize:

* I saw my kid had made a mistake (and, yes, a big one)
* I realized that this wasn't just a parenting problem, that it was an opportunity
* That realization took about all of a minute.
* After that first minute, I had a pretty good idea of what the product was
* Two weeks later I had a functional product

The bottom line for anyone that either perceives themself to be an entrepreneur or wants to be is that when life gives you a problem, you can look at as just that.  Or you can look at your problem as &quot;Is there an opportunity here?&quot; and then, if you think the answer is &quot;Yes&quot; then build a business around it.

</description>
        <pubDate>Fri, 06 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2017/10/06/the-game-nanny-story-or-when-life-gives-you-lemons.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2017/10/06/the-game-nanny-story-or-when-life-gives-you-lemons.html</guid>
        
        <category>game_nanny</category>
        
        <category>startup</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Self Hosting a Jekyll Site</title>
        <description>I've been a blogger since 2002 and over the years there have been a lot of different blogging tools: Radio, WordPress, Serendipity, Movable Type, TypePad, b2, etc.  After a long, on and off hiatus, I made the decision in 2016 to get serious about blogging again and at that point I committed to [Jekyll](https://jekyllrb.com/).  The reason was really the simplicity of: 

* Having all my blog posts in a filesystem, not a database, was a transformative and wonderful experience
* Being able to use my coding editor to jot down a simple blog post was a big, big deal
* Having all my blog posts run thru github, a tool I use not just daily but generally hourly
* The ease of use of Github Pages which magically hosts a blog for you

And, for the past almost 2 years, this has been a great decision.  My blog, [FuzzyBlog](http://www.fuzzyblog.io/blog/), has been an essential tool for writing and just personal documentation.  Basically whenever I have to learn something, I try and write it down and that's a model which works great for me.

I recently needed, however, to make a product specific blog, [blog.gamenanny.io](https://blog.gamenanny.io), for my new [Game Nanny](https://www.gamenanny.io/) product and I discovered that I just couldn't use Github Pages for this at all.  Github Pages creates a single blog tied to your github username and, well, I only have one github username.  This meant that I had to get into self hosting my Jekyll blog or changing from Jekyll to something else.  I chose the self hosting route and there in lies a how to tale.

**Note**: Like most things on the Internet, there must be a way around this limitation but I haven't found it yet.

**Update**: My friend [Steve Grossi](https://www.twitter.com/stevegrossi) told me that you can get around this apparent [Github limitation](https://help.github.com/articles/user-organization-and-project-pages/). I'm not entirely certain that I understand it but I'm glad to have it as an option. Thx Steve!

# Understanding Jekyll's Output

The key thing to understand about Jekyll is that, at its heart, it is a static site generator.  What this means is that when you type:

&gt; jekyll serve

or 

&gt; jekyll build

it creates a set of static files that represent your site based on the markdown documents in _posts and your _config.yml configuration file.  Self hosting a Jekyll site basically amounts to:

1.  Creating a jekyll site.
2.  Creating content in that Jekyll site.
3.  Running jekyll build or jekyll serve to create the static site files.
4.  Getting the files in _site to a public server (rsync).
5.  Having a web server on a public box that can serve the files generated by Jekyll.

The rest of this blog post walks you through all these steps.  I'm a big, big believer in AWS so that's what I used but any public web host will work.

# Creating a Jekyll Site

This can be done with: 

&gt; jekyll new site_name

I used:

&gt; jekyll new game_nanny_blog

I'm not going to cover Jekyll details like theme configuration much although I have a set of references at the end.  Jekyll theme configuration is to me, well, a **black art** that best resembles a *pagan ritual* (do it at in the dark of night, naked, chanting and so on).  I just use the default minima theme and generally hope I can get away with it.

Basic configuration can be done by modifying _config.yml in your Jekyll root directory.  Advanced configuration, well, it can be tedious.

# Configuring Your Server

All you need to self host a jekyll blog is a dead simple web server like Nginx.  Although I've mostly been doing Docker for executable software for sometime now, it just felt simpler to go old school on this.

Install nginx with:

&gt; sudo apt-get install nginx

You may need to edit the main configuration file with: 

&gt; sudo vi /etc/nginx/nginx.conf

I found the defaults are pretty basic but your details may vary.  And you'd also need to create a config file specifically for your main blog with:

&gt; sudo vi /etc/nginx/conf.d/blog.conf

Here's what I have in my blog.conf file:

    server {
      listen 3301;
      server_name blog.gamenanny.io;
      root /home/ubuntu/gamenannyblog/_site;

      expires 30s;
      add_header Cache-Control public;
      add_header ETag &quot;&quot;;

      error_page 404 /404.html;
      error_page 500 /500.html;

      location  ~ ^/assets/ {
        expires max;
      }

      location ~ /\. {
        return 404;
        access_log off;
        log_not_found off;
      }

      location = /favicon.ico {
        try_files /favicon.ico =204;
        access_log off;
        log_not_found off;
      }

      location / {
        try_files $uri $uri.html $uri/ =404;
      }
    }

The main changes that I made in blog.conf are these:

    listen 3301;
    server_name blog.gamenanny.io;
    root /home/ubuntu/gamenannyblog/_site;

Each of these is explained as follows: 

* **listen 3301;** The reason for that is that I have multiple sites on this one AWS instance and my AWS Elastic Load Balancer will be responsible for translation data on port 3301 to an port 80 request on blog.gamenanny.io.  I use a standard set of port mappings on everything I build (*00 is always the main site, *01 is always the blog, *05 is always the ecommerce)
* **server_name server_name blog.gamenanny.io;** This configures the site's domain name.
* **root /home/ubuntu/gamenannyblog/_site;** This sets the root directory to serve the site's data from.

# AWS Specifics

I'm deploying to a basic AWS EC2 instance and here is a quick overview of what I needed to do.  Please note that these are the very broad strokes as I was quite rushed when I did it:

1.  Add blog.gamenanny.io to my Route 53 domain settings.
2.  Add a target group to the ELB for the blog.
3.  Add a listener rule to the ELB for the blog.
4.  Connect the listener rule to the target group.  Until you do this there will be a 503 error and that's what always reminds me to couple the two.
5. Create a directory on your file system where the content can reside.  I did this with:

&gt; mkdir /home/ubuntu/gamenannyblog/_site

The _site corresponds to Jekyll's _site directory where it generates all data.

# Deployment

In order to handle deployment, I wrote a small shell script. deploy3.sh, which invokes jekyll build and then rsync's the content to the server.  I've now spent almost two years using Jekyll with Github pages and my one comment about the git push to get my content live is that **it is slow**.  I was absolutely shocked just how fast invoking jekyll build and then rsync was.  Here's my deploy3.sh script:

    #!/bin/bash

    JEKYLL_ENV=production bundle exec jekyll build
    rsync -avr --rsh='ssh' --delete-after --delete-excluded _site/ ubuntu@52.14.179.50:/home/ubuntu/gamenannyblog/_site
    
This level of speed is just plain awesome.
    
# Thank You

This is actually the first time I've ever successfully used Nginx; previously I was a straight up, old school Apache guy.  My buddy [Nick](http://www.nickjanetakis.com) did my Nginx configuration in about 2 shakes of time (read that as &quot;really, really, really quick&quot;).  Thanks Nick!

# See Also

* [Nginx](https://www.nginx.com/resources/wiki/)
* [Jekyll Home Page](https://jekyllrb.com/)
* [Customizing Minima](https://github.com/jekyll/minima)
* [Customizing Jekyll Themes](https://jekyllrb.com/docs/themes/)
* [Jekyll Deployment](https://jekyllrb.com/docs/deployment-methods/)
* [Jekyll RSync Example](https://github.com/vitalyrepin/vrepinblog/blob/master/transfer.sh)
* [Adding Disqus to Your Jekyll Blog](http://sgeos.github.io/jekyll/disqus/2016/02/14/adding-disqus-to-a-jekyll-blog.html) Disclaimer - this was a total fail for me and I found the documentation horrid but Disqus has always failed for me so perhaps it is just me.
</description>
        <pubDate>Fri, 06 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/jekyll/2017/10/06/self-hosting-a-jekyll-site.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/jekyll/2017/10/06/self-hosting-a-jekyll-site.html</guid>
        
        <category>jekyll</category>
        
        <category>blogging</category>
        
        <category>startup</category>
        
        
        <category>jekyll</category>
        
      </item>
    
      <item>
        <title>Using Chrome Driver with Docker, Rails and Selenium on AWS</title>
        <description>I've done a lot of crawling in my professional career and I mean a **lot**.  The recent trend towards JavaScript based sites, however, has wreaked havoc on my traditional approach of low level html parsing.  For a new product I'm launching, I recently had to make the switch to using [Selenium](https://github.com/SeleniumHQ/) for crawling.  Selenium, commonly known as a testing tool, can be used in an embedded fashion where you use code like this:

    browser = Selenium::WebDriver.for :chrome
    browser.get &quot;https://www.google.com&quot;
    html = browser.page_source

Here's a rough equivalent in Mechanize:

    agent = Mechanize.new
    page = agent.get(&quot;https://www.google.com&quot;)
    html = page.body

Although the lines of code look similar to something like [Mechanize](https://github.com/sparklemotion/mechanize) this is actually an entirely different approach because underneath it all is a full browser including JavaScript which lets something like a page which sends its data down in a JavaScript array and then uses JavaScript to display it actually render and return back to you parseable html.

# Doing this Server Side

If you're going to use Selenium for a server side crawling process then what you actually needs is called ChromeDriver which is the embedded chrome executable that represents a browser.  One approach is to locally install ChromeDriver on each of your compute nodes but the better approach is to Dockerize everything and run ChromeDriver as a container.  Here are the steps to do just that.

## Add ChromeDriver to Your Docker Compose File

You'll likely need to implement both Step 1 and Step 2.

### Step 1

Here's what you'll need for docker-compose.yml or docker-compose.production.yml:


    chromedriver:
      image: 'robcherry/docker-chromedriver'
      ports: 
        - 4444:4444
      restart: on-failure
      shm_size: 1G    
      environment:
        - CHROMEDRIVER_WHITELISTED_IPS=&quot;&quot;
        
### Step 2        

A docker container is dramatically lighter than a full computer and you'll find that Chrome will start.  The specific error you are likely to hit is:

&gt; Selenium::WebDriver::Error::UnknownError: unknown error: Chrome failed to start: crashed

When you discover this you need to change your daemon.json file (normally located in /etc/docker/daemon.json) to have a larger shm_size like this:
    
    cat daemon.json
    {
      &quot;default-shm-size&quot;:&quot;2g&quot;,
      &quot;storage-driver&quot;: &quot;overlay2&quot;,
      &quot;log-driver&quot;: &quot;json-file&quot;,
      &quot;log-opts&quot;: {
        &quot;max-size&quot;: &quot;10m&quot;
      }
    }

Yes I'm aware that I use 1G in the first example and 2g in the second.  This is how I currently have it working and I'm leaving it alone until I play with it more.  I suspect you could sync these but I don't know that definitively.

### Step 3

The next error you are likely to hit is one of these two, possibly both:

&gt; Net::ReadTimeout during browser launch

&gt; UnknownError: session deleted because of page crash from tab crashed

The solution to each of these is how you invoke the core browser object.  Here's what I used:

    browser = Selenium::WebDriver.for :chrome, url: &quot;http://chromedriver:4444&quot;, :prefs =&gt; {:password_manager_enable =&gt; false, :credentials_enable_service =&gt; false}, :switches =&gt; [&quot;disable-infobars&quot;, &quot;no-sandbox&quot;]
                                                    
# Security Issues

Here's an interesting security note: 

  Note: ChromeDriver restricts access to local connections by default. To allow external connections, you can pass in a custom CHROMEDRIVER_WHITELISTED_IPS environment variable. By default, this is set to 127.0.0.1, but this can by any comma separated list of IP addresses. Setting the value as empty will allow all remote connections.
  
AWS security groups really save you on this one.  By setting CHROMEDRIVER_WHITELISTED_IPS=&quot;&quot;, I am able to use ChromeDriver from any other container but nothing else can execute them.  An embedded browser like Selenium is akin to an open proxy so you do not want that.

# References
* [Docker Chrome Driver](https://hub.docker.com/r/robcherry/docker-chromedriver/)
* [Ruby and Selenium](https://github.com/SeleniumHQ/selenium/wiki/Ruby-Bindings)
* [Installing Chrome Driver Locally on Ubuntu](https://gist.github.com/ziadoz/3e8ab7e944d02fe872c3454d17af31a5)
* [Net::ReadTimeout during browser launch](https://github.com/SeleniumHQ/docker-selenium/issues/198)
* [Running Selenium on Headless Chrome](https://intoli.com/blog/running-selenium-with-headless-chrome/)
* [Setting Docker Shm](https://docs.docker.com/engine/reference/run/#ipc-settings-ipc)
* [Docker and Selenium](https://github.com/elgalu/docker-selenium)
* [Zalenium](https://github.com/zalando/zalenium#run-it)
* [Docker and Selenium and Shm / Chrome](https://github.com/elgalu/docker-selenium/issues/20#issuecomment-133011186)
                          
# Thank You

Kudos and thanks to [Nick my Docker buddy](http://www.nickjanetakis.com/).  He teaches Docker professionally and really knows his stuff.  I probably could have slogged through this but his help turned what would have been a man day into a quick one hour pairing session.  Appreciated.</description>
        <pubDate>Sat, 30 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/09/30/using-chrome-driver-with-docker-rails-and-selenium-on-aws.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/09/30/using-chrome-driver-with-docker-rails-and-selenium-on-aws.html</guid>
        
        <category>docker</category>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>selenium</category>
        
        <category>aws</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Creating an HTTPS Site on AWS with a .IO Domain</title>
        <description>I recently registered my first .io domain using an all AWS stack (Route 53, EC2, Elastic Load Balancer or ELB, Amazon Certificate Manager or ACM) and I found the process mildly odd -- to the degree that I know that it will either bite me again or it will bite someone else.  Either way I thought that it would be best to fully document the process.

The site I'm building has login and as such https is important for this site.  Although I could have used [Let's Encrypt or CloudFlare](https://nickjanetakis.com/blog/lets-encrypt-vs-cloudflare-for-https) for the https, if you're using https on AWS then there's a good chance that you are using an ELB and ACM as they make SSL brilliantly simple.  In all my years of deploying SSL, it has never been as simple as using this AWS stack and I sincerely recommend it.  The oddness occurred when I had to bind the .io domain into the certificate itself.  This process requires an email authentication to the owner of the domain to ensure that no one compromised your AWS credentials.  When you are using a normal .com or .net domain and you do this, Amazon sends the email to the email address on the domain registration itself, easy peasy.  When you are using a .io account, the email is NOT sent to the email address on the domain registration but it is instead sent to all of these:

* administrator@your_domain.io
* hostmaster@your_domain.io
* postmaster@your_domain.io
* webmaster@your_domain.io
* admin@your_domain.io

To Amazon's credit, they do kind of disclose this in the ACM documentation [here](http://docs.aws.amazon.com/acm/latest/userguide/troubleshoot-iodomains.html). Here's the relevant passage:

&gt; ACM does, however, send validation email to the following five common system addresses where your_domain is the domain name you entered when you initially requested a certificate and .io is the top level domain.

&gt; (see above)

&gt; To receive validation mail for an .IO domain, make sure that you have one of the preceding five email accounts enabled. If you do not, you will not receive validation email and you will not be issued an ACM certificate. 

The problem here is that like a lot of us, probably most of us, I don't have a mail server running on my .io domain so there's nothing to receive the email (so I can respond to it).  Now I could have set up a mail server but a much, much easier approach is to use [ImprovMX](http://improvmx.com/) which promises &quot;Free painless email forwarding for your domains&quot; and it really, really delivers.  Within only a few minutes, I had my ACM mailing and I was off to the races.  With any luck, I will even launch my new site later this week.  

ImprovMX is strongly, strongly recommended.

# Thank You

As always, I resolved this with my favorite pairing buddy, [Nick Janetakis](http://www.nickjanetakis.com).  He does Docker stuff and is very, very talented.  He is also a Let's Encrypt expert.  Recommended.
</description>
        <pubDate>Sat, 30 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/09/30/creating-an-https-site-on-aws-with-a-io-domain.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/09/30/creating-an-https-site-on-aws-with-a-io-domain.html</guid>
        
        <category>aws</category>
        
        <category>io</category>
        
        <category>https</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Rails, AuthLogic, CSRF, 422 and session_store.rb</title>
        <description>When you're a professional developer, you come to recognize a category of problems that I refer to as &quot;Pair Programming Required&quot;.  These are those mysterious failures where you know damn well that something should work but *nothing* and I repeat ***nothing*** works.  In this situation, you really want to bring another set of eyes to bear on the problem.

I'm in the process of bringing a new Rails powered application online and I discovered around 3:53 am this morning that login, which works perfectly in development, completely fails in production with the wonderfully helpful error message:

&gt; Rails 5 ActionController::InvalidAuthenticityToken error

When I dug into it, I saw that was a CSRF error and, oddly, I was getting a 422 error message returned to me.  

The mystery of all this was that this wasn't new login code -- it was code that I've been using in another application for months and it has been flawless.

Here are just a few of the things that I tried to address this:

* Switch from Rails 5.1.x back to 5.0.x
* Investigate the prepend: true approach to protect_from_forgery ([StackOverflow](http://www.stackoverflow.com/questions/38331496))
* Rewrite most of application_controller.rb
* Massively hack around in the guts of AuthLogic
* Google
* Stack Overflow
* Run production locally
* change, deploy, test, change again, Lather, Rinse Repeat N times where N is &gt; 10 and less than 50

Once I exhausted all these possibilities, I reached out to a [friend](http://www.nickjanetakis.com/) and he and I paired on it.  And that's where the power of pair programming really illustrated itself.  We fairly quickly discovered that the issue was that session_store.rb didn't match the production domain.  I find it unbelievable that the error message wasn't actually useful but since I've been guilty of that sin more than a few times, well, karma I guess.  

Documented here for the next time that I hit this (in the spirit of being a good Internet citizen, I did add it to the Stack Overflow as well).
</description>
        <pubDate>Mon, 25 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/09/25/rails-authlogic-csrf-422-and-session-store-rb.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/09/25/rails-authlogic-csrf-422-and-session-store-rb.html</guid>
        
        <category>rails</category>
        
        <category>authlogic</category>
        
        <category>csrf</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Docker Won't Install Libxml</title>
        <description>One of the most vexing situations in any type of development is when something that you swear you've done literally hundreds of times fails to work -- at all.  It always turns out to be that what you're doing is 99.995% the same as something previous -- and .005% different.  Last night I went to do a docker-compose build on a new application using my standard stack of Ruby, Rails, Mechanize and the like and I got this, well, crap:

    [24/1918]
    Gem::Ext::BuildError: ERROR: Failed to build gem native extension.

       current directory: /usr/local/bundle/gems/libxml-ruby-3.0.0/ext/libxml
    /usr/local/bin/ruby -r ./siteconf20170920-5-mpa6ar.rb extconf.rb
    checking for libxml/xmlversion.h in
    /opt/include/libxml2,/opt/local/include/libxml2,/usr/local/include/libxml2,/usr/include/libxml2...
    no
    *** extconf.rb failed ***
    Could not create Makefile due to some reason, probably lack of necessary
    libraries and/or headers.  Check the mkmf.log file for more details.  You may
    need configuration options.

    Provided configuration options:
           --with-opt-dir
           --without-opt-dir
           --with-opt-include
           --without-opt-include=${opt-dir}/include
           --with-opt-lib
           --without-opt-lib=${opt-dir}/lib
           --with-make-prog
           --without-make-prog
           --srcdir=.
           --curdir
           --ruby=/usr/local/bin/$(RUBY_BASE_NAME)
           --with-xml2-config
           --without-xml2-config
           --with-xml2-dir
           --without-xml2-dir
           --with-xml2-include
           --without-xml2-include=${xml2-dir}/include
           --with-xml2-lib
           --without-xml2-lib=${xml2-dir}/lib
    extconf failure: need libxml2.

       Install the library or try one of the following options to extconf.rb:

         --with-xml2-config=/path/to/xml2-config
         --with-xml2-dir=/path/to/libxml2
         --with-xml2-lib=/path/to/libxml2/lib
         --with-xml2-include=/path/to/libxml2/include


    To see why this extension failed to compile, please check the mkmf.log which can
    be found here:

    /usr/local/bundle/extensions/x86_64-linux/2.3.0-static/libxml-ruby-3.0.0/mkmf.log
    I can't get to the mkmf.log file since that's I think done with a container since its clearly linux not osx.
    No clue why this is happening since I use this same damn gem everwhere
    There was also this info

    Gem files will remain installed in /usr/local/bundle/gems/libxml-ruby-3.0.0 for
    inspection.
    Results logged to
    /usr/local/bundle/extensions/x86_64-linux/2.3.0-static/libxml-ruby-3.0.0/gem_make.out

    An error occurred while installing libxml-ruby (3.0.0), and Bundler cannot
    continue.
    Make sure that `gem install libxml-ruby -v '3.0.0'` succeeds before bundling.
    ERROR: Service 'web' failed to build: The command '/bin/sh -c bundle install --binstubs' returned a non-zero code: 5

One of the major reasons I got into Docker in the first place was to get away from this kind of crap!  Gem builds, particularly where libxml is involved, well, suck.  I spent close to two hours last night tweaking things, updating libraries and such and it was a complete and utter failure.  I even installed Docker on a brand new dev machine and got the exact same result.  

# The Solution

This morning I revisited it using the Google query [docker won't install libxml](https://www.google.com/search?q=docker+won%27t+install+libxml&amp;ie=utf-8&amp;oe=utf-8) and found this [Github Issue](https://github.com/docker-library/php/issues/315) where they suggest adding libxml2-dev to your Dockerfile.  Thus this:

&gt; RUN apk update &amp;&amp; apk add build-base nodejs mariadb-dev tzdata git

becomes

&gt; RUN apk update &amp;&amp; apk add build-base nodejs mariadb-dev tzdata git libxml2-dev

This was hugely surprising to me because according to github, I haven't touched my Dockerfile in **4 months** ever since my [Docker buddy and guru, Nick](https://diveintodocker.com/), helped me set it up.  My docker-compose file, however, was just modified 22 days ago. As docker-compose has become ever more important in the Docker world, the basic Dockerfile is something you touch less and less -- apparently until something bites you in the proverbial arse.

# And Here's Why this Happened

My normal use of libxml, for the past decade or so, is always tied straight to Ruby's [Mechanize gem](https://github.com/sparklemotion/mechanize) which I've had running correctly under Docker for the past six months.  And that's why this was so, so surprising to me.  The mistake that I made was that earlier today I had added [twilio-ruby](https://github.com/twilio/twilio-ruby) to my Gemfile and apparently twilio-ruby has different nokogiri dependencies than Mechanize which caused the Dockerfile to need libxml2-dev.  Digging into the twilio-ruby issues, I found [this issue](https://github.com/twilio/twilio-ruby/issues/315) which seems to address it.

# See Also

My buddy Nick, mentioned above, [talks about this on his blog](https://nickjanetakis.com/blog/docker-tip-9-installing-popular-packages-on-alpine).
</description>
        <pubDate>Thu, 21 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/09/21/docker-won-t-install-libxml.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/09/21/docker-won-t-install-libxml.html</guid>
        
        <category>docker</category>
        
        <category>rails</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Analyzing iPhone X, iPhone 8 and iPhone 8 Plus</title>
        <description>I am not an Apple blogger -- I leave that to people like [Daring Fireball](https://www.daringfireball.net) but I am an avid Apple user.  Between my wife and two sons, all of us have at least one Mac, iPad or iPhone.  I wrote this piece to analyze the differences between the new iPhones and help analyze how to spend what ultimately will be quite a bit of money.

Note: This is all sourced from the web; my thanks to all the wonderful coverage cited below.  Particular kudos to Quincy Larson for his salient [FaceID analysis](https://medium.freecodecamp.org/why-you-should-never-unlock-your-phone-with-your-face-79c07772a28).

# Understanding Your Personal Phone Usage

I think the key to deciding whether or not to upgrade is to understand your personal phone usage and then if the new phones offer *you* a benefit.  You *don't have* to upgrade.  Let me repeat that -- you **don't have** to upgrade.  Personally I am still running a 128 gig iPhone 6.  I never made the leap to the 7 because I use the smaller phone form factor and the camera was mostly unchanged from the 6 to the 7.  Given the importance of the camera to people's daily lives, I was delighted to see all the camera updates in the iPhone X - it is now the same camera as the 6 plus / 7 plus but in the smaller phone's form factor (ok the X is a little bit larger but I would deem it insignificant).

My iPhone usage pattern is simple: it goes **everywhere** I go.  I carry it constantly and the only place I don't take it is swimming. I've carried it with me when I was: 

* working on a roof
* cutting down trees
* fishing
* working out
* cooking

In aggregate, I use my phone more than any other computing device.  I compute less with it than I do a laptop but I use it more (if that makes sense).

Since I am using an iPhone 6 still, this means that I am now two cycles removed from current.  Consequently my battery life is such that I have to recharge during the day even when I'm not using the GPS, I'm constantly running out of space, the screen is scratched (no clue how that happened).  I am thus in the market for a new phone.

# New Phones Summarized

There are three new phones:

* [iPhone 8](https://www.apple.com/iphone-8/specs/) (similar to the current 7).  $699.
* [iPhone 8 Plus](https://www.apple.com/iphone-x/specs/) (similar to the current 7 plus).  $799.
* [iPhone X](https://www.apple.com/iphone-x/specs/) (the new phone).  $999.  Slightly bigger physically than the current iPhone 6 / 7 base model but NOT at all as large as the plus model.

The iPhone X is the new hotness with an all new form factor, better camera, **no home button** and an OLED screen.  Both the iPhone X and iPhone 8 offer wireless charging although Apple's own wireless charging mat will not ship until &quot;2018&quot;.  Given the AirPod shipping delays this is concerning.  Technically any Qi compatible charging mat can be used but my suspicion is that Apple's will somehow be better / faster (otherwise why the delay).

# Key Stats

* iPhone X is available for pre-order on October 27 w/ launch on November 3
* iPhone X is available in 256 and 64 gig capacities
* iPhone X has the **same camera** as on the 7 Plus. 
* iPhone X has no home button and no TouchID.
* iPhone X claims 2 hours longer battery life than the iPhone 7.
* Animoji is exclusive to the iPhone X
* iPhone 8 is available for pre-order on September 15th with launch on September 22
* iPhone 8 and iPhone 8 Plus offer 12 MP cameras but the 8 Plus also has a telephoto lens.

See the [Compare page](https://www.apple.com/iphone/compare/) for more.

# Qualms and a Decision

My biggest issue with the iPhone X is the lack of the home button and TouchID.  This is a fundamental change to the iPhone interface and one that greatly concerns me.  [Quincy Larson](https://medium.freecodecamp.org/why-you-should-never-unlock-your-phone-with-your-face-79c07772a28) raises some excellent points on why you shouldn't unlock with your face.  Personally I find the need to unlock your phone while driving (GPS, play a podcast, etc) and the new FaceID feature disturbing to say the least.  As the parent of a driving age teenager, I will not be eager to see him get anything with FaceID.  Thankfully the device cost alone will prevent that but in a few years ... 

Based on the analysis that I've done so far, I'm likely to upgrade to the iPhone X and do it through Apple's monthly plan.  From what I've read, this amounts to about a $12 and change difference over the baseline 7 model and while I don't like the $999 price, I don't think that's too much for something that I use constantly and travels with me literally everywhere I go that isn't a swimming pool.  

The tipping point that pushed me over the edge to the new iPhone X was the new camera features being available in the iPhone X's smaller form factor.  Had iPhone X had the same camera as the iPhone 7 then I likely wouldn't have upgraded at all.  And had the iPhone 7 had the new camera as the iPhone X or 7 Plus, I likely would have decided to stay with the traditional home button design and not risked learning the new gestures.

# See Also

Here are some excellent breakdowns:

* [Apple iPhone Compare Page](https://www.apple.com/iphone/compare/)
* [Apple iPhone X Page](https://www.apple.com/iphone-x/)
* [Apple iPhone 8 Page](https://www.apple.com/iphone-8/)
* [Apple Watch Page](https://www.apple.com/apple-watch-series-3/)
* [iPhone X Tech Specs](https://www.apple.com/iphone-x/specs/)
* [iPhone 8 Tech Specs](https://www.apple.com/iphone-8/specs/)
* [Ars Technica Hands On on the iPhone X](https://arstechnica.com/gadgets/2017/09/hands-on-with-the-iphone-x-oled-and-hdr-outshine-everything-else/)
* [Ars Technica on the iPhone X](https://arstechnica.com/gadgets/2017/09/apples-radically-different-smartphone-is-called-the-iphone-x/)
* [iPhone 8 and 8 Plus from The Verge](https://www.theverge.com/2017/9/12/16287932/apple-iphone-8-plus-photos-video-hands-on)
* [iPhone X from the Verge](https://www.theverge.com/2017/9/12/16291244/new-iphone-x-photos-video-hands-on)
* [MacRumors on iPhone X](https://www.macrumors.com/roundup/iphone-x/)
* [MacRumors on iPhone 8](https://www.macrumors.com/roundup/iphone-8/)
* [Apple Watch Discussion on Hacker News](https://news.ycombinator.com/item?id=15229317)


</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/iphone/2017/09/13/analyzing-iphone-x-iphone-7-and-iphone-7-plus.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/iphone/2017/09/13/analyzing-iphone-x-iphone-7-and-iphone-7-plus.html</guid>
        
        <category>iphone</category>
        
        <category>apple</category>
        
        
        <category>iphone</category>
        
      </item>
    
      <item>
        <title>Remembering Jerry Pournelle</title>
        <description>There were three main dimensions to how I grew up: 

1.  Reading.
2.  Computing.
3.  Entrepreneurship.  

[Jerry Pournelle](https://en.wikipedia.org/wiki/Jerry_Pournelle) dramatically affected each of these for me.  Here's how:

# Reading

If you strip everything else away from me, at my core, I am a reader, specifically of the science fiction variety -- and this goes all the way back to my childhood.  I've been reading as long as I can remember and it pretty much always has been science fiction.  I got started early with the Heinlein juveniles and it wasn't a far leap from Heinlein to Jerry Pournelle.  I still remember when the librarians let me move from the kids section of the library to the adult section and that's where I found him - King David's Spaceship, West of Honor, Lucifer's Hammer, Oath of Fealty and more.  And I can remember devouring Jerry's work in Analog both under his name and under &quot;Wade Curtis&quot; when I started buying back issues and I remember having [this one](https://www.abebooks.com/book-search/author/curtis-wade-pournelle-jerry/) in my collection.

A good author, whether fact or fiction, imparts something to his readers far beyond the story and what Jerry Pournelle gave to me was a deep appreciation for engineering and just plain *rationality*.  Thanks Jerry!

# Computing

Right along side reading for me was computing which I discovered in the 7th grade in Wilton, Connecticut at the school's computer lab (a selection of TRS-80 Model 1s).  A trip to the local Walden Books made me aware of Byte Magazine way back in April 82 with this issues:

![byte magazine](https://fiu-assets-2-syitaetz61hl2sa.stackpathdns.com/static/use-media-items/25/24048/full-971x1298/56703a87/198204.jpeg?resolution=0)

That issue is still in my office and as I write this it is on my desk; I've owned it since I was 14 years old.

And if I'm being honest, well, I understood about 1 word in 20, mostly the adjectives but what I did find actually not only readable but just plain joyful was *Chaos Manor*, Jerry Pournelle's column.  From that point forward, I read Byte for Jerry's column and he was the person who gave me a deep love for computing in general and software in specific.  I can remember hours upon hours in my college library devouring back issues of Byte Magazine once I found their periodicals section.  

Finally I became aware of hypertext from things written in Byte Magazine -- and my first startup was a hypertext company.  If not for Jerry addicting me to Byte Magazine, I might never have known about it and my whole life would have been different.

**Note:** If you didn't read Jerry back in the day then you don't understand what a colossus Jerry was in small computing.  Computing was deeply different in those days, a much smaller community and if you were influential, well, you were important.  This was a man who knew everyone -- Bill Gates, Marvin Minsky, Steve Wozniak and more.  According to Jerry, Byte Magazine paid him $10,000 per month to write that column and that's a fair bit of $$$ even today -- back in the early 80s, that was a king's ransom just to write a monthly computer column.

# Entrepreneurship

I started my first company, a software firm building hypertext tools, when I was 19 and the very first copy of software I ever sent to a reviewer was sent to Jerry Pournelle.  He didn't cover it then and it took repeated copies that we kept sending him until November 26, 1990 when he did give us some coverage in his [Infoworld column](https://books.google.com/books?id=u1AEAAAAMBAJ&amp;pg=PA64&amp;lpg=PA64&amp;dq=%22jerry+pournelle%22+%22ntergaid%22&amp;source=bl&amp;ots=pDFl9rznro&amp;sig=wHE-VXmUd2h64RPIIYSYmvcs2Rs&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjGjdCY55zWAhUJ5oMKHeZPCHoQ6AEIJjAA#v=onepage&amp;q=%22jerry%20pournelle%22%20%22ntergaid%22&amp;f=false). A huge amount of what I know about the software business, specifically about writing good documentation, providing good technical support and just plain listening to customers, I learned from Jerry and his long tales of Zeke, the S100 bus and more.  Jerry wrote about computing from the user's perspective and he gave a large number of us a deep appreciation for the user.  

# Rediscovering Jerry Pournelle Through Twit

Despite the deep impact that Jerry Pournelle has had on my life, I haven't thought much about Jerry for a long time now -- Byte is long gone and the pace of his fiction output has decreased and his Chaos Manor column on the web didn't resonate for me in the same way as his print column.  But thanks to Leo Laporte's excellent TWIT show on YouTube, I have been able to discover him all over again:

* [Twit 90](https://www.youtube.com/watch?v=S7j3IG4h42Y)
* [Twit 95](https://www.youtube.com/watch?v=_5UVunOiXCk)

There are some amazing stories here:

* In Twit 90, he talks about making nitroglycerin at age 12
* In Twit 95, he talks about the act of god that led him into freelance writing and then discussing the probability of it with Marvin Minsky (Minsky created Lisp and founded the MIT AI lab)
* In Twit 95, we talks about Robert Heinlein and the problem with being a best selling author, no one has the guts to edit you: &quot;Time Enough or Love -- there are three good novels there, shouldn't they be three good novels?&quot; (loose paraphrase but it is pretty close)

In Twit 90, Jerry describes his Byte writing as &quot;I wrote the Field and Stream column&quot; and that analogy instantly brought all of his computing writing into focus for me since I actually grew up hunting **and** reading Field and Stream.  

Thank you very, very much Leo for doing these interviews, listening to you talk with Jerry for a few hours means a huge amount to me.  

Goodbye Jerry, you'll be missed but your influence is still alive and well.</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/personal/2017/09/11/remembering-jerry-pournelle.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/personal/2017/09/11/remembering-jerry-pournelle.html</guid>
        
        <category>personal</category>
        
        <category>scifi</category>
        
        
        <category>personal</category>
        
      </item>
    
      <item>
        <title>SAAS Business Best Practices from Jonathan Siegel</title>
        <description>Jonathan Siegel wrote [The San Francisco Fallacy](https://www.amazon.com/dp/B071NGMJPN) which is my favorite startup book of the year.  I liked it well enough that I'm in the middle of a second read and its one of the books I packed for an upcoming trip to Disney World.  Jonathan was [recently interviewed by Noah Kagan for Ok Dork](http://okdork.com/buying-a-business-jonathan-siegel/) and while all of it was excellent, there was one brief shining moment where it felt like the sun had opened up and ideas were literally shining down.  Here's a recap of the best practices that he identified for a SAAS business:

1.  Testing their adwords.
2.  Doing landing page testing.
3.  Sending out newsletters.
4.  Blogging regularly.
5.  Forming partnerships.
6.  Going to events.
7.  **5 minute callbacks whenever they get a new lead on the website**.
8.  **Try six times to reach every new inbound lead**.

Numbers 7 and 8 are new to me and ones that I definitely want to try.  Much of this interview was covering how he buys a business and the money statement of this was &quot;If they team is already doing all these then I would never buy this company&quot; because these techniques are how he adds value to the business and increases revenue.

All of this was covered around the 24 minute mark.  If you're an entrepreneur, I'd highly recommend listening to this podcast.  Noah's stuff is always great but this particular episode was golden.  There's also a great segment on how he identifies people to hire and why he doesn't hire A players.  </description>
        <pubDate>Wed, 06 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/saas/2017/09/06/saas-business-best-practices-from-jonathan-siegel.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/saas/2017/09/06/saas-business-best-practices-from-jonathan-siegel.html</guid>
        
        <category>saas</category>
        
        <category>marketing</category>
        
        <category>best_practices</category>
        
        <category>business</category>
        
        
        <category>saas</category>
        
      </item>
    
      <item>
        <title>Reclaiming Docker Disc Space on OSX</title>
        <description>A few nights ago, after a truly horrible night's sleep complete with my first ever incidence of acid reflux, I woke to find my Mac nattering at me about being **out of disc space**.  And by out of disc space I mean that I was down to about 3 gigs out of a terabyte sized SSD.  Yikes!

Taking a look at the normal culprits yielded no surprise increases and I can distinctly recall having in excess of 30 gigs just a day or two ago.  Given all the [server side problems with respect to Docker and disc space](http://fuzzyblog.io/blog/docker/2017/08/30/running-out-of-disc-space-with-docker.html) that I have written about previously, I had a strong hunch that somehow Docker was involved.

# Logs

My first thought was that perhaps Docker logs are huge.  On OSX, Docker logs are stored here:

&gt; ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log

But a quick:

    09:41 $ du -shc ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log
     20K	/Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log
     20K	total
     
Nope -- clearly 20K isn't anything for disc space usage.

# Containers

My next thought was that perhaps Docker has some massive backing store for containers.  I did a bunch of searches and ran across this [Docker thread](https://forums.docker.com/t/consistently-out-of-disk-space-in-docker-beta/9438/67
) and then [another thread](https://forums.docker.com/t/consistently-out-of-disk-space-in-docker-beta/9438/36), both of which pointed me to the Docker.qcow2 file.  

Here's what a du -shc showed me on this file:

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
     57G	/Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
     57G	total

# Futile Attempts to Reclaim Space

I tried the normal things like:

&gt; docker system prune
 
And when that failed, I tried a bash based approach: 

    #!/bin/bash
    for item in $(docker ps -aq); do
    docker stop $item
    docker rm $item
    done

    for item in $(docker images --filter dangling=true -q); do
    docker rmi $item
    done

But even after both of these, I still had:
    
    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
     57G	total
    
# My Solution

Here was the approach that I used:

1. Quit Docker for Mac entirely.
2. rm /Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2  
3. As best as I can tell this is actually safe because containers can always be rebuilt.
4. Restart Docker
5. Verify space usage:   

  du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
  1.3G total

I then built a container and then reverified disc space usage.

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    1.3G	total

I then built some more containers and then checked disc space usage:

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    3.0G	total

I then started my overall &quot;build all containers&quot; shell script and tracked progress as it went:

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    3.0G	total

    09:58 $ du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    3.1G	total

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    3.4G    total

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    5.4G    total

By the time all containers were built:
 
    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    7.2G    /Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2

And now, several days later and even more deploys, it has increased again:

    du -shc     ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    8.2G	/Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
    8.2G	total

# Read More
* [Great Docker Thread about Disc Space Usage](https://forums.docker.com/t/consistently-out-of-disk-space-in-docker-beta/9438/67)
* [Docker Thread about Disc Space](https://forums.docker.com/t/where-does-docker-keep-images-containers-so-i-can-better-track-my-disk-usage/8370)

</description>
        <pubDate>Wed, 06 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/09/06/reclaiming-docker-disc-space-on-osx.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/09/06/reclaiming-docker-disc-space-on-osx.html</guid>
        
        <category>docker</category>
        
        <category>osx</category>
        
        <category>mac</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Notes on Upgrading to Rails 5.1</title>
        <description>So this morning I upgraded the suite of Rails apps (7 in total) that make up the product that I've been building from Rails 5.0.2 to 5.1.3.  I took my usual, slow as a turtle, approach to doing this upgrade (5.1.4 has now reached RC1 status which means that 1 release behind is uite stable by now).  The main change driving my desire to upgrade was the improved low level connection handling in ActiveRecord which should make developing multi-tenant applications better.

Here are a few notes on upgrading to Rails 5.1.

# The Lines to Change in Gemfile

Previously I had:

    gem 'rails', '~&gt; 5.0.2'
    gem 'puma', '~&gt; 3.0'

which I changed to

    gem 'rails', '~&gt; 5.1.3'
    gem 'puma', '~&gt; 3.10'

The reason for the Puma update turned out to be unneeded but it is a core part of my stack so it is likely good to upgrade.

# Useful Links

* [Rails 5.1 Readme](http://edgeguides.rubyonrails.org/5_1_release_notes.html)
* [Ruby Gems on Puma](https://rubygems.org/gems/puma/versions/3.4.0)
* [Ruby Gems on Rails](https://rubygems.org/gems/rails/versions/5.0.0)

# Don't Be Afraid to Delete Gemfile.lock and Re Bundle

Out of my seven apps, all built on top of the same version of Rails, two had problems with:

&gt; bundle update rails puma

These two applications had issues with the font-awesome-rails gem and railties.  Rather than try and monkey around with them, I simply did a: 

&gt; git rm Gemfile.lock

and then a:

&gt; bundle install

And that fixed everything.

# Puma Now Needs a Space Before the Port

Right after my Rails update and before I did the Puma update, I got this bit of joy when I started my server:

    bundle exec rails s -p3203
    =&gt; Booting Puma
    =&gt; Rails 5.1.3 application starting in development on http://localhost:3203
    =&gt; Run `rails server -h` for more startup options
    Puma starting in single mode...
    * Version 3.10.0 (ruby 2.3.1-p112), codename: Russell's Teapot
    * Min threads: 2, max threads: 2
    * Environment: development
    * Listening on tcp://
    Exiting
    /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:270:in `initialize': getaddrinfo: nodename nor servname provided, or not known (SocketError)
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:270:in `new'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:270:in `add_tcp_listener'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:105:in `block in parse'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:88:in `each'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/binder.rb:88:in `parse'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/runner.rb:144:in `load_and_bind'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/single.rb:87:in `run'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/puma/launcher.rb:183:in `run'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/puma-3.10.0/lib/rack/handler/puma.rb:69:in `run'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/rack-2.0.3/lib/rack/server.rb:297:in `start'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/commands/server/server_command.rb:44:in `start'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/commands/server/server_command.rb:131:in `block in perform'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/commands/server/server_command.rb:126:in `tap'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/commands/server/server_command.rb:126:in `perform'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/thor-0.20.0/lib/thor/command.rb:27:in `run'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/thor-0.20.0/lib/thor/invocation.rb:126:in `invoke_command'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/thor-0.20.0/lib/thor.rb:387:in `dispatch'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/command/base.rb:63:in `perform'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/command.rb:44:in `invoke'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_admin/gems/railties-5.1.3/lib/rails/commands.rb:16:in `&lt;top (required)&gt;'
            from /Users/sjohnson/Dropbox/fuzzygroup/hyde/seira_watch/seira_admin/bin/rails:9:in `require'

This turned out to be a change somewhere in Rails where a space is now needed between the -p and the port number, so this:

&gt; bundle exec rails s -p3203

needs to be:

&gt; bundle exec rails s -p 3203

This was covered in this [Github Issue](https://github.com/rails/rails/issues/28971). Supposedly the latest version of Puma restores the previous functionality where a space isn't needed but I have **NOT** found that to be so.  The solution was to simply accept that a space after -p is required.  Technically I could have not upgraded Puma but it feels like running the current version of is always a good thing.

# Middleware Now Needs a Class Constant Not a String

What I am building uses a multi-tenant approach based on the [Apartment gem](https://github.com/influitive/apartment) and this requires an initializer that specifies a middleware layer.  Prior to Rails 5.1, this was done as follows:

    Rails.application.config.middleware.use 'Apartment::Elevators::Subdomain'
    (in config/initializers/apartment.rb)
    
When I first started my application using Rails 5.1, I got this unpleasant result:

     bundle exec rails s -p 3210
    =&gt; Booting Puma
    =&gt; Rails 5.1.3 application starting in development on http://localhost:3210
    =&gt; Run `rails server -h` for more startup options
    Exiting
    /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/actionpack-5.1.3/lib/action_dispatch/middleware/stack.rb:35:in `build': undefined method `new' for &quot;Apartment::Elevators::Subdomain&quot;:String (NoMethodError)
    Did you mean?  next
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/actionpack-5.1.3/lib/action_dispatch/middleware/stack.rb:99:in `block in build'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/actionpack-5.1.3/lib/action_dispatch/middleware/stack.rb:99:in `each'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/actionpack-5.1.3/lib/action_dispatch/middleware/stack.rb:99:in `inject'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/actionpack-5.1.3/lib/action_dispatch/middleware/stack.rb:99:in `build'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/railties-5.1.3/lib/rails/engine.rb:508:in `block in app'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/railties-5.1.3/lib/rails/engine.rb:504:in `synchronize'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/railties-5.1.3/lib/rails/engine.rb:504:in `app'
            from /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch/gems/railties-5.1.3/lib/rails/application/finisher.rb:45:in `block in &lt;module
    
Apparently middleware layers that used to take a string now need a class constant so this needs to be rewritten as:

    Rails.application.config.middleware.use Apartment::Elevators::Subdomain

This is discussed in this [Rails Issue](https://github.com/rails/rails/issues/28946)

# skip_before_filter Is Now skip_before_action 

Although I was able to start my application correctly in development mode with a still in place skip_before_filter, when I tried it in production, I got:

    =&gt; Run `rails server -h` for more startup options
    Exiting
    /Users/sjohnson/Dropbox/fuzzygroup/hyde/seira_watch/seira_watch_web_app/app/controllers/api_controller.rb:3:in   `&lt;class:ApiController&gt;': undefined method `skip_before_filter' for ApiController:Class (NoMethodError)
    Did you mean?  skip_before_action

This was an easy change but it is still something that could easily trip you up.  Obviously I have been seeing the deprecation warnings for some time now and it is my bad for not having made these changes.

# Conclusions

While mildly annoying these are relatively small issues htat I was able to work around quite cleanly.  The total time to upgrade 7 Rails apps from 5.0.2 to 5.1 was less than an hour in total even including the research and deploy time.  **Recommended**.

# Public Service Announcement

If you haven't upgraded your Ruby Gems executable, you likely should.  [More details are here](https://www.ruby-lang.org/en/news/2017/08/29/multiple-vulnerabilities-in-rubygems/).</description>
        <pubDate>Fri, 01 Sep 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/09/01/notes-on-upgrading-to-rails-5-1.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/09/01/notes-on-upgrading-to-rails-5-1.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>puma</category>
        
        <category>apartment</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Running out of Disc Space with Docker</title>
        <description>I've now been exploring [Docker](https://www.docker.com/) for almost a year now and using it daily for five months or so.  And, as I get ever closer to shipping a product that runs using containers for everything, I have been continually hitting issues regarding running out of disc space.  

# Understanding The Problem

The platform I have been using is:

* Docker Community Edition, Docker version 17.06.0-ce, build 02c1d87
* AWS EC2 instances including ELB
* No use of Kubernetes or the AWS ECS
* Ubuntu Linux
* Docker Hub for image hosting
* Rails / Ruby as an application language

What I have experienced is constant growth in disc space usage ultimately ending up in 0 bytes of free disc space and subsequent failure conditions in most of my containers.

# Docker on Linux Basics

On Ubuntu and most Linux platforms that I understand, the core Docker installation of your data is stored in /var/lib/docker and then a collection of subdirectories.  Here's an example:

    ls -l /var/lib/docker
    total 44
    drwx------ 21 root root  4096 Aug 30 01:17 containers
    drwx------  3 root root  4096 Aug 30 01:10 image
    drwxr-x---  3 root root  4096 Aug 30 01:10 network
    drwx------ 85 root root 12288 Aug 30 01:17 overlay2
    drwx------  4 root root  4096 Aug 30 01:10 plugins
    drwx------  2 root root  4096 Aug 30 01:10 swarm
    drwx------  2 root root  4096 Aug 30 01:16 tmp
    drwx------  2 root root  4096 Aug 30 01:10 trust
    drwx------ 12 root root  4096 Aug 30 03:01 volumes
    
There are two interesting directories here: containers and overlay2.  You should note that on a default docker installation the overlay2 directory would be named aufs.  The directories overlay2 and aufs are different filesystems that Docker can use to store your containers, volumes, etc.  The default Docker filesystem is called aufs and it is the oldest Docker filesystem.  The overlay2 filesystem is newer and seems to have some dramatic advantages.

I am currently involved in what is termed *green field* software development -- this is a development term related to creating a brand new product i.e. everything is a green field waiting to be plowed.  One of the characteristics of green field development is a **lot** of deploys.  As I initially looked into this problem, its characteristics seemed to map directly to the number of deploys -- more deploys meant more disc space used.  When you see this type of situation, it tends to argue that the underlying issue is somehow tied to garbage collection.  My research and analysis of this led me to think that the issue was somehow tied to issues in the Docker aufs filesystem and I switched my installation from aufs to overlay2 and thought it was resolved.

Last night I started getting alerts that my production server was again almost out of disc space (thank you [Monit!](https://github.com/arnaudsj/monit)).  Now the interesting thing is that between when I thought this was resolved and last night, I **have not been deploying at all**.  Over the past 5 days, I have been involved in an intense refactor of my new product's two core [god objects](https://en.wikipedia.org/wiki/God_object) - course.rb and teacher.rb.  In software development parlance, a god object is an object that knows too much or does too much and it is regarded as an [anti pattern](https://en.wikipedia.org/wiki/Anti-pattern).  When you do this type of refactoring, it tends to shut down everything since it breaks, well, **everything**.  Seeing that I was again running out of disc space -- while I wasn't deploying -- argued that my working theory was just plain **wrong**.

My next step was to ssh into the box (yes, even with a containerized architecture, there are still servers and sshing in is still a thing) and look into /var/lib/docker once again.  My general tool for this was the command du -shc *  which translates to &quot;show me the disc space usage at a summary level and translate it to human style (i.e. k / megs / gigs)&quot;.  Here's an example of my command flow: 

    sudo su - 
    cd /var/lib/docker
    du -shc * 
    
    du -shc *
    10G	containers
    11M	image
    140K	network
    3.4G	overlay2
    20K	plugins
    4.0K	swarm
    4.0K	tmp
    4.0K	trust
    3.3M	volumes
    13.5G	total

I started to wonder what could possibly be in the containers directory with a size of **10G** so I changed into that directory and I found an anomaly, a single file, 6.2 gigs in size, like this:

    586e6e0b559281785d023097518ed9303e15db66eee04173792856ff7b2da528-json.log

When I looked at that file, it was a log file showing the log output from the underlying crawler at the coder of the product I have been building.  And with this one discovery, the problem came into focus:

1. While there may have been issues related to a constant ongoing deploy process, the core underlying issue seems to be disc usage due to log file build up.
2. Docker makes it very hard to see the underlying problem since there doesn't seem to be a &quot;where is my damn disc space going&quot; type of command.  Update: Try using &quot;docker system df&quot; to visualize docker disc space usage.  I only found this late in the writing process on this post.  The docker system df command doesn't specifically report log file space usage which I suspect would illuminate this problem.
3. Logs appear to be persistent over time and not reclaimed as you deploy.  My suspicion is that logs are only reset when you stop the Docker daemon (and sometimes not even then).
4. Traditional log management like log rotate won't work unless you restart the Docker daemon.
5. **Sidebar**: I wonder how many people that have struggled with this issue have actually had log file growth issues not actual Docker problems?  Most of the unresolved Docker / Moby issues below don't explore the logs possibility.  

It should be noted that I'm not logging to files from within my application code and I'm using the log to standard out approach from the [Orats gem](https://github.com/nickjj/orats).

# Fixing this Problem Once and For All

Here are the steps that I took to address this problem: 

## Step 1: Stop the Docker Daemon

The first step is shutting down Docker itself:

&gt; sudo service docker stop

## Step 2: Delete /var/lib/docker 

The next step appears drastic and it is.  If you have important data in your Docker system then you're going to lose it at this stage but when I attempted to do this piecemeal, I got bizarre deployment errors related to missing containers and even redeploying did not fix it.

    sudo su
    umount /var/lib/docker/overlay2
    cd /var/lib/docker
    rm -rf * 

This blows away everything in your Docker installation.  The second command line is only necessary if you have already switched your system to overlay2 as I had.

## Step 3: Switch from aufs to overlay2 and Add Log Limits to Docker Config

**Note**: Full use of the overlay2 driver is covered [here](https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/#configure-docker-with-the-overlay-or-overlay2-storage-driver) and should probably be read before you make this switch.  Not everyone can make use of overlay2.

The core docker config file is:

&gt; /etc/docker/daemon.json

You should edit this file and make it look something like this:

    {
      &quot;storage-driver&quot;: &quot;overlay2&quot;,
      &quot;log-driver&quot;: &quot;json-file&quot;,
      &quot;log-opts&quot;: {
        &quot;max-size&quot;: &quot;100m&quot;
      }
    }



**Disclaimer**: I'm honestly not 100% certain that switching from aufs to overlay2 is absolutely required but it was a part of the overall solution and does seem to have benefits so I left it in here although I suspect that the logging is clearly the biggest win here.

## Step 4: Restart Docker Daemon

Start Docker up again:

&gt; sudo service docker start

## Step 5: Add Logging Limits to Your Compose Files

On your local machine where you do your development, you need to set the logging options on a per container basis to your docker-compose file.  The lines to add are to each service are:

    logging:
      options:
        max-size: 50m

Here's an example in the context of a full container:

    services:
      redis:
        image: 'redis:3.2-alpine'
        ports:
          - '6379'
        volumes:
          - 'redis:/var/lib/redis/data'
        restart: on-failure
        logging:
          options:
            max-size: 50m

Theoretically I could have ignored this at a per container level and just relied on the log management defined in /etc/docker/daemon.json but when you have a system level config file, that often gets changed and not checked into version control, belt **and** suspenders is better.  Setting this at the application level and the system level should ensure that I don't get bit by this again.  This will also protect your local dev box from unlimited log growth which could otherwise be a problem since your local dev box isn't configured by the same /etc/docker/daemon.json file.

## Add Cron Jobs for Removing Unused Stuff Periodically

I added cron jobs to my underlying instance for cleaning up after dangling containers and volumes:

    #Ansible: docker rmi images
    1 1 * * * docker rmi -f $(docker images -a -q -f dangling=true)
    #Ansible: docker volume rm
    1 3 * * * docker volume rm -f $(docker volume ls -q -f dangling=true)

The #Ansible comment indicates that these were added by Ansible as part of my machine provisioning script (Step 8 below is now also part of that same script).

## Step 7: Get /etc/docker/daemon.json into Ansible / Version Control

However you configure a new instance, you should make sure that your modified daemon.json file from Step 3 is part of that process or you'll find that setting up a new machine has this same problem.

## Step 8: Redeploy Everything

The final step is to re-deploy everything as all containers, volumes, etc have been deleted earlier in the process.  Hopefully you will find that your disc space usage comes under control.

# Sidebar 1: docker system df

As I was finishing this post, I found the command docker system df which shows you the space docker uses.  Here's an example from my local dev box:

    FuzzygroupMacbookPro2016% docker system df
    TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
    Images              671                 67                  40.59GB             24.07GB (59%)
    Containers          84                  5                   742.2MB             718MB (96%)
    Local Volumes       41                  22                  634.5MB             1.553kB (0%)

# Sidebar 2: docker system prune

The command docker system prune reclaims dangling images and stopped containers.  Here's an example:

    FuzzygroupMacbookPro2016% docker system prune
    WARNING! This will remove:
            - all stopped containers
            - all networks not used by at least one container
            - all dangling images
            - all build cache
    Are you sure you want to continue? [y/N] y
    Deleted Containers:
    4fc625609ac8f86f8d8f9076a8e75d5ccb31c1e871ed6f4589b79de2721af02c
    ... (A long, long list of containers was here)
    Total reclaimed space: 28.72GB

The interesting thing here is that before I ran this I had 46 gigs of free space on my local dev box and after I ran this, I still had 46 gigs free.  I don't know why Docker states that it reclaimed space when it doesn't.

Note: I'm not the only person who can't reclaim this space.  

# What to Learn from This

I would argue that the big takeaway from this isn't actually the specific Docker commands, useful as they are, but the observation that disc space growth wasn't tied to deploys but instead to system operation.  Realizing this changed how I approached the problem.  When you build complex systems, learning how to observe them and then correlating that with what you are doing to the system is a key technique.

# Thank Yous

Most of what I know about Docker, I learned from the courses of [Nick Janetakis](https://diveintodocker.com/).  He is a friend and he pitched in greatly on the analysis and resolution of this.  Thanks man!

# References

* [Interesting Blog Post about Cleaning Up After Docker](http://blog.idetailaid.co.uk/docker-using-up-all-your-disk-space-dont-forget-to-clean-up-after-docker/ ) 
* [Docker Logs on Stack Overflow](https://www.google.com/search?tbs=li:1&amp;q=how+much+space+are+my+docker+logs+taking)
* [Docker Disc Space Quotas and aufs](https://github.com/moby/moby/issues/3804)
* [Docker in Production a History of Failure](https://thehftguy.com/2016/11/01/docker-in-production-an-history-of-failure/)
* [Docker in Production a History of Failure](https://news.ycombinator.com/item?id=12872304) (search for overlay2)
* [Docker not Cleaning Up tmp Files](https://github.com/moby/moby/issues/22207)
* [Docker Orphaned Diffs](https://github.com/moby/moby/issues/29486)
* [Using Overlay2](https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/)
* [A seemingly good shell script for cleaning up after Docker](https://gist.github.com/stanislavb/6634fc35b3d1655201a93d2dd2c3a366)</description>
        <pubDate>Wed, 30 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/08/30/running-out-of-disc-space-with-docker.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/08/30/running-out-of-disc-space-with-docker.html</guid>
        
        <category>docker</category>
        
        <category>aufs</category>
        
        <category>disc_space</category>
        
        <category>bloat</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>When Gems Won't Install - The mkmf.log Problem</title>
        <description>Computing is rich enough and deep enough as a profession that you can spent literally over a decade at something and still learn new things.  And, as frustrating as this is, I find that one of the best parts.   Personally I've been using Ruby now since 2006 and one of the things that is absolutely bedeviling is when a gem won't install.  

**Upfront Disclaimer**: This problem was one of my own making but it proved to be an interesting exercise as I waited for someone to arrive for a meeting.  Far better than simply surfing the web or playing on my phone.

I was trying to get the [FeedBin](https://github.com/feedbin/feedbin) aggregator to install so I did the usual open source Rails dance:

* fork it
* clone it
* bundle install 

And, WHAM, this gave me the wonderfulness below:

    Gem::Ext::BuildError: ERROR: Failed to build gem native extension.

        current directory: /Users/sjohnson/.rvm/gems/ruby-2.3.1@feedbin/gems/nio4r-2.1.0/ext/nio4r
    /Users/sjohnson/.rvm/rubies/ruby-2.3.1/bin/ruby -r ./siteconf20170828-91881-jc70x3.rb extconf.rb
    --with-cflags=-std=c99
    checking for unistd.h... *** extconf.rb failed ***
    Could not create Makefile due to some reason, probably lack of necessary
    libraries and/or headers.  Check the mkmf.log file for more details.  You may
    need configuration options.
    
Nothing like an abstractly referenced log file **WITHOUT A FULL PATH** to be completely unhelpful.  I've been seeing references to mkmf.log about as long as Gems have been in common usage and I've never understood exactly where that persnickety file was located.  Normally a quick Google or Stack Overflow gives me the answer but this time those just weren't helpful.  What I did [discover](https://stackoverflow.com/questions/20379274/when-a-gem-fails-where-do-i-find-the-mkmf-log-file) was that the mkmf.log file is bundled with the gem where it is built.  Given that I use RVM, the example was to look in a path like this:

&gt; ~/.rvm/gems/ruby-1.9.3-p194/gems/some-cool-gem-name/ext/mkmf.log

I started with this find command:

&gt; find ~/.rvm -name mkmf.log

and that produced a huge number of results, 995 to be specific (a decade or more of hacking Ruby stuff will do that).

This took me into adding a grep to the end:

&gt;  find ~/.rvm -name mkmf.log | grep nio4r

which still gave me a full page of output so I added feedbin as a secondary grep:

&gt;  find ~/.rvm -name mkmf.log | grep nio4r | grep feedbin

which finally produced:

&gt; /Users/sjohnson/.rvm/gems/ruby-2.3.1@feedbin/extensions/x86_64-darwin-15/2.3.0/nio4r-2.1.0/mkmf.log

Digging into that file revealed this error:

    xcrun: error: active developer path (&quot;/Applications/Xcode.app/Contents/Developer&quot;) does not exist, use `xcode-select --switch path/to/Xcode.app` to specify the Xcode that you wish to use for command line developer tools (or see `man xcode-select`)
    
So somehow XCode is missing and I'm certain that's my fault (this isn't my primary dev box so perhaps it got deleted at some point to save disc space).  </description>
        <pubDate>Mon, 28 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/08/28/when-gems-won-t-install-the-mkmf-log-problem.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/08/28/when-gems-won-t-install-the-mkmf-log-problem.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>gems</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Rails, Apartment, Tenancy and Sidekiq</title>
        <description># TLDR

This is a long one.  If you need the quick answer on how to solve Apartment-Sidekiq errors then scroll down to the section titled **Correctly Implementing Sidekiq with Apartment**.

# Thanks

This blog post is dedicated to three people / things: 

* My good friend [Nick Janetakis](https://www.nickjanetakis.com) who helped me debug these issues
* [Mike Perham](http://www.mikeperham.com), the author of [Sidekiq](https://www.contribsys.com), whose candor about Apartment-Sidekiq helped point the way
* The [Influitive](https://influitive.com/) team at the Apartment gem; great work guys, thank you

# Overview

Tenancy in database application development refers to using a separate storage system (think a partition within a database) or a whole separate database per &quot;user&quot; (note that user might mean a group of people).  The idea behind tenancy is to: 

* Isolate one user from another; this tends to provide a much more secure approach
* Isolate storage so that the capacity needs of one user don't affect other users

Tenancy is something that we don't often think about but examples of it abound on the Internet.  The best known example is Wordpress.com where every user's data is stored in a separate database.

I've been developing a new product for sometime now and I went into it knowing that if I was successful, storage was going to be a huge issue.  When I first developed [AppData](https://web.archive.org/web/20100610030143/http://www.appdata.com:80/) I had no idea that I was beginning a near [decade long]*https://web.archive.org/web/20160905051931/http://www.appdata.com/ cycle of struggling with storage.  As with all engineers, I was determined to at least not hit the same errors (better to make different ones) so I knew that I needed a tenancy solution for my development platform, Rails, and that led me to the [Apartment gem](https://github.com/influitive/apartment).  Apartment views tenancy as a problem of managing the underlying database connection so that when you goto foo.app.com, foo generally represents the underlying database or &quot;tenant&quot;.

# Enter Sidekiq; Enter Tenancy Problems

When you develop a web application you are quickly going to discover the need for asynchronous processing.  This is a fancy term that could be defined as &quot;don't make the user wait while a long running operation completes&quot;.  Let's consider the very simple case of sending an email with account details once a user signs up.  Sending that email might be instantaneous or the email server might have problems and might take a few seconds.  By handling this operation asynchronously, the email is send by a separate process and control to the user returns instantly.  In the Rails world the leading technology for this is an open source tool called Sidekiq and Sidekiq is a fantastic bit of code.  It runs as a background daemon coupled to your application through Redis and its multi threading scalably handles all kinds of asynchronous needs.  In the past I've used it to handle email deliveries, data imports, whole site crawling and more.

The problem I've been struggling with for sometime now is handling back data imports.  My new product has a cool feature -- you sign up and the back history for your account is imported from a central data archive.  I wanted to avoid the problems with a data tool where data builds over time.  My goal was for people to sign up and then be instantly product, not productive after a week of data acquisition, at which point they might have stopped caring due to the waiting.  This was clearly a job for Sidekiq but how could that work since the underlying database connection changed for every single user?

My tenancy solution was a Ruby gem called Apartment and there is actually an extension gem called [Apartment-Sidekiq](https://github.com/influitive/apartment-sidekiq).  What apartment sidekiq purports to do is push into the redis stream a reference to the tenant and then patch sidekiq so that every time it processes the redis data it will connect to the right tenant.  The problem here is that this seems to only sort of work.  For the past two weeks or so my asynchronous code has worked -- but with errors.  Sometime I would see 1 error related to tenancy when a back data import was processed and sometimes I would see 26 errors per import.

## The Solution - Don't Use Apartment Sidekiq

This problem was on my radar for quite a while and then I finally said &quot;Ok I can't ship until this is addressed&quot; and I've been working it for the past two days.  Understanding the solution came from this [Github Issue](https://github.com/mperham/sidekiq/issues/3005).  The key bit of wisdom is here:

&gt; Just as a side note, passing the tenant as a job argument is a hack. The correct way to implement a cross-cutting concern (like tenant) is with client and server middleware. You just need to copy and configure the two bits of code [here](https://github.com/influitive/apartment-sidekiq/tree/master/lib/apartment/sidekiq/middleware).

When you work with an open source project like Sidekiq, understanding who the people involved are is key because it tells you who to trust.  I've now used Sidekiq for years and years and I trust Mike implicitly when it comes to these matters.  When Mike Perham, the author of Sidekiq, describes something as a **hack**, well, that tells me there might be real issues.

I looked at the related [Stack Overflow](https://stackoverflow.com/questions/41229392/why-is-apartment-sidekiq-not-finding-the-tenant/41471241#41471241) but no where did I have a problem with my environment and that also didn't mesh which Mikes comment about passing the tenant as a job argument.  Sorting through all kinds of tenancy issues took me different places:

* [Possible Version Conflicts](https://github.com/influitive/apartment-sidekiq/issues/10)
* [Maybe It is Rails 5](https://github.com/influitive/apartment-sidekiq/issues/12#ref-pullrequest-190952852)
* [More Perham Commentary](https://github.com/influitive/apartment-sidekiq/issues/11)

In the More Perham Commentary, I found this bit of wisdom:

&gt; @andrba the intention cannot be done safely. You need to explicitly switch and cleanup any connections. I hate that callback and wish I'd never implemented it.

When something like the *underlying callback* on which Apartment-Sidekiq is described in this way it made me realize, &quot;Hm... this isn't going to work, is it&quot;.  

## Correctly Implementing Sidekiq with Apartment

Once I accepted that I couldn't use the Apartment-Sidekiq gem, the solution was pretty obvious:

### Step 1: Remove Apartment-Sidekiq from Gemfile

This was pretty easy - just delete one line and then run bundle install.

### Step 2: Pass the Current Tenant in my Calls to the Sidekiq Worker

Most of my import routines are after_create calls that look like this:

    after_create :import_back_history_sidekiq

This method looks like this:

    def import_back_history_sidekiq
      InstructorImportBackHistoryWorker.perform_async(self.id, Apartment::Tenant.current)
    end
    
There are two parameters here:

* self.id - Sidekiq is oriented around passing low level primitives not full blown ActiveRecord objects so you pass an id reference to the object you want to do an asynchronous call on and then you reload it in the Sidekiq context.
* Apartment::Tenant.current - This is a string that represents the name of the current tenant.  We need to know this because the real solution to this entire problem is to switch to the right tenant in our Sidekiq worker code.

### Step 3: Switching to the Right Tenant

Here's what the underlying Sidekiq worker class looks like:

    class InstructorImportBackHistoryWorker
      include Sidekiq::Worker

      def perform(id, tenant)
        Apartment::Tenant.switch!(tenant)
        instructor = Instructor.where(id: id).first
        if instructor
          instructor.import_back_history
        else
          # Some error handling code goes here
        end
      end
    end

All this does is invoke Apartment::Tenant.switch!(tenant) at the start of the asynchronous processing and that ensures that the correct tenant is used.  After that the import proceeds as normal.  The reason that I have a wrapper approach (import_back_history versus import_back_history_sidekiq) is that I have cases where I use these calls in batch process routines where I don't invoke Sidekiq.

Using this approach to managing tenancy and Sidekiq, I went from multiple tenancy connection errors tracked thru [Errbit](http://fuzzyblog.io/blog/aws/2017/08/11/using-errbit-to-host-your-own-error-tracker-on-aws-for-rails-apps.html) to zero errors.

# References

* [Apartment-Sidekiq Stack Overflow](https://stackoverflow.com/questions/41229392/why-is-apartment-sidekiq-not-finding-the-tenant)
* [Long Error Thread](https://github.com/influitive/apartment-sidekiq/issues/11)</description>
        <pubDate>Mon, 21 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/08/21/rails-apartment-tenancy-and-sidekiq.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/08/21/rails-apartment-tenancy-and-sidekiq.html</guid>
        
        <category>rails</category>
        
        <category>apartment</category>
        
        <category>tenancy</category>
        
        <category>multi_tenant</category>
        
        <category>sidekiq</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Marketing 101 - Ride the Wave If You Can</title>
        <description>**Note**: I haven't written a Marketing 101 piece since 2003 but a friend recently pointed out that even [my old ones](http://fuzzyblog.io/blog/tag.html#marketing101) were pretty good so I'm giving it a shot.

One of the more difficult things in marketing to grasp is the idea of a *wave*.  I don't know if there's a better term for this but wave is how I've been thinking of this for the better part of two decades now.  A wave is an underlying meme or movement that resonates at the industry or even society level and I think the best way to illustrate this is with an example.  My first company, founded in 1987, was a hypertext authoring system (think FrontPage with an integrated browser but back in the DOS days).  There was a small burst of interest in all things hyper* due to Apple's HyperCard but that pretty much subsided by late 1988 / early 1999.  And then, over the next few years the industry shifted, in a huge way, to **multimedia**.  Multimedia became a huge wave -- and we shifted our product features slightly and our marketing dramatically to ride the multimedia wave: 

* We added a small handful of features for controlling digital video discs
* We went to multimedia centric trade shows
* We adopted the multimedia term and related iconography to all our product literature

We saw our fortunes increase in parallel with multimedia -- there is truth in the old aphorism, *a rising tide lifts all boats*.

When you are a teeny, tiny startup, one of the best things that you can do is to *ride the wave* when there is a wave.  I've been consulting recently with an up and coming online education product and my strongest advice to them has been to position their product as a *STEM learning tool*. STEM is the current term in vogue for *science, technology, engineering and mathematics* education.

When you're a pure technical person, the idea of riding a wave can at times be disturbing.  Technical founders tend to think of their product solely in the context where the original idea.  And, when that wasn't the wave, repositioning the product in terms of a wave can feel a bit like being a [carpet bagger](http://www.urbandictionary.com/define.php?term=carpet%20bagger).  I'm here to tell you, both as a technical person and a marketer, that this just is **not** the case.  Waves are often large in nature -- what was multimedia after all -- and as long as the product is credible in terms of the wave, repositioning can actually benefit both you and the product's customers:

For you:

* You get to tap into a market that is growing faster that normal
* You get access to a set of focused marketing events such as trade shows that are wave focused
* You get access to a smaller but more focused group of customers
* Customers are often willing to spend more due to funding specific to the wave (new budgets, grants, etc)

For the customer:

* The customer gets a more specific product
* The customer gets the benefit of at least some wave specific features
* The customer gets the benefit of a company focusing very specifically on their needs

</description>
        <pubDate>Thu, 17 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/marketing/2017/08/17/marketing-101-ride-the-wave-if-you-can.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/marketing/2017/08/17/marketing-101-ride-the-wave-if-you-can.html</guid>
        
        <category>marketing101</category>
        
        <category>marketing</category>
        
        
        <category>marketing</category>
        
      </item>
    
      <item>
        <title>10 Steps to Debugging Containerized Applications</title>
        <description>I've recently been building a new product using Rails based on an approach that I call *Breaking the Monolith*.  Rather than build a traditional Rails monolith, I use multiple microservices / small Rails applications and deploy them all with Docker into a distributed system.  The hardest part about distributed systems is always **debugging** and I've written this guide as a step by step approach for how to go from a misbehaving application right down to a malfunctioning container -- this is the actual process that I follow.

All of this is being done on Ubuntu under AWS but the debugging process applies to any *nix type environment or Platform as a Service (PAAS).

# Disclaimer: But You Don't SSH into Containers...

A lot of the examples below are based around SSH'ing into a server and diagnosing the error in context.  I've seen a lot of things since the advent of containers that seemingly want you to believe, that in this brave new world, you just don't SSH in anymore.  Now, perhaps I am doing things wrong, but I have not found that to be the case at all.  It may be that once I am out of active development that I will no longer be SSH'ing into servers but, for now, SSH is still a dear old friend.

# 01: Failure Context - 504 Gateway Error

The general system for a failure on this application seems to be a 504 Gateway Error which basically means that the application load balancer (ALB) isn't receiving output back from one of the HTTP subsystems.

# 02.  Check the Url in Development after a Server Restart

Every single time you do a deploy with Docker, your entire Gem stack along with any initializers is rebuilt and that means that a stack level change that you made in development but failed to catch can break everything.  So the first diagnosis step is to stop the development server and make sure that things come back up correctly.  Each of my applications runs on a different port so I can't give a single example here -- do the ctrl+c on Puma and then restart it and check the status.

**Note**: If you don't have a simple health check for your application then I would strongly recommend it.  Here's a [simple gist showing a /health url](https://gist.github.com/fuzzygroup/7dec79f94deac117ce591598243f596a) for a Rails application. 

If your Rails app is running on say port 3200 then you can just do: 

    curl http://localhost:3200/health
    
and you should see:

    ok

# 03: Test the Health Check Logged into the Server

Once you've verified that things are correct in development, the next step is to log into the server and run the same curl test on the server where the failure is occurring.  In order to make this type of debugging extremely simple for me, I run all my applications server side on exactly the same port structure that I do in development.  Even http services like the main web site run on their development port since I can let the load balancer handle translation back to 80.  Having a deployment environment that mirrors development is a huge conceptual boon. Assuming our same 3200 port example, we would:

    curl http://localhost:3200/health
    
and you should see:

    ok

**Note:** If the failure is happening solely within the same subsystem then this usually is sufficient to reveal the problem.

# 04: Check the Application Docker Logs

The next step is the application level Docker logs.  My deployer engine, Dockerano, generates a per application shell script which generates logs for the &quot;main&quot; container called dshell so I see something like this

    ./dlogs

    web_1        | 2017-08-11T21:51:59.474344180Z /app/config/initializers/constants_global.rb:6: warning: previous definition of REAL_SKYPE was here
    web_1        | 2017-08-11T21:51:59.474348492Z /app/config/initializers/constants_system.rb:6: warning: already initialized constant STATUS_OK
    web_1        | 2017-08-11T21:51:59.474352887Z /app/config/initializers/constants_global.rb:7: warning: previous definition of STATUS_OK was here
    web_1        | 2017-08-11T21:51:59.474356555Z /app/config/initializers/constants_system.rb:7: warning: already initialized constant HYDE_API_KEY
    web_1        | 2017-08-11T21:51:59.474360577Z /app/config/initializers/constants_global.rb:8: warning: previous definition of HYDE_API_KEY was here
    web_1        | 2017-08-11T21:51:59.783745567Z /app/config/initializers/seira_servers.rb:1: warning: already initialized constant HYDE_API_KEY
    web_1        | 2017-08-11T21:51:59.783780705Z /app/config/initializers/constants_system.rb:7: warning: previous definition of HYDE_API_KEY was here
    web_1        | 2017-08-11T21:52:00.421398929Z /app/app/controllers/home_controller.rb:124: warning: key :course is duplicated and overwritten on line 129
    web_1        | 2017-08-11T21:52:00.421438372Z /app/app/controllers/home_controller.rb:147: warning: key :course is duplicated and overwritten on line 153
    web_1        | 2017-08-11T21:52:01.118340090Z * Listening on tcp://0.0.0.0:3210
    web_1        | 2017-08-11T21:52:01.118720653Z Use Ctrl-C to stop
    
What this is doing under the hood is a simple: 

    #!/bin/bash
    docker-compose -f docker-compose.production.yml logs -f -t web

# 05: Check the Free Disc Space 

As with anything, ever, resource consumption can always be an issue and our normal OS tools include df:

    df -h
    Filesystem      Size  Used Avail Use% Mounted on
    udev            3.9G     0  3.9G 0% /dev
    tmpfs           799M   83M  716M11% /run
    /dev/xvda1       16G  7.0G  8.5G46% /
    tmpfs           3.9G  7.0M  3.9G 1% /dev/shm
    tmpfs           5.0M     0  5.0M 0% /run/lock
    tmpfs           3.9G     0  3.9G 0% /sys/fs/cgroup
    tmpfs           799M     0  799M 0% /run/user/1000

I spent a lot of time on this project trying desperately to use T2.micro instances because, well, they're cheap and, at best, it was a false economy.  Severe bloat within the Docker AUFS filesystem found me continually running out of disc space after multiple deploys even though my containers were actually tiny.  This is a [known Docker Moby issue](https://github.com/moby/moby/issues/22207) that has been open for over a year and a half now and is still **unassigned** to anyone.  

In order to avoid this bug, I ended up moving from multiple T2.micro instances to a single m4.large instance and then doubling the underlying storage from 8 gigs to 16.  And, when I did that, a lot of my issues just magically disappeared.  Being cheap truly was a false economy here because I ended up with fewer instances and not only did my reliability go up but my bill went down.

# 06: Check the CPU Usage and Ram Usage

If you don't have htop installed on all your instances then you really, really should.  htop kicks the absolute snot out of classic top.  Install it with:

    sudo apt-get install htop

And then invoke it with: 

    htop

At this point you can easily see the underlying machine load, etc.

# 07: Look at Individual Container Status

If you're having an issue with a given application then you want to look at all the containers for that application.  The easiest way is to grep by name.  Let's say that your underlying application is called seirawatchwebapp:

    docker ps | grep seirawatchwebapp
    23c1b98a2add        fuzzygroup/seirawatchwebapp_web   &quot;bundle exec clock...&quot;   11 hours ago        Restarting (10) 9 seconds ago                              seirawatchwebapp_clockwork_1
    6eb89122ee73        fuzzygroup/seirawatchwebapp_web   &quot;bundle exec sidek...&quot;   11 hours ago        Up 11 hours                                                seirawatchwebapp_sidekiq_1
    36f49f07273f        fuzzygroup/seirawatchwebapp_web   &quot;/bin/sh -c 'puma ...&quot;   11 hours ago        Up 11 hours                     0.0.0.0:3210-&gt;3210/tcp     seirawatchwebapp_web_1
    49d72363de84        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   11 hours ago        Up 11 hours                     0.0.0.0:32820-&gt;6379/tcp    seirawatchwebapp_redis_1

The thing to be concerned about here is 23c1b98a2add and the reason is that it generally shouldn't be continuously restarting which is what this view shows.

# 08: Application Level logs - Timber.io

I've recently started using [Timber.io](https://www.timber.io) which is a cross application logging environment and I've been very, very happy with it.  If you haven't looked at Timber.io for your Rails development, I'd recommend it.  Even the free tier is actually quite useful.

Timber.io is a full web app rather than a command line tool so you need to log into the Timber service and then select your application where you want to view the logs.

# 09: Check Your Error Logger

If you aren't running a dedicated error tracker, whether HoneyBadger, BugSnag or Errbit, you really, really should.  

# 10: The Answer: Check All Your Containers

What I'm building is a multi-container system, a distributed system in truth, with formal APIs between each of the components and what this means is that a container failure in subsystem X can affect subsystem Y or subsystem Z without it being clear as to why.  The trouble with this type of debugging is getting a high enough level view to understand it as a whole.  

The easiest way to do this on a single machine is to use the **docker stats** command:

    docker stats
    
    CONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS
    b17d34bf7268        0.01%               207.6MiB / 7.795GiB   2.60%               1.1MB / 1.53MB      20.5kB / 7.68MB     23
    5c5460a763b1        0.00%               210.4MiB / 7.795GiB   2.64%               3.62MB / 900kB      1.18MB / 0B         4
    ec28ede65792        0.06%               133.4MiB / 7.795GiB   1.67%               12.9MB / 16.9MB     0B / 0B             9
    bb5839c2a6a7        0.00%               130.9MiB / 7.795GiB   1.64%               1.13MB / 1.61MB     86kB / 16.6MB       13
    5e78dbc9489e        0.09%               6.215MiB / 7.795GiB   0.08%               16.9MB / 12.9MB     69.6kB / 434kB      3
    2d5b14feb009        0.03%               119.3MiB / 7.795GiB   1.49%               8.48MB / 15.4MB     0B / 0B             7
    13d42ed0ba35        0.01%               120.7MiB / 7.795GiB   1.51%               2.51MB / 3.88MB     401kB / 37.7MB      12
    3fbd80153022        0.08%               6.219MiB / 7.795GiB   0.08%               15.4MB / 8.46MB     24.6kB / 434kB      3
    c520fc5504f1        --                  -- / --               --                  --                  --                  --
    26ee413fab7f        0.01%               116.2MiB / 7.795GiB   1.46%               1.23MB / 1.62MB     77.8kB / 16.7MB     11
    1ab35bf6514c        0.07%               6.219MiB / 7.795GiB   0.08%               85.3kB / 0B         127kB / 0B          3
    ac1462fccc60        0.00%               106.1MiB / 7.795GiB   1.33%               1.14MB / 1.37MB     172kB / 7.92MB      10
    422787e2d5ab        0.12%               16.15MiB / 7.795GiB   0.20%               75.9MB / 52MB       754kB / 425MB       3
    b19bb9629925        0.00%               235.6MiB / 7.795GiB   2.95%               93.4MB / 116MB      508kB / 8.19kB      3
    79f484ac7c89        0.18%               388MiB / 7.795GiB     4.86%               6.35GB / 25.4GB     5.24MB / 12.3kB     15
    e34789eed4cc        0.00%               108.5MiB / 7.795GiB   1.36%               22.2kB / 17.7kB     0B / 0B             4
    6f0cd03996a3        0.12%               116.4MiB / 7.795GiB   1.46%               6.33MB / 13MB       45.1kB / 0B         6
    4d8486ff6046        0.23%               344.9MiB / 7.795GiB   4.32%               1.17MB / 1.69MB     1.16MB / 16.7MB     28
    4d978af45ff2        0.16%               6.219MiB / 7.795GiB   0.08%               13MB / 6.34MB       102kB / 442kB       3
    533d96fc2ce1        0.24%               120.7MiB / 7.795GiB   1.51%               9.18MB / 16.7MB     258kB / 0B          6
    6b60a945bff7        0.01%               188.3MiB / 7.795GiB   2.36%               14.5MB / 4.69MB     2.03MB / 0B         10
    9c2e8ec55a06        0.10%               6.215MiB / 7.795GiB   0.08%               16.7MB / 9.17MB     369kB / 475kB       3
    
The curious thing here is that one container, c520fc5504f1, is showing -- for CPU % and all other metrics.  Let's zoom in on that one.  Personally I find the view above to be more granular than needed and missing the application specific details that I need so my deployer generates a dstats shell script which does this:

    ./dstats 
    CONTAINER                      CPU %               MEM USAGE / LIMIT     CONTAINER ID
    seirawatchshop_sidekiq_1       --                  -- / --               c520fc5504f1321d44078ea3df8a2f1ffc9147d0cb117564e913596eda76db32
    seirawatchshop_web_1           0.01%               91.61MiB / 7.795GiB   cc24d6bcc576cc1ce0c7d04ba8af6b51a1b7599fbbfdcf0c1cb98bd1553e1224
    seirawatchshop_redis_1         0.09%               6.215MiB / 7.795GiB   796829845da6dd5f5f63b7449ce3c95581e386e691626344b5b59010b088311d
    seirasearch_web_1              0.47%               207.6MiB / 7.795GiB   b17d34bf72689e75f6332f387f964e1e3f2803c3cf544d3bb7de6d7b501c2a38
    seirawatchwebapp_clockwork_1   0.01%               210.4MiB / 7.795GiB   5c5460a763b11f4347a0dc806cc7ff5b1314220a9d93ed55f729d4853c1d094f
    seirawatchwebapp_sidekiq_1     0.32%               133.4MiB / 7.795GiB   ec28ede65792d435c7b3042909925a17c79182e820dfa16964be151b07241c07
    seirawatchwebapp_web_1         0.00%               131MiB / 7.795GiB     bb5839c2a6a7e87e694baa84d1f7b102c54aebde6991203da14836fa60006742
    seirawatchwebapp_redis_1       0.14%               6.215MiB / 7.795GiB   5e78dbc9489ecd3b050715b0aa1fe002cebb4636a858e17f93da56f5736aae2d
    seirawatchsite_sidekiq_1       0.10%               119.3MiB / 7.795GiB   2d5b14feb009aa4eaf72cde6969bb271a6c81d6cd196a9648a5c64184ffab242
    seirawatchsite_web_1           0.02%               121.1MiB / 7.795GiB   13d42ed0ba35bd56ffa0977fde429759f4713ec8e58ae0715e273d040a7a276c
    seirawatchsite_redis_1         0.10%               6.219MiB / 7.795GiB   3fbd801530227940730abe8197b24d3e24d52edf9bdfb4145d30e3ae40399418
    seirawebappapi_web_1           0.00%               106.1MiB / 7.795GiB   ac1462fccc60a569d86ce0a0b7939a66af42ae07328a261265d90bcc6928c372
    seiracrawler_redis_1           0.15%               16.15MiB / 7.795GiB   422787e2d5ab64647fd67a0c9c25af2a8ecdbb7ce3ab0a6bc7ca16ededb5a93b
    seiracrawler_rake_1            0.01%               236.5MiB / 7.795GiB   b19bb96299253434c9ea94f4e1c9640b255e5f359c3af7060acd49a6070ffc01
    seiracrawler_sidekiq_1         0.14%               400MiB / 7.795GiB     79f484ac7c8958742dc744da0404652ff100a9e299080014a4d64b5b78ea000c
    seiraadmin_clockwork_1         0.01%               108.5MiB / 7.795GiB   e34789eed4cc4a6bf7173ef3f278033799272e116f3f670ceb0754b521065b0d
    seiraadmin_sidekiq_1           0.08%               116.4MiB / 7.795GiB   6f0cd03996a3414ee7bc0c3fb76adb542d06e3c1c0f7fa1fe108f156a9d9ac0f
    seiraadmin_web_1               0.25%               121.3MiB / 7.795GiB   4d8486ff604656515f0e0e433b99486cf91131ce4a93b82701aa42ef77015155
    seiraadmin_redis_1             0.11%               6.219MiB / 7.795GiB   4d978af45ff2db55c08212e404264422a39f4ee4fa4b6656b149a37df65bf147
    shouldigem_sidekiq_1           0.06%               120.7MiB / 7.795GiB   533d96fc2ce1fb492faa477dfd171e370a30c107becce00e5a52d56bfe3b2622
    shouldigem_web_1               0.10%               187.1MiB / 7.795GiB   6b60a945bff74748f3c447ff7b57f75ef12518758c79e75bd587d59531771f3e
    shouldigem_redis_1             0.09%               6.215MiB / 7.795GiB   9c2e8ec55a066d382d0ee0ceadfb3e061196127f6b334e16b73e43551ee4f435

Note 1: Source for dstats is [here](https://gist.github.com/fuzzygroup/b4293b4a7d15a9d8ea88a50ddb2ab3f0).

Note 2: If you're curious about how to configure the output of docker stats then see [this link](https://docs.docker.com/engine/reference/commandline/stats/#formatting).

If I then docker ps and grep either with the name or the process id, I will see the same container:

    docker ps | grep seirawatchshop_sidekiq_1
    631359e0cec8        fuzzygroup/seirawatchshop_web     &quot;bundle exec sidek...&quot;   10 hours ago        Restarting (1) 46 seconds ago                             seirawatchshop_sidekiq_1

    docker ps | grep 631359e0cec8
    631359e0cec8        fuzzygroup/seirawatchshop_web     &quot;bundle exec sidek...&quot;   9 hours ago         Restarting (1) 26 seconds ago                             seirawatchshop_sidekiq_1

Once we have this then we can get just the logs on this one container:

    docker logs 631359e0cec8
    
    Error connecting to Redis on 127.0.0.1:6379 (Errno::ECONNREFUSED)
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:345:in `rescue in establish_connection'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:331:in `establish_connection'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:101:in `block in connect'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:293:in `with_reconnect'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:100:in `connect'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:364:in `ensure_connected'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:221:in `block in process'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:306:in `logging'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:220:in `process'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis/client.rb:120:in `call'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis.rb:251:in `block in info'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis.rb:58:in `block in synchronize'
    /usr/local/lib/ruby/2.3.0/monitor.rb:214:in `mon_synchronize'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis.rb:58:in `synchronize'
    /usr/local/bundle/gems/redis-3.3.3/lib/redis.rb:250:in `info'
    /usr/local/bundle/gems/sidekiq-4.2.7/lib/sidekiq.rb:113:in `block in redis_info'
    /usr/local/bundle/gems/sidekiq-4.2.7/lib/sidekiq.rb:95:in `block in redis'
    /usr/local/bundle/gems/connection_pool-2.2.1/lib/connection_pool.rb:64:in `block (2 levels) in with'
    /usr/local/bundle/gems/connection_pool-2.2.1/lib/connection_pool.rb:63:in `handle_interrupt'
    /usr/local/bundle/gems/connection_pool-2.2.1/lib/connection_pool.rb:63:in `block in with'
    /usr/local/bundle/gems/connection_pool-2.2.1/lib/connection_pool.rb:60:in `handle_interrupt'
    /usr/local/bundle/gems/connection_pool-2.2.1/lib/connection_pool.rb:60:in `with'
    /usr/local/bundle/gems/sidekiq-4.2.7/lib/sidekiq.rb:92:in `redis'
    /usr/local/bundle/gems/sidekiq-4.2.7/lib/sidekiq.rb:106:in `redis_info'
    /usr/local/bundle/gems/sidekiq-4.2.7/lib/sidekiq/cli.rb:71:in `run'
    /usr/local/bundle/gems/sidekiq-4.2.7/bin/sidekiq:12:in `&lt;top (required)&gt;'
    /usr/local/bundle/bin/sidekiq:22:in `load'
    /usr/local/bundle/bin/sidekiq:22:in `&lt;top (required)&gt;'

So what's going on here is clearly at the Sidekiq stack layer and it is some type of connection to the underlying containerized Redis instance.  Once we know that, troubleshooting this should actually be pretty simple; it eventually turned out to be a missing pair of files - config/sidekiq.yml.erb and config/initializers/sidekiq.rb that had been overlooked in my initial configuration.

Just as a sidebar, a docker ps -a is also sometimes useful:

    docker ps -a
    CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS                          PORTS       NAMES
    b17d34bf7268        fuzzygroup/seirasearch_web        &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3230-&gt;3230/tcp    seirasearch_web_1
    5c5460a763b1        fuzzygroup/seirawatchwebapp_web   &quot;bundle exec clock...&quot;   9 hours ago         Up 9 hours       seirawatchwebapp_clockwork_1
    ec28ede65792        fuzzygroup/seirawatchwebapp_web   &quot;bundle exec sidek...&quot;   9 hours ago         Up 9 hours       seirawatchwebapp_sidekiq_1
    bb5839c2a6a7        fuzzygroup/seirawatchwebapp_web   &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3210-&gt;3210/tcp    seirawatchwebapp_web_1
    5e78dbc9489e        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:32779-&gt;6379/tcp   seirawatchwebapp_redis_1
    2d5b14feb009        fuzzygroup/seirawatchsite_web     &quot;bundle exec sidek...&quot;   9 hours ago         Up 9 hours       seirawatchsite_sidekiq_1
    13d42ed0ba35        fuzzygroup/seirawatchsite_web     &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3200-&gt;3200/tcp    seirawatchsite_web_1
    3fbd80153022        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:32777-&gt;6379/tcp   seirawatchsite_redis_1
    631359e0cec8        fuzzygroup/seirawatchshop_web     &quot;bundle exec sidek...&quot;   9 hours ago         Restarting (1) 58 seconds ago       seirawatchshop_sidekiq_1
    26ee413fab7f        fuzzygroup/seirawatchshop_web     &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3205-&gt;3205/tcp    seirawatchshop_web_1
    1ab35bf6514c        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:32775-&gt;6379/tcp   seirawatchshop_redis_1
    ac1462fccc60        fuzzygroup/seirawebappapi_web     &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3220-&gt;3220/tcp    seirawebappapi_web_1
    422787e2d5ab        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:32774-&gt;6379/tcp   seiracrawler_redis_1
    b19bb9629925        fuzzygroup/seiracrawler_rake      &quot;bundle exec clock...&quot;   9 hours ago         Up 9 hours       seiracrawler_rake_1
    79f484ac7c89        fuzzygroup/seiracrawler_rake      &quot;bundle exec sidek...&quot;   9 hours ago         Up 9 hours       seiracrawler_sidekiq_1
    e34789eed4cc        fuzzygroup/seiraadmin_web         &quot;bundle exec clock...&quot;   9 hours ago         Up 9 hours       seiraadmin_clockwork_1
    6f0cd03996a3        fuzzygroup/seiraadmin_web         &quot;bundle exec sidek...&quot;   9 hours ago         Up 9 hours       seiraadmin_sidekiq_1
    4d8486ff6046        fuzzygroup/seiraadmin_web         &quot;/bin/sh -c 'puma ...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:3203-&gt;3203/tcp    seiraadmin_web_1
    4d978af45ff2        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   9 hours ago         Up 9 hours                      0.0.0.0:32773-&gt;6379/tcp   seiraadmin_redis_1
    533d96fc2ce1        fuzzygroup/shouldigem_web         &quot;bundle exec sidek...&quot;   10 hours ago        Up 10 hours       shouldigem_sidekiq_1
    6b60a945bff7        fuzzygroup/shouldigem_web         &quot;/bin/sh -c 'puma ...&quot;   10 hours ago        Up 10 hours                     0.0.0.0:3500-&gt;3500/tcp    shouldigem_web_1
    9c2e8ec55a06        redis:3.2-alpine                  &quot;docker-entrypoint...&quot;   10 hours ago        Up 10 hours                     0.0.0.0:32770-&gt;6379/tcp   shouldigem_redis_1
    6c2b013947d1        google/cadvisor:latest            &quot;/usr/bin/cadvisor...&quot;   11 hours ago        Exited (137) 11 hours ago       cadvisor2
    34e1edbad906        google/cadvisor:latest            &quot;/usr/bin/cadvisor...&quot;   11 hours ago        Created       cadvisor1
    8478172d0f2b        google/cadvisor:latest            &quot;/usr/bin/cadvisor...&quot;   11 hours ago        Exited (137) 11 hours ago       cadvisor
    5b5c1709c1af        errbit/errbit:latest              &quot;bundle exec puma ...&quot;   3 days ago          Exited (0) 10 hours ago       errbit_errbit_1
    ac91a943e789        mongo:3.2                         &quot;docker-entrypoint...&quot;   3 days ago          Exited (0) 10 hours ago       errbit_mongo_1

# 11: Go Nuclear - Restart the Docker Daemon

The absolute nuclear approach here is to simply restart the [docker daemon](https://docs.docker.com/engine/admin/) itself.  On Ubuntu, this is:

    sudo service docker restart
    
I'm not going to go so far as to say that you don't have to, rarely, restart the docker daemon but it is just that -- **rare**.  Your problems are far, far, far more likely to be application side errors, even when it looks like Docker is at fault.  I've mistakenly pointed the finger at Docker too many times -- and I was **wrong**.

# Pitch for a Friend: Learn from Nick

All my Docker knowledge came from Nick Janetakis’ [Dive into Docker](https://diveintodocker.com/) course and he does a great job teaching about Docker. He also kibitzed with me on this debugging process although he never saw the final draft before it went live.  Any errors are mine not his.  Strongly recommended.
</description>
        <pubDate>Tue, 15 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/08/15/10-steps-to-debugging-containerized-rails-applications.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/08/15/10-steps-to-debugging-containerized-rails-applications.html</guid>
        
        <category>docker</category>
        
        <category>debugging</category>
        
        <category>rails</category>
        
        <category>monolith</category>
        
        <category>aws</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Fundamental Usability Problems with Medium</title>
        <description>Yesterday I was at the movies with my son and during the endless roll of trailers, promos and advertisements, I was terribly, terribly bored so I dropped into the Medium app and saw something that I **knew** I wanted to read in more depth so I hearted it (by hearted I mean I clicked on the heart icon on the toolbar and watched it change color).  I figured that would be enough to find it later.  I also remembered a little bit about it:

* author: james altschuler
* keywords: hedge fund marriage thestreet.com 

Just to cut to the chase, here's the [exact article](https://medium.com/the-mission/step-by-step-guide-to-make-10-million-and-then-totally-blow-it-a9283b6de90c) but the process by which I found something that I had:

* already read
* that I had hearted as a favorite
* that I knew had certain keywords

was an absolute usability failure and one that I just don't understand.

# Usability Failure #1 - Where Are My Hearted Things?

The next day I went to Medium in my laptop's browser and signed in and I **could not** find the things I liked.  So I looked again, nope.  My next thought was ok maybe these are just stored in the Medium app on my phone -- nope I couldn't find anything there either.  This led me back to my laptop's browser where I finally googled it and found a [Quora post](https://www.quora.com/How-do-I-find-a-list-of-all-the-articles-I-recommended-on-Medium) that talked about it but their solution was no longer part of the Medium user interface.  They also pointed out that you could goto http://medium.com/@user/has-recommended and find your posts.  I typed in the correct url for me: [http://medium.com/@fuzzygroup/has-recommended](http://medium.com/@fuzzygroup/has-recommended) and that actually did work but the question remains:

&gt; Medium has serious founders, serious money, serious talent -- how can this be a problem?

**Note**: I never did find a way to get back to my hearted items on my phone.

# Usability Failure #2 - Search

When I couldn't find the article I **knew** was there, I turned to search as the next obvious way to find it.  Here was my process:

1.  So I started with a search for [james altschuler](https://medium.com/search?q=james%20altschuler) and that gave zero hits.  
2.  Then I continued with a search for [thestreet.com](https://medium.com/search?q=thestreet.com) and that gave me hits but not the article I knew I had read.  Now I have no problem with a spelling error on altschuler but &quot;thestreet.com&quot; -- **nope**.
3.  I then proceeded to search for [hedge fund](https://medium.com/search?q=hedge%20fund) and again the article that I wanted wasn't there.
4.  Ok I'll accept that there are lots and lots of articles about hedge funds but how many [hedge fund articles talk about the collapse of his marriage](https://medium.com/search?q=hedge%20fund%20marriage) - that must work, right?  Nope.  That three word query brings back just a [single article](https://medium.com/thinkprogress/hedge-fund-manager-runs-anti-gay-attack-ad-against-liz-cheney-2e851c82b758).  Interestingly Google finds [360 things](https://www.google.com/search?biw=1313&amp;bih=646&amp;q=site%3Amedium.com+%22hedge+fund%22+%22marriage%22&amp;oq=site%3Amedium.com+%22hedge+fund%22+%22marriage%22&amp;gs_l=psy-ab.3...34964.39387.0.39831.4.4.0.0.0.0.62.207.4.4.0....0...1.1.64.psy-ab..0.0.0.FNpHjvAPKsM) on medium that contain hedge fund and marriage and the post I was looking for was on the second page.  I ran the google search with &quot;hedge fund&quot; and &quot;marriage&quot; to force all the terms to be in there as well as to treat &quot;hedge fund&quot; as a string.

# How Did I Finally Find It?

I finally ended up giving up on Medium's user interface and its search engine and using my incorrect version of the author's name and [Google found the right author as the very first result](https://www.google.com/search?q=site%3Amedium.com%20james%20altschuler).  I then scrolled down and found [the article](https://medium.com/the-mission/step-by-step-guide-to-make-10-million-and-then-totally-blow-it-a9283b6de90c).

# Conclusion

By any Silicon Valley standards, Medium is a real thing: 

* [5 years old](https://en.wikipedia.org/wiki/Medium_(website))
* Incredible Founder - Blogger and Twitter were Ev's prior companies
* Huge $$$ - More than [$107 Million](https://techcrunch.com/2016/04/21/medium-series-c/)
* Has north of [167 employees](https://www.quora.com/How-many-employees-does-Medium-have-What-do-they-do) as of November 2015

It is very, very hard for me to understand this level of usability failure at a company with these type of metrics.  I can, perhaps, excuse the user interface to hearted items as being a redesign where something got lost in the shuffle.  But the search failure is deeply, deeply troubling.  If you're in the content business then search is a **requirement** not an option and a search algorithm that: 

* deals with misspellings
* takes into account user input such as hearts / likes
* actually finds the damn keywords the user puts in 

should not be a problem in 2017.  I'm spent my career in search in retrieval and we were solving these types of issue back in the early 90s if not sooner.  I just can't fathom exactly what Medium is doing but they would likely be better served at this point by just embedding google instead of whatever search tool they've 

# Obligatory Advertisement for Myself

If you have search and retrieval needs, I've spent a long time working on these kinds of issues and I'm always happy to talk to potential clients.  My contact details are on [my resume](http://fuzzyblog.io/blog/resume.html).</description>
        <pubDate>Mon, 14 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/medium/2017/08/14/the-problem-with-medium.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/medium/2017/08/14/the-problem-with-medium.html</guid>
        
        <category>medium</category>
        
        <category>blogging</category>
        
        
        <category>medium</category>
        
      </item>
    
      <item>
        <title>When Font Awesome Won't Render in Production on Rails 5.x</title>
        <description>[Font Awesome](http://fontawesome.io/) is one of those staggeringly good open source projects that you don't realize just how good it is until you use it -- and then it goes away.  The degree of *polish* and *finish* that an easy to use, inline icon adds to a project honestly just astounds me.  That's why, when I moved some code to production recently, and Font Awesome failed to render, well, I was **beyond frustrated**.

My current production platform is Rails 5.x and this is how I got past that particular hellish little bit of the Rails asset pipeline.  The context is that I've been developing with Font Awesome for over 8 months now and I even spent the $$$ to back their [KickStarter](https://www.kickstarter.com/projects/232193852/font-awesome-5/comments) since Font Awesome is just that, well, **awesome** and I've been viewing their icons daily for 8 months -- until I deployed at which point they vanished.  

On digging into this problem, I found a lot of discussion and suggestions in the [normal ](https://stackoverflow.com/questions/17904949/rails-app-not-serving-assets-in-production-environment) [places](https://github.com/FortAwesome/Font-Awesome/issues/5559) but I never found this solution.

1. Diagnose the problem by looking at the Chrome JavaScript console and seeing if you get a 404 error related to missing font files such as *fontawesome-webfont.woff2*.  That's the issue that this solution tackles.
2. Use the font-awesome-rails gem in Gemfile: **gem &quot;font-awesome-rails&quot;**.
3. In your application.css.scss file you should have an import directive at or near the top like @import &quot;font-awesome&quot;;
4. In config/environments/production.rb, you need a series of asset compilation directives like these: 

    # Do not fallback to assets pipeline if a precompiled asset is missed.
    config.assets.compile = false
    # font_awesome additions
    config.serve_static_assets = true
    config.assets.compress = true
    config.assets.compile = true
    config.assets.digest = true

# Before and After

Here's the obligatory before and after screenshots:

![font_awesome_broken.png](/blog/assets/font_awesome_broken.png)    

![font_awesome_broken.png](/blog/assets/font_awesome_working.png)    

The working screenshot has slightly different data in it since this was written over a series of days while this issue was researched.

# Honest Disclaimer

I've now been using Rails since 2007 and Ruby since 2006 (and HTML since 93) which means that I so thoroughly precede the asset pipeline that it honestly makes me laugh.  In all honesty, I really don't understand the asset pipeline particularly well so if this advice is technically wrong, I **apologize**.  What I can tell you is that this advice took a Rails 5 system where Font Awesome worked in development but failed in production and made it work in production -- but when you don't understand the low level details, well, it makes you *uncomfortable*.
</description>
        <pubDate>Sat, 12 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/08/12/when-font-awesome-won-t-render-in-production-on-rails-5-x.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/08/12/when-font-awesome-won-t-render-in-production-on-rails-5-x.html</guid>
        
        <category>rails</category>
        
        <category>font_awesome</category>
        
        <category>asset_pipeline</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Using Errbit To Host Your own Error Tracker on AWS for Rails Apps</title>
        <description>In this tutorial, I walk you through using [the Errbit project](https://github.com/errbit/errbit), an open source error tracker to host your own error tracker.  Errbit is a competitor to [HoneyBadger](http://www.honeybadger.io), [BugSnag](http://www.bugsnagcom/), [AirBrake](http://www.airbrake.io) and other similar hosted error tracking tools that generally cost $29 to $49 / month or more.  Using Errbit means you only pay for infrastructure capacity and end up with a solution you can maintain yourself.

I've written about HoneyBadger [previously](http://fuzzyblog.io/blog/containers/2016/08/26/in-the-world-of-containers-honeybadger-will-reign-supreme-bye-bye-airbrake.html) and it really is an excellent, excellent tool but I'm cheap and I wanted to experiment with an open source project that I've known about for years and never had the opportunity.  Although Errbit really is excellent, I found the getting started documentation lacking hence this post.

I initially tried to get this running on Heroku but that resulted in what I termed [Heroku Fail](http://fuzzyblog.io/blog/fail/2017/08/08/utter-and-complete-heroku-fail.html).

# Step 1: Getting it Running Locally Using Docker

Errbit is a modern Rails app and it requires: 

* Rails
* Mongo

The easiest way to get this running locally is to just pull down the docker-compose.yml file from the github repo and use docker-compose to run it.  If you don't have Docker installed then you need to [install it now](http://www.docker.com/) for development and on your server for production use.  After that:

    mkdir errbit
    cd errbit
    wget https://github.com/errbit/errbit/blob/master/docker-compose.yml
    docker-compose up 

Once that is running, in a separate terminal window you need to bootstrap the installation and generate an admin user.  Do that with: 

     docker-compose exec errbit bundle exec rake errbit:bootstrap

The default errbit port is 8080 and you can access it by going to http://localhost:8080 where you can use the credentials you generated by bootstrapping it to log in.  

Once you've logged in, you need to add your app by clicking on the Add a new App button and then setting: 

* app name
* github repo
* issue tracker
* notification service

Once this is done, Errbit will generate you instructions for what to add to your Gemfile and what to configure in the errbit.rb initializer.  Generally this looks something like this:

    # Require the airbrake gem in your App.
    # ---------------------------------------------
    #
    # Ruby - In your Gemfile
    # gem 'airbrake', '~&gt; 5.0'
    #
    # Then add the following to config/initializers/errbit.rb
    # -------------------------------------------------------

    Airbrake.configure do |config|
      config.host = 'http://localhost:8080'
      config.project_id = 1 # required, but any positive integer works
      config.project_key = '2020e526a09c78462f0f9d45010efc6c'

      # Uncomment for Rails apps
      # config.environment = Rails.env
      # config.ignore_environments = %w(development test)
    end

If you want to test Errbit's integration then you can use this rake task:

    rake airbrake:test
    
Once you've confirmed that this works, you can proceed to Step 2 and move it a server.

# Step 2: Moving it to a Server

Installing Errbit on a docker enabled server really is exactly the same as you did above.  Here's the quick recap:

1.  Log into your server.
2.  Make an errbit directory where you want it.
3.  wget https://raw.githubusercontent.com/errbit/errbit/master/docker-compose.yml
4.  Run **docker-compose up** 
5.  In another terminal window, run **docker-compose exec errbit bundle exec rake errbit:bootstrap** and make note of the credentials.

At this point you have an http service running errbit on port 8080 which your AWS security group likely isn't configured for and that brings us to step 3.

# Step 3: AWS Configuration

Our goal in this step is to configure a domain name like errbit.foo.com where foo.com is your base domain name.  I'm assuming that you are using the standard AWS tools including Route 53 for DNS and an ALB for load balancing.  Here are the things we need to do to make this work on AWS: 

1. Create a dns name for it.
2. Open a security group port for it.
3. Add it to our load balancer.
4. Add it to our monitoring tool

## Creating a DNS Name for It

Here are the steps to follow:

1.  Go into AWS console for Route 53.
2.  Select your main domain name, the &quot;foo.com&quot; from above.
3.  Select Create Record Set.
4.  In the name field enter errbit and then choose that it is type A and that it is an alias.
5.  In the alias target select your load balancer.
6.  Click the Create button.

This has created the domain name errbit.foo.com and assigned it to your load balanced AWS stack.  

## Opening a Security Group Port

Your AWS security group is really nothing more than a firewall and to let traffic through you need to expose a hole for the port 8080.

1.  Go into the AWS console for EC2 instances.
2.  Select Security Groups from the left hand pane.
3.  Click the Inbound tab and then click the Edit button.
4.  Scroll all the way down to the bottom and click Add Rule.
5.  Enter 8080 into the Port Range field as a custom TCP rule accessible to everywhere and then click the Save button.

## Adding Errbit to Your AWS ALB Load Balancer

At this point we just need to add Errbit to our AWS load balancer.  Please note that if you aren't using a load balancer then at this point you would actually be done but since I have one, I should go through all the way to the end.

The first step is to define a Target Group for your load balancer.  This allows you to map a service on a given port to an AWS instance.

1. Go into the AWS console for EC2 instances.
2. Select Target Groups from the left hand pane.
3. Click the Create Target Group button.
4. Enter errbit as the Target group name and specify /robots.txt as the health check path and then click Create.

Once you have a Target Group then you need to add an actual Target for the group to serve traffic to:

1.  Select the errbit Target Group in the list of target groups on the top.
2.  Select the Targets tab on the bottom.
3.  Click the Edit button.
4.  **IMPORTANT**: This next step is confusing so please pay attention.  You are now adding from a list on the bottom of the screen to a list on the top and then saving your work with a button on the bottom.  Honestly this is kind of a *shite show* as far as UI design goes but it does work; it is just tricky and unintuitive.
5.  Select your instance where errbit is installed from the list of instances on the bottom and then check its box on the far left.  Enter 8080 in the port field and then click the Add to registered button.  
6.  Now click the Save button.

You now have both a target group and a target which means you can now create a load balancer rule to process the traffic.

1.  Go into the AWS console for EC2 instances.
2.  Select Load Balancers from the left hand pane
3.  Select your load balancer from the list of load balancers at the top.
4.  Select Listeners from the bottom grouping of tabs.
5.  Out of the box, Errbit only supports http not https so on the http listener select View/edit rules.
6.  Select the + icon to add a rule.
7.  At the top of the load balancer select the Insert Rule link.
8.  Add errbit.foo.com (make sure you specify your correct base domain) to the Host field in the **IF** section of the rule.
9.  Select your target group from the Forward to section of the **THEN** section of the rule.
10. Click the Save button.

# Conclusion

At this point you should goto your url, something like http://errbit.foo.com/ and you should get a login screen where you can use the credentials you generated earlier.

Your first tasks now need to be:

1.  Add yourself as a user so your email address is supported for notifications (note I still need to configure email sending and that will happen but likely in a later blog post).
2.  Add any other team members.
3.  You need to add all of your applications to Errbit and then configure your applications accordingly.
4. Use the Rake task above to test each of your applications and verify that they are connected to errbit.

# If You Need to Learn Docker

All my Docker knowledge came from [Nick Janetakis' Dive into Docker course](https://diveintodocker.com/) and he does a great job teaching about Docker.  Strongly recommended.</description>
        <pubDate>Fri, 11 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/08/11/using-errbit-to-host-your-own-error-tracker-on-aws-for-rails-apps.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/08/11/using-errbit-to-host-your-own-error-tracker-on-aws-for-rails-apps.html</guid>
        
        <category>rails</category>
        
        <category>aws</category>
        
        <category>errbit</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Referencing Images in Rails 5 CSS Stylesheets</title>
        <description>I recently had the issue where I switched the images in my stylesheet from an absolute url on someone else's domain to a relative path indicating a file in my /app/assets/images directory and that worked fine -- until I deployed and then my image disappeared.  It took a few iterations to figure it out and here's the trick:

1.  Make sure you are using the sass-rails gem which provides the necessary helpers.
2.  For an image located in /app/assets/images/intro-bg.jpg, assuming that it is a CSS background image, you want to reference it as background: url(asset_path(&quot;intro-bg.jpg&quot;)).
3.  You need to rename your stylesheet with a .scss extension so it is fed through the preprocessor.

Here's a [Stack Overflow](https://stackoverflow.com/questions/15257555/how-to-reference-images-in-css-within-rails-4) on this but please note that the highest answer was actually wrong (at least for me). 

You should also note that, in my opinion, the asset pipeline is tricky and I no longer trust anything in it until I've deployed to production and tested it myself.  
</description>
        <pubDate>Wed, 09 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/08/09/referencing-images-in-rails-5-stylesheets.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/08/09/referencing-images-in-rails-5-stylesheets.html</guid>
        
        <category>rails</category>
        
        <category>CSS</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Expanding an AWS Instance Volume</title>
        <description>One of the true promises of cloud computing is freeing you from resource limits.  In an ideal world you'd be able to easily example: 

* ram
* storage
* networking
* cpu

And while this is somewhat true, the reality is always a bit trickier.  I've had a new project underway for a while and I've been plagued by constant disc space issues.  And while this is a multi container project, my core /var/lib/docker/containers directory is only 1.1 gigs according to a du -sh /var/lib/docker/containers and /var/lib/docker/volumes is only 288K but my /var/lib/docker/aufs directory is 14 + gigs.  Despite diving deeply into the [spotify-gc project](https://github.com/spotify/docker-gc) and the [open issue on Moby](https://github.com/moby/issues/22207), none of these actually solved the issue.

So it was time to just suck it up and add more disc space -- even though I am an admitted cheap bastard and this bothers the hell out of me.  I've done this in the past and while it wasn't [as bad as the last time](http://fuzzyblog.io/blog/aws/2016/11/26/fear-and-loathing-in-awsville-or-adventures-in-partition-resizing.html), it still wasn't obvious.

# How to Expand Your AWS Volume on a Running Instance

I've broken this into two stages.  Here is stage 1 - using the web UI.  **Keep in mind that you are going to need to do a reboot at the end and that means some level of downtime so plan this out accordingly.**

1.  Start at your dashboard on [console.aws.amazon.com](http://console.aws.amazon.com) or however you log in and go into the EC2 section.
2.  Select volumes on the left hand sidebar.
3.  Select the instance.
4.  From the actions drop down select Modify Volume.
5.  Change the disc space to whatever you want and click ok.

This starts the process of AWS reallocating your disc.  If you're on Linux you now need to proceed to stage two:

1.  Log into your box.
2.  Run a df -h and see if the new space shows up.  It almost certainly won't but you can always try.
3.  Run a lsblk to list your blog devices and you need to note the one that your system is using.  The default is /dev/xvda1
4.  Run a sudo resize2fs /dev/xvda1
5.  Reboot the box with /sbin/reboot
6.  Wait the appropriate amount of time and ssh in again.  Re-run df -h and you should see the new space.

# Commentary

As a computer scientist I entirely understand why you have to reboot.  But as a consumer of cloud services it feels to me like we should be past this at this stage of the using cloud services.  All of my AWS experience is generally using Ubuntu and if Amazon was to tell me that this wasn't necessary on the AWS Linux, well, that alone would be enough to make me switch distros.

# Links

Here are two useful links that dig into this:

* [EC2 Console Notes](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-volume.html?icmpid=docs_ec2_console)
* [Linux Recognizing the Resized Space](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-volume.html#recognize-expanded-volume-linux)

</description>
        <pubDate>Wed, 09 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/08/09/expanding-an-aws-instance-volume.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/08/09/expanding-an-aws-instance-volume.html</guid>
        
        <category>aws</category>
        
        <category>docker</category>
        
        <category>aufs</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Utter and Complete Heroku Fail</title>
        <description>So this morning I wanted to get a self hosted version of the [Errbit](https://github.com/errbit/errbit) project running.  This is an equivalent to BugSnag, Airbrake, HoneyBadger -- a tool for tracking errors in live apps.  The inspiration for this was [Jeff Atwood's Developer on Fire interview](http://developeronfire.com/podcast/episode-258-jeff-atwood-sharing-the-house) where he advised three things: 

    Jeff's top 3 tips for delivering more value:

    1. Measure performance all the time - Know how long every unit of work in your app is taking
    2. Have user friendly error handling for both the end user and the developer built into your application
    2. Have a place where people can go to discuss your software

I'm close to releasing something and #2 has been on my list for sometime so I was pretty damn motivated to finally address this. And I've wanted to try Errbit for literally years and years so I turned to their [deployment](https://github.com/errbit/errbit/blob/master/docs/deployment/heroku.md) page where they, quote clearly, offered Heroku as an option and recommended it as the easiest way to start.  Awesome -- right?  Unfortunately I found nothing but complete and total fail.  Here's what happened:

1.  I created an account.  This is normal and worked well.
2.  I had to verify my password via email.  Again normal and worked well.
3.  I returned to the [automatic app creation](https://heroku.com/deploy?template=https://github.com/errbit/errbit/tree/master) where it now told me I had to enter a credit card.  I did.
4.  Installation failed and wouldn't tell me why &quot;App creation failed.  Please ensure you have valid values in the above form field&quot; -- they were all there.
5.  Installation failed and it told me to that I needed to enter a credit card -- which I did.  But I can do that dance again.
6.  I then get &quot;the account foo@bar.com is not permitted to install the sendgrid add-on at this time.  If you believe this is an error please contact support and reference ID 56fda43c-8abb-4378-a320-079bec73f142&quot; (note I did give heroku my correct email).
7.  I attempted to put in my own SMTP server but I'm told the same thing about sendgrid even though I put in a [sparkpost](http://www.sparkpost.com) domain.
8.  There is no link to support in this context.  Shouldn't this have been automatically linked in place?
9.  I find the support button at the bottom of the page (disclaimer - it wasn't all that far away) which takes me to help.heroku.com.
10.  On help.heroku.com I can find no way to actually create a ticket to enter the ID they gave me.  See Screenshot 1 below.
11. There is a [Ticket history](https://help.heroku.com/tickets) button which also doesn't give me a way to create a ticket.  See Screenshot 2 below.
12. At this point I've given up in disgust and I'll figure out how to do this on Docker and my own AWS instance.  

# Summary

This is a failure on so many damn levels that it is absurd:

1.  Having to enter my credit card twice.
2.  Not allowing an authenticated, validated, paying user to use a baseline feature.
3.  Not allowing a way around the failure (use of another smtp resource).
4.  Not making support easy.
5.  Not even allowing me to create a ticket AS THEY TOLD ME TO.

The utter and complete pity of this is that Heroku allowing me to easily run a github repo would have been beautiful.  Here is the beautiful simplicity of the Errbit deploy to Heroku url:

  https://heroku.com/deploy?template=https://github.com/errbit/errbit/tree/master
  
That is fantastic -- I do not think it could be any simpler than that.  And, if it had worked, I can easily see myself doing this fairly often.  Now?  I suspect I'll be deleting my Heroku account shortly and walking away forever.  And that's a crying shame.

# Screenshot 1

![herokufail1.png](/blog/assets/herokufail1.png)

So where would I even create a ticket?

# Screenshot 2

![herokufail2.png](/blog/assets/herokufail2.png)

So where would I even create a ticket?
</description>
        <pubDate>Tue, 08 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/fail/2017/08/08/utter-and-complete-heroku-fail.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/fail/2017/08/08/utter-and-complete-heroku-fail.html</guid>
        
        <category>heroku</category>
        
        <category>fail</category>
        
        <category>rails</category>
        
        
        <category>fail</category>
        
      </item>
    
      <item>
        <title>Adding AutoSave on Focus Lost to TextMate 2</title>
        <description>I've looked this up and set this on every single mac I've used since I switched to [TextMate 2](https://macromates.com/) so I guess it is time to finally write it down for myself:

1.  Open a command prompt.
2.  Edit the file ~/Library/Application Support/TextMate/Global.tmProperties
3.  Add the line saveOnBlur = true in the top region before the square bracket sections.
4.  Save and Exit
5.  Relaunch and it should be there.

</description>
        <pubDate>Mon, 07 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/textmate/2017/08/07/adding-autosave-on-focus-lost-to-textmate-2.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/textmate/2017/08/07/adding-autosave-on-focus-lost-to-textmate-2.html</guid>
        
        <category>textmate</category>
        
        
        <category>textmate</category>
        
      </item>
    
      <item>
        <title>A Bash Function for the 2 AM Blind Deploy</title>
        <description>I'm doing a lot more bash scripting these days -- ever since I opted to write my new deployer, dockerano, in Bash.  Tackling something significant in Bash has absolutely forced me, 20 years plus after I started using it, to finally start to understand it.  By no means do I feel that I have a good handle on Bash -- I'm still heavily reliant on [ShellCheck](http://fuzzyblog.io/blog/bash/2017/07/17/improving-your-bash-scripting-with-shellcheck.html) but I can actually do things with Bash that I feel somewhat confident about.

Here's a bash function that I extended this morning to add OSX specific audio output after a deploy.  The problem at hand is what I call the &quot;2 AM sleep blind deploy&quot;.  We all know what this is: 

* You get a downtime alert on a service
* You blearily struggle out of bed at 2 am and fix the code
* You deploy it and you want to get back to bed as soon as possible
* Something goes wrong with the deploy and you get alerted again just **after** you get back in bed
* Lather, rinse, repeat until it is either fixed or morning comes

I'm getting close to release on my much obliquely referenced new product and so deploy issues are now first and foremost in my mind. My task for this morning was to add audio output to my deployer **when** there is a failure.  Here's what I did:

1.  My first task was to recognize that this only ever had to run on OSX as I develop and deploy from OSX.
2.  There were two options - afplay and say.  The **afplay** utility plays any media file from the command line and that's good but a loud noise alone won't tell me *what* is down just that *something* is down.  And while this works well -- once upon a time, I used to trigger Billy Idol's White Wedding whenever something was down -- this time I wanted something a bit more nuanced and information rich.  I could also use the osx command **say** to speak anything including the url which is failing.  Here is a [Stack Overflow](https://superuser.com/questions/298201/how-can-i-play-a-sound-from-the-command-line-in-mac-os-x) on the options.
3.  An sample of this is &quot;say http://foo.com/ is down&quot; which will be played thru the speaker.

My new tech stack is 7 different services each of which provides a simple health check api but since the obviously differ for each service, I needed a way to store this on a per service basis which I opted to do with a .dockerano_post_deploy_check_urls file in the root directory of each project.  This file can store as many urls as are needed and it will be looped over by the bash script.  Given that most of the site needs to work in both http and https and with www and non www variants, I didn't want a single check url.  If you allow for both https and http and non www and www variants, that amounts to 4 urls:

* http://foo.com/health
* http://www.foo.com/health
* https://foo.com/health
* https://www.foo.com/health

And, yes, this may be overkill but it is all too easy for almost anything to break so I would argue that the right thing to do is be aggressive in your testing.

Here's the bash function I wrote for this with the addition of say:

    function verify_site_is_up_after_post_deploy() {
      sleep 5
      IFS=$'\n' read -d '' -r -a site_urls &lt; .dockerano_post_deploy_check_urls  
  
      for site_url in &quot;${site_urls[@]}&quot;; do
        echo &quot;  Processing url: $site_url&quot;
        if curl --output /dev/null --silent --head --fail &quot;$site_url&quot;; then
          status_message_good &quot;$url exists; deploy worked&quot;
        else
          status_message_bad &quot;$url DOES NOT EXIST; DEPLOY FAILURE!!!!&quot;
          say &quot;$url is DOWN!&quot;
        fi
    
      done
    }

I adapted the code above based on this  [source](https://unix.stackexchange.com/questions/148985/how-to-get-a-response-from-any-url).</description>
        <pubDate>Sun, 06 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/bash/2017/08/06/a-bash-function-for-the-2-am-blind-deploy.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/bash/2017/08/06/a-bash-function-for-the-2-am-blind-deploy.html</guid>
        
        <category>devops</category>
        
        <category>deploy</category>
        
        <category>bash</category>
        
        
        <category>bash</category>
        
      </item>
    
      <item>
        <title>Monitoring Free Disc Space On AWS Instances with Monit, Ansible and SparkPost</title>
        <description>There are lots and lots of things that can go wrong with an AWS instance but I have always found that the single most common problem is nothing more than running out of disc space -- log files fill up, too many things get deployed, etc.  And with low end AWS instances being only 8 gigs of free disc space, well, this is bound to happen.

The expensive solution is simple -- buy more disc space -- but I'm **cheap** and that means that the real solution is **monitoring** and **alerting**.  Here's what this means: 

* You need a way to configure monitoring, **automatically**, on **not** just one server but **all** servers you have.  We'll use [Ansible](http://www.ansible.com/) for this.
* You need a monitoring daemon, a piece of software which looks for error conditions and sends email when they occur.  We'll use [monit](https://github.com/arnaudsj/monit) for this.
* You need an email server to send the messages through.  We'll use [SparkPost](http://www.sparkpost.com/) for this.

# Step 1: Installation of Monit with Ansible

Ansible, which I've [written a lot about](http://fuzzyblog.io/blog/category.html#ansible), is a configuration tool for servers, your laptop, etc -- essentially any device that you want to configure can be managed by Ansible.

Here is a basic Ansible role which installs monit along with a configuration file:

    ---
    - name: install monit
      apt: pkg=monit state=present
  
    - name: copy config file
      template: src=roles/install_monit/files/monit.rc dest=/etc/monit/monitrc
  
    - name: start monit
      service: name=monit state=started
    
The lines above would go in a file roles/install_monit/tasks/main.yml.  You could then call that from a playbook with nothing more than:

    - { role: install_monit, tags: monit}
    
As long as all your AWS instances are listed in an inventory file then this will install monit on all boxes, copy up the configuration file and then restart the service.

# Step 2: Monit Configuration

We do need a simple Monit configuration file to tell it about our email server, credentials and the monitoring rules.  Monit is very powerful and can monitor for all kinds of issues but I'm only going to configure free disc space checking.  Where you see things in UPPERCASE then you need to configure them for your settings.

This configuration file will be installed into /etc/monit/monitrc by your Ansible playbook.

    set daemon 120 with start delay 60

      set mailserver smtp.sparkpostmail.com  port 587 username &quot;SMTP_Injection&quot; password &quot;PASSWORD&quot; using tlsv1 with timeout 30 seconds

      set alert YOURADDRESS@YOURSERVER.COM
  
    mail-format {
        From: noreply@YOURSERVER.COM
        Subject: MONIT ERROR !!! [$HOST] $EVENT $SERVICE
        Message: $DESCRIPTION
                 $DATE
    }

    check filesystem &quot;root&quot; with path /dev/xvda1
      if space usage &gt; 90% for 8 cycles then alert
      
And that's it.  Here is the english language explanation of the Monit configuration:

* Set the daemon to monitor every 2 minutes (120 seconds) with a delay of 60 seconds for checking on startup
* Set the mail server to be sparkpost on 587 with the SMTP_Injection username and a password
* Set the alerting email address
* Set the format for the alerting email including subject and description
* check the filesystem device and then alert if the space usage is more than 90% for 8 cycles

# Step 3: Email Configuration

[SparkPost](http://www.sparkpost.com) is a provider of email sending services and they have an incredibly generous free plan with up to 10,000 emails per month for free.  And while I could have used SES or another email service, I've had wonderful success using SparkPost to power my Rails ActionMailer routines so I just reused the same email credentials for Monit and it worked like a charm.  Here's what you need to do:

1.  To get a SparkPost account, sign up [here](https://app.sparkpost.com/account/credentials).
2.  Go to your [dashboard](https://app.sparkpost.com/account/credentials) to get an API key
3.  You do need to goto [Sending Domains](https://app.sparkpost.com/account/sending-domains) and configure the domains that you are sending from and then you need to add the DKIM settings to your DNS provider to prove that you own the domain in question.

# Conclusion

If you put these three pieces together then you will have an Ansible playbook that you can deploy onto all your servers that monitors for free disc space and alerts you via email.  Down the road you could easily enhance the monit script with other things that need monitoring such as http services, CPU load and more.

# Useful References

* [Akabos Gist](https://gist.github.com/akabos/3897117)
* [Art Chang](http://blog.artchang.com/post/37424851210/monitor-memcached-with-monit-and-alert-with-g
)
* [Ansible](http://www.ansible.com/)
* [Monit](https://github.com/arnaudsj/monit)</description>
        <pubDate>Wed, 02 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/08/02/monitoring-free-disc-space-on-aws-instances-with-monit-ansible-and-sparkpost.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/08/02/monitoring-free-disc-space-on-aws-instances-with-monit-ansible-and-sparkpost.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>monit</category>
        
        <category>SparkPost</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Hacks for When Ansible Galaxy Isn't Working</title>
        <description>As of late I have manually bootstrapped a handful of machines into production using a text file with some command lines.  Yes this is a crap ass way to do it but one of the key things, Docker, is a damn pain in the ass to install.  Here's what I was using as the command lines:

    sudo apt-get install \
        apt-transport-https \
        ca-certificates \
        curl \
        software-properties-common
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    sudo add-apt-repository \
       &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
       $(lsb_release -cs) \
       stable&quot;
    sudo apt-get update
    sudo apt-get install docker-ce
    sudo apt install docker-compose
    sudo groupadd docker
    sudo usermod -aG docker $USER
    sudo systemctl enable docker
    
Even for me that's damn ugly -- so it is clearly time for [Ansible](http://www.ansible.com/).  Ansible is a machine provisioning tool that lets you reduce commands like the above to something short, sweet and idempotent.  For this particular thing I wanted to use [Nick Janetakis' Ansible Galaxy role](https://nickjanetakis.com/blog/automate-installing-docker-and-docker-compose-with-ansible) to make installing this trivial.  Specifically I wanted what his blog post promises:

    - { role: nickjj.docker, tags docker}
    
which is all you need to setup Docker.  And that's actually what I got but I hit a few snags hence the workaround.

The problem that I hit was that his ansible galaxy role has some kind of version control conflict so that the version that gets installed when you type:

    ansible-galaxy install nickjj.docker
    
is actually the version from November 2016, not the current 17.xx version.  What I needed was the master version of the Github repo not the older tagged version.  A bit of googling and some interactions with Nick told me just clone it directly into my own project as a work around.  Here's how I did that:

    # Change into the right directory
    cd ~/me/fuzzygroup/hyde/seira_watch/ansible/roles
    
    git clone git@github.com:nickjj/ansible-docker.git
    
This created an ansible-docker folder in my roles directory and all that I needed to make it work was to change the nickjj.docker reference to:

    - { role: ansible-docker, tags docker}
    
Nick also helpfully pointed out that I could have also cloned to tmp and then renamed to nickjj-docker before copying it over and that would have worked just fine tool.  Thanks Nick!

Note 1: Another workaround supposedly would be to make a requirements.yml file and specify the branch there but I'm less certain on how to do that.

Note 2: I talked about this same type of problem [once before](http://fuzzyblog.io/blog/ansible/2016/10/09/ansible-quickie-fixing-a-poorly-designed-galaxy-role.html).</description>
        <pubDate>Tue, 01 Aug 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2017/08/01/hacks-for-when-ansible-galaxy-isn-t-working.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2017/08/01/hacks-for-when-ansible-galaxy-isn-t-working.html</guid>
        
        <category>ansible</category>
        
        <category>ansible-galaxy</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Configuring Your Mac with Ansible Take 2</title>
        <description>The last new Mac I setup was back in [2016, November 20](http://fuzzyblog.io/blog/osx/2016/11/20/ansible-for-configuring-your-mac-so-much-better.html) and to do that I used ansible which was a wonderful experience.  Rather than groveling for software to install, shell hacks, etc, my ansible routine handled all of those crappy tasks and it was brilliant.  I can't claim any originality here -- lots of people other than me have done this.  In specific I drew inspiration from [Jeff Geerling's](https://twitter.com/geerlingguy) ansible Macbook Pro repository.  He and I did it differently (his ansible skills are better than mine) but the end result was much the same -- a functional machine with minimal manual work.

I recently wrote about [my lack of system stability](http://fuzzyblog.io/blog/osx/2017/07/19/why-i-haven-t-switched-away-from-osx.html) and figured that it was time to address it.  My thesis for some time has been that most of my errors are somehow related to lack of RAM so moving to a bigger box made sense.  Over the weekend I converted my Linux Intel NUC over to an OSX Hackintosh and used an updated version of the same playbook to handle configuration.  This means that I'm now working on a 32 gigs of RAM development box.  It is unclear if this will work out long term or not but it is an interesting experiment.  Of course since its a Hackintosh there are certain bits of weirdness like the fact that audio doesn't work at all at present.  Ah well...

# What to Do When You Just Installed OSX

Here are the steps to follow for a brand new Mac when you don't have any tooling installed yet:

1.  Install the command line tools: xcode-select --install
2.  Install pip: easy_install --user pip
3.  Get pip into the path with: 
    
    $ printf 'if [ -f ~/.bashrc ]; then\n  source ~/.bashrc\nfi\n' &gt;&gt; $HOME/.profile
    $ printf 'export PATH=$PATH:$HOME/Library/Python/2.7/bin\n' &gt;&gt; $HOME/.bashrc
    $ source $HOME/.profile
    
4.  Install ansible: pip install --user --upgrade ansible
5.  Create the global dir: sudo mkdir /etc/ansible
6.  Copy in a configuration file: sudo curl -L https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg -o /etc/ansible/ansible.cfg
7.  Verify it works with: ansible localhost -m ping

A good reference is the Binary Nature link below.

# Useful Links

* [Jeff Geerling's Github Mac Repository](https://github.com/geerlingguy/mac-dev-playbook)
* [Vandenbrad.org on how to configure the OSX Dock]https://blog.vandenbrand.org/2016/01/04/how-to-automate-your-mac-os-x-setup-with-ansible/
* [My Github Repo for Macbook Configuration](https://github.com/fuzzygroup/ansible-macbook-pro)
* [TonyMacx86](https://www.tonymacx86.com/) - The definitive OSX Hackintosh Resource
* [Ansible](https://www.ansible.com/)
* [Binary Nature](http://binarynature.blogspot.com/2016/01/install-ansible-on-os-x-el-capitan_30.html)</description>
        <pubDate>Mon, 31 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/mac/2017/07/31/configuring-your-mac-with-ansible-take-2.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mac/2017/07/31/configuring-your-mac-with-ansible-take-2.html</guid>
        
        <category>mac</category>
        
        <category>ansible</category>
        
        <category>hackintosh</category>
        
        
        <category>mac</category>
        
      </item>
    
      <item>
        <title>Rails and Address Already In Use - Bind Error</title>
        <description>Lately I've seen this error come up quite a few times:

  /Users/sjohnson/.rvm/gems/ruby-2.3.1@seira_watch_web_app_api/gems/puma-3.9.1/lib/puma/binder.rb:269:in `initialize': Address already in use - bind(2) for &quot;::1&quot; port 3220 (Errno::EADDRINUSE)
  
I'm running a fairly complex stack that mirrors an 8 micro service application where things are expected to be on certain points in development so having port 3220 not working means that one of my APIs isn't available -- and that's **unacceptable**.  So I dug into it a bit and with the help of [this StackOverflow](https://stackoverflow.com/questions/10261477/tcpserver-error-address-already-in-use-bind2), I found the answer was to locate the offending process and then kill it dead:

&gt; lsof -wni tcp:3220

That gives the process id or PID and then you can just kill it:

&gt; kill -9 12345 (or whatever)
</description>
        <pubDate>Sat, 29 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/07/29/rails-and-address-already-in-use-bind-error.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/07/29/rails-and-address-already-in-use-bind-error.html</guid>
        
        <category>rails</category>
        
        <category>sysadmin</category>
        
        <category>bind</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Why I Haven't Switched Away from OSX</title>
        <description>I'm writing this at 4:12 am sitting next to a machine with 11:50 hours of uptime.  The machine I'm using is an old OSX Macbook Air running OSX El Capitan 10.11.6 and the machine with less than 12 hours of uptime is running Sierra 10.12.5 and is a new (6 months old) Macbook Pro 13, the last one of the old generation before Apple foolishly messed around with the keyboard travel and layout on the new generation.  Yesterday my wife was driving and I was in the car drafting blog entries for a new, very focused blog I want to launch.  We were on flat dead empty and she stopped at a gas station so I got out to pump the fuel.  I came back to that worst of all experiences -- my laptop had rebooted spontaneously and was sitting at the login prompt.  *Groan*.  Even worse there was no system report this time so I don't even know what pathetic excuse for a piece of software screwed up (again).  So I'm just left wondering.  But since this happens so frequently, I found a [crash report from about a week ago](https://gist.github.com/fuzzygroup/a4589f50665fd62f9ded74d54cd77e74) when my box had an uptime of 94000 seconds (yes that's just one day).

My old friend [Gene Callahan](https://gene-callahan.blogspot.com/), the man who two decades ago taught me Awk and pattern matching, skills that have subsequently defined much of my technical career, asked me recently why don't I switch to something stable and answering that is a challenge but I think I'm finally ready to address that.  Here's the tldr with a deep discussion on each below.

* Lack of Faith in the Alternatives
* Textmate, Textmate, Textmate
* iTerm2
* My Phone
* My Pictures / My Data
* Time Machine
* Apple Hardware is Simply Beautiful

Gene was asking me this, I suspect, because I wrote [a series of Postmac](http://fuzzyblog.io/blog/tag.html#postmac) blog posts at the end of 2016 into 2017 and then I simply stopped.  I was [very serious](http://fuzzyblog.io/blog/postmac/2016/11/05/life-in-a-postmac-world.html) about this initially and then I went dark on the topic and this digs into that.

# Disclaimer: I'm a Power User

It should be noted that I'm a power user and a software developer.  I mostly use a terminal and an editor.  My recent list of &quot;5 Apps I use Everyday&quot; on [Hacker News](iTerm, TextMate, Enpass, Ruby, Git) was iTerm, TextMate, Enpass, Ruby, Git -- just one of those was a GUI app (a password manager).  What I develop are mostly Rails apps with a smattering of straight ruby and, lately, some JavaScript / Node for a [course I'm teaching / mentoring](http://fuzzyblog.io/blog/how_to_be_a_developer/2017/07/02/how-to-be-a-developer-001.html).

Now the way I develop Rails apps is optimized for developer productivity so I start every project by opening a succession of terminal windows:

* Web Server Tail
* Database window
* Rails Console
* Command Line
* Remote Command Line

By keeping all the resources for whatever I need to do always active, I minimize cognitive overhead due to switching time / &quot;ok do I want to drop out of console to check the log file; is that worth it?&quot;.  

I have the definition of these set in a [Tmuxinator](https://github.com/tmuxinator/tmuxinator) profile so when I want to work on project xyz, I just tell the command line &quot;tmuxinator seira_watch_web_app&quot; and all the terminals for that project spring into existence (and it is wonderful to not have to think about 'ok what port do I need the web server on').  Right now I have 97 profiles setup for Tmuxinator.

You're probably thinking well -- 5 terminal windows, how is that a power user?  *Chuckle*, you should know play the opening bars of *[welcome to the jungle](https://www.youtube.com/watch?v=o1tj2zJ2Wvg)* in your head.  I've made the technical focus for 2017 learning how to do Rails development in a distributed, **non monolithic** fashion so that means your traditional Rails monolith has now been decomposed into N separate Rails projects with formal APIs between them and that you generally need all parts live concurrently.  In my case that generally amounts to 8 separate projects, each with at least 5 terminal windows so my 5 terminal windows is now 5 * 8 or 40 plus terminal windows just to work on one side project -- and that doesn't include consulting work, side, side projects, etc.  Generally speaking I run about 100 to 200 separate bash sessions.  The reason why I know about things like [pkill](http://fuzzyblog.io/blog/unix/2016/11/23/pkill-rocks.html) is from trying to tease out just one terminal session from the sea of them that I normally am swamped by.

Now I'm sure that 100 terminal sessions sounds like absolute madness but it actually isn't all bad.  The terminal ITerm2 is actually pretty damn good and I manage it through a combination of tooling ([Tmux](https://github.com/tmux/tmux/wiki) / Tmuxinator) and convention (terminal 2 is my core software development window; terminal 3 is blogging and so on).

So if you add 100 plus terminal windows to a browser or three each running say 50 plus tabs, I have very, very serious memory needs.  Apple's persistent inability / lack of desire to ship a laptop with more than 16 gigs of RAM is something I find absolutely maddening.

But for all of my bitching about stability and investigating alternatives, 8 months since I first wrote about &quot;postmac&quot;, I'm still here using OSX every single day so what went wrong?

# Lack of Faith / Everything Crashes

I'm now 49 years old and I have an appalling lack of faith in software quality -- or as I like to describe it &quot;everything crashes&quot;.  This past November I bought an Intel NUC for the purposes of just running Ubuntu and experimenting with desktop linux to see if it was an alternative.  I went with Ubuntu 16 and used Ubuntu Mate as the closest linux experience to OSX that I could find (and yes I flirted heavily with [Elementary](http://www.hongkiat.com/blog/elementary-os-luna/) but it wasn't stable).  And you know what I found out, while Ubuntu didn't crash as hard as OSX, I still lost the networking stack about every week.  And a machine without networking, well in 2016 / 2017, that's not much of a computer.  And the only way I ever found to get this fixed was a full system restart.  Now I'm sure that if I had dug into it enough, I could have found a way to restart the network stack but I never found it.  Oh and this was an Intel NUC with twice the ram of my OSX that I was using under far less demanding circumstances.

Honestly the experience of trying to switch off OSX onto Ubuntu and finding that Ubuntu wasn't stable, at least when used in a GUI context, was absolutely disheartening, so much so that that box sites on my desk and every time my eye falls on it, I get more than a little sad.  I run ubuntu linux boxes 24x7x365 with well over a *full year* of uptime so the network stack on my personal Ubuntu box failing after less than a week (and failing consistently) puzzles me.  My suspicion is that this somehow related to a full GUI running on top of Ubuntu itself.

So, honestly, everything crashes and if you're going to run a full computing stack, with a GUI, browser and everything else, I no longer think that there is such a thing as stability -- at least not if you're a power user like myself and really pushing the limits of the machine.  And yes that's a crap ass attitude but it is what a lifetime of being a power user has reduced me to; sigh.

# If Everything Crashes then Resume Quality is Key

So if you assume that everything crashes then the quality of resume is key.  And this is something where I found a dramatic difference between Ubuntu and OSX.  Sure Ubuntu didn't hard crash but if I have to restart the machine to get networking running again, well, it might as well have.  While I remain pissed that my machine crashes regularly (I'm averaging about once per week now), I will state that Apple / the application vendors have finally managed to make this suck less.  Specifically:

* iTerm2 remembers every window and tab's position so all I have to do is relaunch Tmuxinator with a profile; since tab names are persistent (I'm glaring at you Apple Terminal), the name tells me what profile to launch
* My editor, covered next, restores bloody everything
* Apple generally relaunches every application I had going

So while its not perfect, resume quality is far, far better than I have ever had it before.  Even the core browsers I rely on, Chrome, Firefox and Safari actually get window / tab restoration mostly correct.

# Textmate, Textmate, Textmate

The very first editor I used once I switched fulltime to OSX from Windows / Linux (both on ThinkPad hardware) back in 2006 was [TextMate](https://macromates.com/) and I'm still using it.  I've written hundreds of thousands of lines of code in it and while the world has mostly moved on, I still find it to be the single best editor I've ever used.  Sure its old and a wee bit clunky but it: 

* Has restore implemented perfectly (even &quot;unsaved&quot; files are restored after a crash allowing you to treat it as a virtual scratch pad; once you drop things into a new window they never go away unless you choose kill them)
* Has extensions specific to Ruby / Rails
* Has extensions for almost anything
* Fast enough
* Great syntax highlighting
* Macro facility

I used to laugh at engineers who were so closely tied to a particular editor and think &quot;I'll never be that&quot;; karma is a bitch. But, seriously, an editor is what I spend maybe 90% + of my time in so having one that I love really matters and that is actually a switching cost.  

When I attempted to using Ubuntu on the NUC, I used [Sublime](https://www.sublimetext.com/), [Atom](http://www.atom.io) and [Visual Studio Code](https://code.visualstudio.com/).  Sublime never felt right and I just can't get past the performance / memory bloat of anything written using Electron (as beautiful as VS Code is). I should note that Visual Studio Code is actually awesome and Microsoft has done amazing work on it.

But I still love my TextMate.

# iTerm2

While iTerm2 takes some crap for not always fixing bugs, being sometimes [slow](https://danluu.com/term-latency/) and a few other failings, I personally find it awesome.  Once again the ability to resume after a crash looms large.  Oddly if I choose to install updates, it can't resume anything so I rarely choose to update it.  Similarly its resumption doesn't work if you exit so if I have to shut it down, well, I use Activity Monitor to kill it and then it brings back all my terminals perfectly.  I wish I could accept the updates but, honestly, decent resume is so wonderful that I can live without them.

iTerm2 has Tmux integration and decent multi pane support that could theoretically save me from having to be so reliant on Tmuxinator but I've explored it being really happy with Tmux / Tmuxinator.

# My iPhone / Phone Integration

This one is an absolute killer.  I'm an iPhone user and the integration that Apple has put together is fairly compelling.  Specifically if my wife or kids text me, I can just reply using my laptop and not have to struggle with my fat fingers on a small phone screen.  That one feature is huge for me and after WhatsApp has completely stopped working for me since early 2017, I'm back to using SMS for texting with my family so this is huge.  I used to get around this with a dedicated WhatsApp app on OSX (which I think exist on other platforms).  Given that mobile isn't going to any less important, integration is key and I find it hard to believe that I could ever have a better experience with an iPhone on non Apple hardware.

# My Pictures / My Data 

Moving between systems is always a lossy operation -- you're going to lose something whether it is a much loved app or data that doesn't convert correctly or something else.  Given that I have a decade plus worth of files on OSX, I'm going to lose stuff when I move and just the prospect of that is daunting.

# One Device

I used to have a work laptop for coding and then a personal computer for &quot;my data&quot;.  Ever since I switched to OSX, I've had only one device to worry about and that's pretty damn wonderful.  Given that I use an iPhone, if I was to switch away from OSX for my daily coding, I'm likely still going to have OSX in life and that ruins the one device strategy.

# Time Machine

If you weren't a hardcore Mac person when Time Machine shipped you likely don't remember how rarely, if ever, most of us backed up.  I keep one machine constantly backed up with Time Machine and I actually trust it.  I've had only one back up issue with Time Machine since I started using it and that includes reformatting a machine that had every family photo on it ever and Time Machine brought back every drop of data.  Sure there are other backup solutions on other platforms but I **know** that Time Machine works and that level of trust is pretty important.

# Apple Hardware is Simply Beautiful and Lasts

If you're a PC guy then you may not understand the hardware quality of an Apple box.  While I have lots and lots of issues with Apple at times, I have to admit that their hardware is simply **beautiful** -- and it **lasts**.  My first 15&quot; Macbook pro, that old 2010 model, it still runs daily as the family Plex server hooked up to like 9 gigs of storage and runs pretty much like a champ.  My 2012 Macbook Pro?  Its still around and working perfectly.  So while the software has issues, the physical hardware is beautiful and durable.  If you're going to use something on a daily basis then it being beautiful isn't a bad thing -- I spent a lot of years using Dell and Toshiba hardware and the build quality just sucked.  I used to know that when I heard rattling in my Toshiba's it meant that a screw had come out (again) and that the machine would die shortly thereafter.  Overall the hardware side of my Macs haven't let me down.

# Closing Thoughts on Software Quality

I should note that software quality problems are rampant in the industry.  Just as an example, if I prematurely abort a test run on RSpec with ctrl+c, well, I lose the cursor in the terminal -- and there is nothing I can do to get it back, not clear, not ctrl+k, etc.  I just have to shut down that terminal session.  So it isn't just OSX or the GUI on Ubuntu, it is everywhere and it is a damn pity.</description>
        <pubDate>Wed, 19 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/07/19/why-i-haven-t-switched-away-from-osx.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/07/19/why-i-haven-t-switched-away-from-osx.html</guid>
        
        <category>osx</category>
        
        <category>linux</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Improving Your Bash Scripting with shellcheck</title>
        <description>I know to the outside world it often appears that a &quot;developer&quot; or &quot;software engineer&quot; knows everything when it comes to &quot;coding&quot; but the reality is that most of us are fairly specialized and we have both strengths and weaknesses.  My personal weaknesses include Bash scripting and current front end technologies (JavaScript, CSS).  Now, even when you have weaknesses, you can still improve them and I'm currently working hard on my Bash skills.  The project at hand is that I'm writing a custom deployer for a stack of software related to a new SAAS app I'm building.

Bash is one of those technologies that most of us know a little but that small amount of knowledge often prevents you from really learning -- you, instead, google about a bit and put some crap into a file with a .sh extension (or that you add an executable bit to and a shebang line) and you think &quot;I know bash&quot;.

I recently found a tool that dramatically improved my shell scripting and, well, is really what enabled me to write a deployer for Rails apps under Docker using bash.  The tool is called [shellcheck](https://github.com/koalaman/shellcheck) and it is a linting tool for bash scripts.

# Installation

Here's how to install it on OSX:

&gt; brew install shellcheck 

# Tutorial and Examples

ShellCheck is ridiculously easy to use: 

&gt; shellcheck filename &lt;ENTER&gt;
  
Obviously press ENTER not type it and this will analyze your bash script and give you feedback.  Here are some examples and the errors it found for me:

    ip-10-19-48-2% shellcheck deploy.sh

    In deploy.sh line 40:
    for host in &quot;${hosts[@]}&quot;; do
    ^-- SC1009: The mentioned parser error was in this for loop.

This was a problem in overall parsing of the shell script; [more details](https://github.com/koalaman/shellcheck/wiki/SC1009).

    In deploy.sh line 42:
      for file in &quot;${files[@]}&quot;; do
      ^-- SC1073: Couldn't parse this for loop.
                                 ^-- SC1061: Couldn't find 'done' for this 'do'.

Here I was missing the done terminator for the loop.

    In deploy.sh line 6:
      cmd_output=$(eval $1)
                        ^-- SC2086: Double quote to prevent globbing and word splitting.

Technically I should have been using double quotes around the variable to prevent any issues with globbing / splitting.

    In deploy.sh line 31:
    files=(&quot;.env&quot;, &quot;.env.production&quot;, &quot;dockerstats&quot;, &quot;docker-compose.production.yml&quot;)
          ^-- SC2054: Use spaces, not commas, to separate array elements.

I'm a Ruby guy so my natural array syntax has commas between elements.  Here ShellCheck is pointing out that I just need to have spaces between them.

# More Details / Some Good Bash Stuff

* ShellCheck home page: [https://github.com/koalaman/shellcheck](https://github.com/koalaman/shellcheck)
* Hacker News on getting better at bash: [https://news.ycombinator.com/item?id=14634964](https://news.ycombinator.com/item?id=14634964)
* [Bash Guide](http://mywiki.wooledge.org/BashGuide)
* [Bash FAQ](http://mywiki.wooledge.org/BashFAQ)
</description>
        <pubDate>Mon, 17 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/bash/2017/07/17/improving-your-bash-scripting-with-shellcheck.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/bash/2017/07/17/improving-your-bash-scripting-with-shellcheck.html</guid>
        
        <category>bash</category>
        
        <category>shellcheck</category>
        
        
        <category>bash</category>
        
      </item>
    
      <item>
        <title>JavaScript and CSS Magic for the Rails Front End Challenged using Zoom.js for Image Zooming</title>
        <description>I am, as I have often written, front end challenged.  I grew up on the Internet writing HTML long before JavaScript and CSS and there's that old aphorism about old dogs, new tricks... Happily, the people in my nerd life, offer to teach me a trick or two from time to time and, well, [Nick](https://www.nickjanetakis.com/blog/) came thru for me this morning so I thought I'd write it all down.

The problem at hand was a landing page for a SAAS app with screenshots where it was hard to read the text in the screenshots (think a dense, information packed dashboard).  Nick said to me &quot;those should be zoomable; just [zoom.js](http://github.com/fat/zoom.js)&quot;.  I made my usual grumbles about being front end challenged but then realized that I was basically being a **winer** -- and I hate winers, so to work!

Zoom.js is a combination of CSS and JavaScript that makes an image clickable as a zoomable overlay onto the current page.  It is basically the same as [Medium.com](https://medium.design/image-zoom-on-medium-24d146fc0c20)'s image zooming which is a brilliant implementation (thank you Brad Birdsall and the whole Medium team).  And thanks to Github user [fat](http://www.github.com/fat/) who actually did the hard work on this.

It should be noted that Zoom.js is jQuery based which means that it won't work off the bat in Rails 5.1 without the gem 'jquery-rails' line being present in Gemfile.  

**Disclaimer:** I recognize that some of this isn't perfect.  Specifically this library is clearly packaged to be installed by gulp in an automated fashion but I've never had a lot of luck with gulp hence the use below of wget / curl.

# How to Add zoom.js to a Rails Application

Please note that jQuery has been a core Rails dependency for years and years so steps 1 and 2 likely aren't needed for you.

1. Add jquery to your Rails app if it isn't present.  Add jquery-rails to Gemfile:
&gt; gem 'jquery-rails'
2. Require jquery in your application.js:
&gt; //= require juery
3. Add the data-action=&gt;&quot;zoom&quot; to your image_tag calls for any images you want to be zoomable like this:
&gt; &lt;%=image_tag(&quot;screenshot_dashboard.png&quot;, :class =&gt; &quot;img-responsive&quot;, :alt =&gt; &quot;a dashboard view&quot;, &quot;data-action&quot; =&gt; &quot;zoom&quot;)%&gt;
4. In your terminal, change into the app/assets/stylesheets directory:
&gt; cd app/assets/stylesheets
5. Do a wget or curl on the url for the css.  I got this url by navigating the source tree down to zoom.css and then viewing the file and clicking the raw view:
&gt; wget https://raw.githubusercontent.com/fat/zoom.js/master/css/zoom.css
6. Do a wget or curl on the url for the js.  I got this url by navigating the source tree down to zoom.js and then viewing the file and clicking the raw view:
&gt; https://raw.githubusercontent.com/fat/zoom.js/master/js/zoom.js
7. Add the css and js to your application layout, application.html.erb, with two lines like these (obviously you need to add angle braces around them; my blogging tool eats the text if I show them, alas):
    
&lt;p&gt;link href=&quot;assets/zoom.css&quot; rel=&quot;stylesheet&quot;&lt;/p&gt;
    
&lt;p&gt;script src=&quot;assets/zoom.js&quot;&lt;/p&gt;
  
</description>
        <pubDate>Sat, 08 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/07/08/javascript-and-css-magic-for-the-rails-front-end-challenged.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/07/08/javascript-and-css-magic-for-the-rails-front-end-challenged.html</guid>
        
        <category>rails</category>
        
        <category>javascript</category>
        
        <category>css</category>
        
        <category>images</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>How To Be A Developer 001 - Commit</title>
        <description># Background

I've taken on a new project recently.  I'm helping teach some local (Indianapolis) homeless youth basic software engineering using the web developer curriculum from [Free Code Camp](http://www.freecodecamp.com).  Initially I'm teaching two students and we're working out of the [Outreach Indiana](https://outreachindiana.org/) facility in downtown Indianapolis.  Outreach Indiana is a fantastic charity that truly does good work.

I thought I'd write a series of essays on How to Be a Developer that reflect some of the things I've learned over a career working in high tech.  While I am a fan of what Free Code Camp does, I thought some practical, hands on advice might add some value for my students.  I've structured these as a series of single topic narratives arranged numerically.

# Lesson 001 - Commit

The topic for my first lesson is that to be a developer you need to **commit**.  Now I don't mean a *git commit* (more on that in #19), what I mean is that the nature of being a developer is **committing to solving problems**.  I'm going to give you an example of a problem I solved just three days ago:

Broken:

&gt; docker-compose -f docker-compose.production.yml exec web bundle exec rake db:migrate

Working:

&gt; docker-compose -f docker-compose.production.yml exec -T web bundle exec rake db:migrate

What these commands do doesn't really matter but the only difference between these two commands is a small **-T** flag that only discovered after troubleshooting an obscure Ansible to Docker error:

    Traceback (most recent call last):\n  File \&quot;/usr/bin/docker-compose\&quot;, line 9, in 
    &lt;module&gt;\n    load_entry_point('docker-compose==1.8.0', 
    'console_scripts', 'docker-compose')()
    File \&quot;/usr/lib/python2.7/dist-packages/compose/cli/main.py\&quot;, 
    line 61, in main
    command()
    File \&quot;/usr/lib/python2.7/dist-packages/compose/cli/main.py\&quot;, line 113, 
    in perform_command handler(command, command_options)
    File \&quot;/usr/lib/python2.7/dist-packages/compose/cli/main.py\&quot;, line 441, in exec_command
    pty.start()
    File \&quot;/usr/lib/python2.7/dist-packages/dockerpty/pty.py\&quot;, line 338, in start
    io.set_blocking(pump, flag)
    File \&quot;/usr/lib/python2.7/dist-packages/dockerpty/io.py\&quot;, line 32, 
    in set_blocking
    old_flag = fcntl.fcntl(fd, fcntl.F_GETFL)\nValueError: file descriptor cannot be a negative integer (-1)
    
It took considerable sleuthing to figure this out and the error was identified in [a single Github issue](https://github.com/docker/compose/issues/3352) on the underlying Docker project.  And when I say that this problem was obscure, even Docker experts I know had no idea about the -T flag.  Here's part of the problem description:

    So basically, you need -i for docker exec when you are piping in data to the command.

    And you should not use -t for docker exec when you are piping data out from the command.

    And docker-compose exec only have -T, which means neither -i or -t. But you cannot represent -i without -t in docker-compose.

And that's what I mean by obscure...  As a developer you run into this kind of stuff all the time -- there's a reason why when you start with Free Code Camp, it emphasizes using Google before you ever get into a programming language.  A huge part of being a developer is now your skill at using Google and that's something that I'll cover in #11.    

To me the very essence of being a developer is being willing to commit to **problem solving** at this level.  It means: 

* accepting that you will continually be encountering new things that you don't know in depth
* trying hard to understand whatever the problem you encounter
* researching things in depth, sometimes over and over as the problems shift
* sometimes solving things by experimenting until you come up with a solution that works

As a developer, sometimes the problem you solve:

* is one of languages where you can't figure out how to do something using your language of choice
* is one of your tooling where the tool you're using either doesn't work or doesn't work as expected
* is one of an algorithm where the underlying approach to solving the problem is unclear
* is an obscure bug which only happens from in certain circumstances; in this case you need to figure out what those circumstances are and how to repeat them

And there are perhaps an infinite number of variants on these but the underlying nature of being a developer is really problem solving and in order to do that a developer has to commit himself or herself fully.  I'm going to close this with a song lyric that really reminds me of being a developer:

    I get knocked down, but I get up again
    You are never gonna keep me down
    I get knocked down, but I get up again
    You are never gonna keep me down
    I get knocked down, but I get up again
    You are never gonna keep me down
    I get knocked down, but I get up again
    You are never gonna keep me down
    
    [TubThumping by Chumbawamba](https://www.google.com/search?q=i+get+knocked+down+lyrics&amp;ie=utf-8&amp;oe=utf-8) | [Youtube](https://www.youtube.com/watch?v=2H5uWRjFsGc)
    
The problems you solve will continually knock you down and you **have** to get back up again.  Some of the best developers I have ever worked with actually weren't the smartest but they knew how to commit and how to just stick with problems like a terrier until it was solved.
</description>
        <pubDate>Sun, 02 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/how_to_be_a_developer/2017/07/02/how-to-be-a-developer-001.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/how_to_be_a_developer/2017/07/02/how-to-be-a-developer-001.html</guid>
        
        <category>how_to_be_a_developer</category>
        
        <category>software_development</category>
        
        
        <category>how_to_be_a_developer</category>
        
      </item>
    
      <item>
        <title>Adding Mosh Support to Your AWS Servers</title>
        <description>Mosh is an open source project that I've been tracking for the past several years as an SSH replacement and it appears that it is finally at a point where it is ready for me to put it into production.  In this tutorial I document the process of installing Mosh client side and server side along with the necessary security group configuration.  In case you're not aware of Mosh, [Mosh](https://mosh.org/) is a UDP based addition to SSH which improves the reliability of connecting to your servers, particularly on mobile.  One of the notable benefits of mosh over ssh is that mosh doesn't fill up buffers so a CTRL+C key sequence actually stops things immediately. 

# Step 1: Installing the Mosh Client Software

If you're using OSX and HomeBrew then you can install the mosh client software with a simple:

&gt; brew install mobile-shell

If you use a different platform then you can find installation instructions [here](https://mosh.org/#getting).

# Step 2: Installing the Mosh Server Software

On an Ubuntu box, Mosh is now much easier to install since the package is covered in the standard repositories:

&gt; sudo apt-get install mosh

# Step 3: Opening Your Security Group

The hardest part of all this is actually opening a *hole* in your AWS security group to allow the Mosh communications protocol to pass through.  Since ssh is an internet standard, a hole for ssh's port 22 is always open on an AWS security group.  What you have to do for mosh is:

* Open a UDP type
* Port Range 60000 - 61000
* Accessible to anywhere (or just your ip address; up to you)

Here's what your security group settings should look like:

![mosh_security_group.png](/blog/assets/mosh_security_group2.png)

Here's how to get to your AWS security group:

1.  Log into the AWS console.
2.  Select EC2 instances.
3.  Scroll down on the left hand sidebar until you see Security Groups.
4.  Click the Security Group links and then select the group you want to modify from the list at the top.  
5.  Select the Inbound tab and then click the Edit button. 
6.  Scroll to the bottom and then click Add Rule and define the rule per the screenshot above.

# Step 4: An Ansible Role for Provisioning Your Server

Here is a simple ansible role for adding mosh support to your server using Ansible as provisioning tool:

    - name: install mosh
      apt: pkg=mosh state=present

By adding this role into whatever you use for a bootstrapping task, you can add mosh support to all your boxes with one command.  And, if you used a single security group for all your AWS instances, that may be the only thing to you need to make all boxes accessible by mosh instead of ssh.

# Step 5: An Example of &quot;moshing in&quot; Using an AWS Pem File

I ssh in to my AWS boxes using a command like this:

&gt; ssh -i ~/.ssh/fuzzygroup2.pem ubuntu@xyz.com

Here is the Mosh equivalent:

&gt; mosh --ssh=&quot;ssh -i ~/.ssh/fuzzygroup2.pem&quot; ubuntu@xyz.com

My thanks (and an upvote) to [Server Fault](https://serverfault.com/questions/825173) for answering this part of it.
</description>
        <pubDate>Sun, 02 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/07/02/adding-mosh-support-to-your-aws-servers.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/07/02/adding-mosh-support-to-your-aws-servers.html</guid>
        
        <category>aws</category>
        
        <category>mosh</category>
        
        <category>devops</category>
        
        <category>osx</category>
        
        <category>ansible</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Beware Docker Swarm</title>
        <description>I have previously mentioned that I was using Docker Swarm and fairly happy with it.  I even went so far as to document the [deploy process](http://fuzzyblog.io/blog/devops/2017/06/20/no-ci-and-no-cd-deploying-docker-swarm-with-bash-and-ansible.html).  I now have to retract everything positive I said about Docker Swarm.  I have now reverted to just using Docker and Docker Compose and bare containers with an AWS ELB on top of it.  Here's why:

1.  I attempted to have multiple apps with Docker Swarm on the same instance.  Nothing seemed to work and I had continuous troubles including containers running out of memory and the instance itself becoming hugely unresponsive.  This led me to the conclusion that Docker Swarm wasn't really ready for production.
2.  On the instance where I had previously used Docker Swarm I disabled the swarm functionality and attempted to use the instance just for raw containers.  This utterly failed with no http services (puma) being able to serve content but no clear errors either (i.e. the request wasn't even getting to the http server).  Digging into it deeper showed that Docker Swarm had left all kinds of digital garbage in iptables.  So I flushed that with a sudo iptables --flush and did a fresh deploy of the containers.  This still failed.
3.  Finally I built a new instance from scratch and assigned that instance the same ip address as the instance referenced in #2 and did another deploy.  The result?  The container worked perfectly on the very first try.

My final conclusions are:

* Swarm isn't ready for production use.
* If you have to use Swarm then put only one &quot;stack&quot; on a physical instance.
* Don't try and use raw containers on a swarm box that aren't using swarm.
* If you want to stop using swarm then, well, destroy the instance; do not waste time trying to use the instance for another Docker related thing - something is fundamentally wrong at the box level after Docker Swarm is shut down.

In closing I'd like to point out that I am still using containers and I am finding a lot of luck with docker-compose.  This is not an anti docker blog post but, rather, an anti Docker Swarm blog post.
</description>
        <pubDate>Wed, 28 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/06/28/beware-docker-swarm.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/06/28/beware-docker-swarm.html</guid>
        
        <category>docker</category>
        
        <category>swarm</category>
        
        <category>devops</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Docker Tutorial Understanding Container Memory Usage</title>
        <description>I know that it might seem like in 2017, in our new container centric world, that understanding memory usage on a per container basis isn't all that necessary but my recent experiences with my side project are, sorely, disagreeing with that.

I'm developing a SAAS product where I've firmly and irrevocably broken the traditional Rails monolithic development model.  So rather than have one giant Rails app, I have multiple Rails apps -- the promotional website, the crawler, the admin tool and so on.  .  My current deployment model is Docker containers and each Rails app is represented by a container stack.  For cost reasons I want to use resources efficiently allowing multiple container stacks to exist on the same host.  This requires understanding our low level memory usage so here goes...

Note: All of this is being done on AWS under Ubuntu 16.04 using docker-compose 1.8

# Box Level Memory

Use free -m to find out the current memory status of your box:

&gt; free -m

                  total        used        free      shared  buff/cache   available
    Mem:           1998        1163          90          22         743         554
    Swap:             0           0           0

# Docker Container Memory

Use docker stats to get the low level container memory usage:

&gt; docker stats

    CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O           PIDS
    a5251659f7c9        0.23%               15.29 MiB / 1.952 GiB   0.76%               35.5 MB / 20.3 MB   1.35 MB / 40.5 MB   3
    59ce2ad1dc64        0.01%               257.8 MiB / 1.952 GiB   12.90%              25.6 MB / 33.9 MB   143 kB / 0 B        6
    b4aaa4c36791        64.55%              237 MiB / 1.952 GiB     11.86%              639 MB / 115 MB     3.39 MB / 4.1 kB    14
    a7c039f80931        0.00%               199.8 MiB / 1.952 GiB   10.00%              152 kB / 1.22 MB    1.2 MB / 0 B        23
    ee27a6e37fef        0.00%               131.4 MiB / 1.952 GiB   6.57%               1.22 MB / 2.33 MB   26.9 MB / 0 B       6
    07d5252abf5f        0.00%               196.6 MiB / 1.952 GiB   9.84%               127 kB / 689 kB     1.63 MB / 0 B       23
    4aa705c8cf4b        0.07%               6.234 MiB / 1.952 GiB   0.31%               9.11 kB / 1.94 kB   0 B / 0 B           3
    78f495fcf733        0.06%               6.922 MiB / 1.952 GiB   0.35%               145 MB / 91.1 MB    766 kB / 2.97 MB    3
    
If you wanted to reduce the amount of output to the bare essentials then try:

&gt; docker stats --format &quot;table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}&quot;

    CONTAINER           CPU %               MEM USAGE / LIMIT
    a5251659f7c9        0.10%               15.29 MiB / 1.952 GiB
    59ce2ad1dc64        0.01%               239.1 MiB / 1.952 GiB
    b4aaa4c36791        21.33%              478.5 MiB / 1.952 GiB
    a7c039f80931        0.01%               251.1 MiB / 1.952 GiB
    ee27a6e37fef        0.00%               109 MiB / 1.952 GiB
    07d5252abf5f        0.00%               250.9 MiB / 1.952 GiB
    4aa705c8cf4b        0.05%               6.234 MiB / 1.952 GiB
    78f495fcf733        0.05%               6.664 MiB / 1.952 GiB

Now the clear and obvious problem here is that a5251659f7c9 or 59ce2ad1dc64 mean **absolutely nothing** to us.
# But What is a5251659f7c9 or Understanding Docker Process IDs?

The a5251659f7c9 is a Docker process id and you need to grep the docker process list to find out.  Docker process ids change with every single container  deploy so you always have to look them up with the *docker ps* command.

    docker ps | grep a5251659f7c9
    a5251659f7c9        redis:3.2-alpine  &quot;docker-entrypoint...&quot;   27 minutes ago      Up 27 minutes       0.0.0.0:32771-&gt;6379/tcp   seiracrawler_redis_1
    
So the way to interpret all this is the docker container with process id a5251659f7c9 corresponds to the named container seiracrawler_redis_1 and is using 15.29 mb RAM and 0.23% CPU.

# Putting it All Together - dockerstats

If you're thinking that this two step process, well, **sucks**, yep.  I took a pass at cleaning this up into a single shell script but I found my bash skills, sadly weren't up to it.  Happily I did additional research and managed to put this together from some things I found on the web:

    #!/bin/bash
    docker stats --format &quot;table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}&quot; $(docker ps|grep -v &quot;NAMES&quot;|awk '{ print $NF }'|tr &quot;\n&quot; &quot; &quot;)

If you put this into a shell script named dockerstats and then make it executable with:

&gt; chmod +x dockerstats

then you'll have a single shell script that produces this:

    CONTAINER                                            CPU %               MEM USAGE / LIMIT
    seiracrawler_redis_1                                 0.12%               15.29 MiB / 1.952 GiB
    seiracrawler_rake_1                                  0.01%               239.1 MiB / 1.952 GiB
    seiracrawler_sidekiq_1                               11.99%              480.5 MiB / 1.952 GiB
    seirawatchsite_web.2.3x1bx3ji93hueuidv4dgfeab3       0.00%               251.1 MiB / 1.952 GiB
    seirawatchsite_sidekiq.1.8nxcvjx353725lsv70h72mzup   0.10%               109 MiB / 1.952 GiB
    seirawatchsite_web.1.rxbzptnfuopfauqwjf4ib4bjr       0.01%               250.9 MiB / 1.952 GiB
    seiraadmin_redis.1.ws2hc37dihkbh0cejk6z75140         0.06%               6.234 MiB / 1.952 GiB
    seirawatchsite_redis.1.siatex4zd6xezsevxfk4no98n     0.09%               6.664 MiB / 1.952 GiB

You'll seem some redundancy here because one my Docker installations on this particular box is using Docker Swarm (not recommended btw) and there are  replicas of some of the containers due to my swarm configuration.

Personally I think this functionality is important enough that I've added it to my deploy process on all my boxes running Docker and I'll recommend the same to all my clients.

If you just want the dockerstats shell script, here is a [gist with it](https://gist.github.com/fuzzygroup/e000a972a999b406a38557324e11d1c0).

# See Also

Extracting just a section of the Docker stats output came from the official Docker docs for [Docker Stats](https://docs.docker.com/engine/reference/commandline/stats/#formatting).  Feeding the output from Docker Stats to a docker ps grep statement came from a [github issue on moby](https://github.com/moby/moby/issues/10772).

</description>
        <pubDate>Sun, 25 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/06/25/docker-tutorial-understanding-container-memory-usage.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/06/25/docker-tutorial-understanding-container-memory-usage.html</guid>
        
        <category>docker</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Using Ansible on Ubuntu 16.04 When which python Fails</title>
        <description>[Ansible](https://www.ansible.com) is my favorite devops tool.  I've used [Chef](https://www.chef.io/chef/) extensively and despite my Ruby background, well, I hated it.  I've slutted around with [TerraForm](https://www.terraform.io); I've played with [Puppet](https://puppet.com) and I've put [Salt](https://saltstack.com) on my food and servers but time in and time out, I always come back to Ansible.  I even like Ansible enough that I wrote about it recently on [a Quora post](https://www.quora.com/What-do-I-need-to-study-before-studying-Ansible-Im-from-a-networking-background-and-we-like-to-do-automation-via-Ansible/answer/Scott-Johnson-16).

One of the beautiful things about Ansible is that it **does not** require a server side agent.  Ansible works via SSH purely and that means that as long as you can login to the server then you can use Ansible to provision it, update it, deploy stuff, etc.  

I just deployed my first ever Ubuntu 16.04 LTS box on AWS the other day and one of my surprises was that a which python **failed**.  Oy vey.  This means that you can't use Ansible without installing Python -- or so I thought.  Happily a bit of googling and I discovered the [ansible_python_interpreter](https://docs.ansible.com/ansible/python_3_support.html) option which lets you override the default of Python 2 and use Python 3.  This gets, oddly, embedded in the inventories file:

    [web]
    site1 ansible_ssh_host=foo.com

    [web:vars]
    ansible_python_interpreter=/usr/bin/python3

The way to interpret this is all boxes in the [web] group use the /usr/bin/python3 executable.
</description>
        <pubDate>Tue, 20 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2017/06/20/using-ansible-on-ubuntu-16-04-when-which-python-fails.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2017/06/20/using-ansible-on-ubuntu-16-04-when-which-python-fails.html</guid>
        
        <category>ansible</category>
        
        <category>devops</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>OSX Tip Using mdfind to search your hard disc for files by name</title>
        <description>As a long time Unix user, the lack of locate being automatically turned on in OSX has always, alway bothered me.  I have always wondered about this until yesterday when I couldn't find where my.cnf was on the disc and I turned to the Internet.  Somewhere I located this syntax:

&gt; mdfind -name my.cnf

When I run this I get this output:

    /private/etc/my.cnf
    /Users/sjohnson/appdatallc/ansible/my.cnf
    /Users/sjohnson/Dropbox/appdatallc/chef2/cookbooks/mysql/templates/default/5.0/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/cookbooks/mysql/templates/default/5.1/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/cookbooks/mysql/templates/default/5.5/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/cookbooks/mysql/templates/default/5.6/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/cookbooks/mysql/templates/default/deprecated/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/berks-cookbooks/mysql/templates/default/deprecated/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/cookbooks/mysql/templates/default/5.0/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/cookbooks/mysql/templates/default/5.1/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/cookbooks/mysql/templates/default/5.5/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/cookbooks/mysql/templates/default/5.6/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/cookbooks/mysql/templates/default/deprecated/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.0/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.1/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.5/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.6/my.cnf.erb
    /Users/sjohnson/appdatallc/chef2/berks-cookbooks/mysql/templates/default/deprecated/my.cnf.erb
    /Users/sjohnson/Dropbox/consulting/interana/sso_portal/script/ansible/roles/mariadb/templates/my.cnf
    /Users/sjohnson/Dropbox/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.0/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.1/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.5/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/chef2/berks-cookbooks/mysql/templates/default/5.6/my.cnf.erb
    /Users/sjohnson/Dropbox/appdatallc/ansible/my.cnf
    /Users/sjohnson/Dropbox/consulting/interana/master_before_move_back_to_detached/sso_portal/script/ansible/roles/mariadb/templates/my.cnf
    /Users/sjohnson/Dropbox/appdatallc/ansible-clean/roles/install-mariadb55/files/etc/mysql/my.cnf.default
    /Users/sjohnson/Dropbox/fuzzygroup/ansible/ansible-for-aws/wordpress/roles/mysql/templates/my.cnf.j2
    /usr/local/etc/my.cnf
    /usr/local/etc/my.cnf.d
    /Library/Ruby/Gems/2.0.0/gems/mysql2-0.4.4/spec/my.cnf.example
    /Users/sjohnson/Dropbox/fuzzygroup/rails/crawl_evaluation/daimon_as_framework/udemy/docker-cache/bundle/ruby/2.4.0/gems/mysql2-0.4.5/spec/my.cnf.example

So it looks to me like this gives me pretty much what I would have gotten from locate without having to worry about the locate db being updated and the like.  Kudos to Apple for this and bad on me for not knowing about this even after using OSX continuously since 2006 or so.  Sigh.
</description>
        <pubDate>Tue, 20 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/06/20/osx-tip-using-mdfind.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/06/20/osx-tip-using-mdfind.html</guid>
        
        <category>find</category>
        
        <category>mdfind</category>
        
        <category>command_line</category>
        
        <category>osx</category>
        
        <category>locate</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>No CI and No CD - Deploying Docker Swarm with Bash and Ansible</title>
        <description>This post may well verge on the heretical.  I'm getting close on a new SAAS app that is going to be run using containers and Docker Swarm as my container engine.  Rather than go the whole CI / CD route, I actually ended up using a combination of Bash and Ansible to handle my deployment onto AWS.  This is a side project type of thing and I didn't want to spend the time / $$$ to setup CI / CD which, honestly, can be frightfully expensive (and, yes, I'm glaring at you CircleCI and your ridiculous $50 / container / month model; containers allow nice partitioning down to the microservice level but pricing plans like this want you to run screaming back to monoliths; ARGH).  But I'm digressing into a rant so ...

# Under the Hood for a Docker Swarm Deploy

Docker Swarm is a way to easily run a cluster of Docker containers as a shared, coherent entity.  Here is what has to happen under the hood for a Docker Swarm deploy (or at least how I've done it; I'm sure there are other ways):

## Stage 1: Docker Build

The first stage is just building your Docker containers.  That looks like this:

    docker-compose build web
    docker tag seirawatchsite_web fuzzygroup/seirawatchsite_web:latest
    docker push fuzzygroup/seirawatchsite_web:latest
    
As you can see I'm using the Docker Hub container registry here and the reason is that Docker Hub really gets this right.  Nick goes over some of the numbers about Docker Hub in a [recent blog post](https://diveintodocker.com/blog/the-3-biggest-wins-when-using-alpine-as-a-base-docker-image) and the number of pulls is astonishing.

## Stage 2: Copy Up

The next stage is to simply copy of your .env, .env.production and docker-compose files up to all the hosts where your swarm runs.  I suppose if you *know* that these files won't change then you don't need to do this but, personally, I feel that you want this as part of your deploy process.  You can do this anyway you move files but I just used straight up scp commands:

    scp -i ~/.ssh/fuzzygroup2.pem .env  ubuntu@foo.com:~/seirawatchsite
    scp -i ~/.ssh/fuzzygroup2.pem .env.production  ubuntu@foo.com:~/seirawatchsite
    scp -i ~/.ssh/fuzzygroup2.pem docker-compose.production.yml ubuntu@foo.com:~/seirawatchsite
    
These are just going up to a project level directory on the underlying linux box where the container will run.  Putting them in a directory allows for another container for this project to exist on the same hardware.

## Stage 3: Restart Docker

The final stage is telling Docker Swarm to use the new files.  This requires using this Docker specific command:

    cd ~/seirawatchsite/ &amp;&amp; docker stack deploy -c docker-compose.production.yml --with-registry-auth seirawatchsite
    
The trick here is that you need to run that on all your Docker swarm hosts -- once you are logged into them.  Up to this point all of our commands have executed locally on a development system.  With this command we need to now run on the remote box where swarm is executing -- and that's where Ansible comes in but first we have to learn a cool bash trick.

# A Cool Ass Bash Trick

As I've alluded to in the past, I'm not a bash guy.  I initially grew up with DOS .bat files and I think that must have scarred me for life since while I use bash regularly, I've never, not once, really felt comfortable with.  For example, anyone who knows me personally knows that my bash prompt is never right.  One of the issues with deploy is that, in my opinion, when there is an error, it needs to HARD STOP IMMEDIATELY.  Otherwise you'll end up with your remote system in an arbitrary state which is **dangerous**.  So I turned to Google and StackOverflow with the phrase &quot;bash script exit on error&quot; and found a [wonderful answer](https://stackoverflow.com/questions/1378274).  You start by adding this function to the top of your bash script:

    function run() {
      cmd_output=$(eval $1)
      return_value=$?
      if [ $return_value != 0 ]; then
        echo &quot;Command $1 failed&quot;
        exit -1
      else
        echo &quot;output: $cmd_output&quot;
        echo &quot;Command succeeded.&quot;
      fi
      return $return_value
    }
    
You can then wrap each of your lines with the run command like this:

    run &quot;date&quot;
    run &quot;false&quot;
    run &quot;date&quot;

And then it exits if there is an error and it tells you where it stopped.

# Putting it All Together

Now that we have a mechanism for handling errors we can bring this all together into a single deploy.sh bash script:

    #!/bin/bash

    #
    # The Docker Specific Stuff
    #

    #function run comes from: https://stackoverflow.com/questions/1378274/ ; thank you velotron

    function run() {
      cmd_output=$(eval $1)
      return_value=$?
      if [ $return_value != 0 ]; then
        echo &quot;Command $1 failed&quot;
        exit -1
      else
        echo &quot;output: $cmd_output&quot;
        echo &quot;Command succeeded.&quot;
      fi
      return $return_value
    }

    echo &quot;Stage 1: Docker Build&quot;
    run &quot;docker-compose build web&quot;
    run &quot;docker tag seirawatchsite_web fuzzygroup/seirawatchsite_web:latest&quot;
    run &quot;docker push fuzzygroup/seirawatchsite_web:latest&quot;

    echo &quot;Stage 2: Copy Up&quot;

    run &quot;scp -i ~/.ssh/fuzzygroup2.pem .env  ubuntu@foo.com:~/seirawatchsite&quot;
    run &quot;scp -i ~/.ssh/fuzzygroup2.pem .env.production  ubuntu@foo.com:~/seirawatchsite&quot;
    run &quot;scp -i ~/.ssh/fuzzygroup2.pem docker-compose.production.yml ubuntu@foo.com:~/seirawatchsite&quot;

    echo &quot;Stage 3: Restart Docker Using Ansible&quot;

    run &quot;ansible-playbook -i script/ansible/inventories/production script/ansible/deploy.yml&quot;

The Ansible deploy.yml is about as simple as it gets for Ansible; damn close to a one liner:

    ---
    - name: Restart Docker Swarm
      become: no
      shell:  &quot;cd ~/seirawatchsite/ &amp;&amp; docker stack deploy -c docker-compose.production.yml --with-registry-auth seirawatchsite&quot;
      
Yes this needed to be wrapped in an entire Ansible environment including the full directory structure, etc but that really is pretty simple.  Here's what the whole thing looked like:


    ├── ansible.cfg
    ├── deploy.yml
    ├── group_vars
    ├── inventories
    │   └── production
    └── roles
        └── restart_docker_swarm
            └── tasks
                └── main.yml

# Closing Thoughts

So the question comes up whether or not this is a good way to tackle a Docker Swarm deploy and I have mixed feelings.  Yes CI / CD is definitely better but it is also a lot &quot;heavier&quot;.  I am very much a believer in understanding exactly the bits that you are deploying and CI / CD always makes me feel uncomfortable in that regard.  When you understand how deploy is working you also understand how to handle the inevitable deploy problem -- and there is always something.

Note: I did not deal with migrations in the course of this writing; I'll cover that in a future blog post.

# Thanks

My thanks, as always, go out to my Docker friend and mentor, [Nick Janetakis](https://diveintodocker.com/courses/dive-into-docker), a strong shout out to [Velotron on Stack Overflow](https://stackoverflow.com/users/958118/velotron) for his bash assistance.</description>
        <pubDate>Tue, 20 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/devops/2017/06/20/no-ci-and-no-cd-deploying-docker-swarm-with-bash-and-ansible.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/devops/2017/06/20/no-ci-and-no-cd-deploying-docker-swarm-with-bash-and-ansible.html</guid>
        
        <category>ci</category>
        
        <category>cd</category>
        
        <category>docker</category>
        
        <category>swarm</category>
        
        <category>docker_swarm</category>
        
        <category>bash</category>
        
        <category>ansible</category>
        
        
        <category>devops</category>
        
      </item>
    
      <item>
        <title>Freelance Billing Issues 1099 C2C W2</title>
        <description>While I'm not doing a lot of consulting right now, I just ran across something interesting when talking to a recruiter the other day.  He was asking me about my rates and referenced a number of terms that I actually didn't understand.   This blog post is my attempt to tease this out and understand it a bit more.

Traditionally I find an engagement and bill the client directly on a 1099 basis, easy peasy.  This recruiter specifically stated that his customer, the company I would ultimately work for, would not do 1099 at all.  The &quot;rationale&quot; was that they don't want any IRS issues with &quot;is this person really an employee?&quot;.  Given the myriad issues with 1099s and the way that companies have abused it, I can understand the caution.  He proceeded to ask me if I was &quot;C2C&quot; and apparently this means &quot;Company to Company&quot; and can be translated as &quot;Do you have a corporate entity of your own that you can bill through?&quot;.  

When I stated that I wasn't C2C he then asked me for my &quot;W2 rate&quot; and this apparently means &quot;Our hiring agency can actually employ you on a w2 basis&quot;.  The reason that he asked for a different rate was to cover the employment taxes.  Since he claimed the employment tax amounted to 7.5%, I simply took my normal billing rate and discounted it by 7.5%.  Had I not earlier told him my normal billing rate then I would have added the 7.5% to my billing rate.  Sigh.

Hopefully this is useful to someone out there.  If not at least this will be a place where I can find it in the future...</description>
        <pubDate>Sat, 17 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/freelance/2017/06/17/freelance-billing-issues-c2c-w2-etc.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/freelance/2017/06/17/freelance-billing-issues-c2c-w2-etc.html</guid>
        
        <category>freelance</category>
        
        <category>billing</category>
        
        <category>w2</category>
        
        <category>1099</category>
        
        <category>c2c</category>
        
        
        <category>freelance</category>
        
      </item>
    
      <item>
        <title>The Tools I Use</title>
        <description>My buddy [Nick Janetakis](https://nickjanetakis.com/), a solid guy and Docker Captain wrote an interesting thing on his blog: [The Tools I Use](https://nickjanetakis.com/blog/the-tools-i-use).  So I thought I'd take a shot at authoring the same thing only from an OSX perspective.  I am going to shamelessly steal his organization and approach to this topic.  

# OS 

Much to my regret my principal OS is Mac OSX Seira.  I've seen Apple's OS stability go down release by release from Snow Leopard forward and I find Seira to be the absolute worst yet.  Yes I'm a developer and my usage of iTerm is intense but a Unix box (and that's what all Mac's actually are) having an update of less than 30 days is disturbing at best.  Right now I have an 11 day uptime but lately I've been averaging about 2 days before my 6 month old box forcibly restarts all on its own.  Stack traces available on request if you care.

# Code Editor and Terminal

I use TextMate 2, a damn near antiquated editor by today's standards and I have a real love / hate relationship with it but I've written more code in than anything else except for VIM and it works fine for my needs.  A new editor would be wise but nothing feels as good as TextMate to me.

I use iTerm as a terminal and I regularly run an absolutely disturbing about of terminal sessions managed by Tmux which remains a stellar bit of technology.  Kudos to [Dv](https://www.linkedin.com/in/dvsuresh/) for hooking me on Tmux back in 2015; it genuinely improved my life.

# Notable Apps

Other than a text editor, terminal and Tmux, I use pretty little software but here goes:

* Chrome or Firefox for a browser with Brave on the side
* Activity Monitor at all times to shut down rogue Firefox sessions due to memory bloat
* Pages and Numbers for classical productivity although I do less and less of that every year
* Docker for containers
* [Transmit by Panic](https://panic.com/transmit/) as an FTP / S3 client
* [Fantastical 2](https://flexibits.com/fantastical)
* [ngrok](https://ngrok.com/) for http connection sharing (if you've never used ngrok and you're a developer, run, not walk and get it -- it is that good)
* Apple Mail for mail
* Slack because, well, Slack
* Skype (which gets worse and worse ever year it seems)
* TweetBot
* Wunderlist
* [Acorn](http://fuzzyblog.io/blog/software_worth_purchasing/2016/09/11/software-worth-purchasing-01-acorn.html)
* [Keypad Layout 2](https://github.com/janten/keypad-layout) for command key drive window resizing; yes I have the keyboard grid taped to my mac, it is that useful
* [Alfred 3](https://www.alfredapp.com/)
* [Enpass](http://fuzzyblog.io/blog/software_worth_purchasing/2016/09/15/software-worth-purchasing-02-enpass.html) as a digital wallet
* [Dropbox](http://fuzzyblog.io/blog/dropbox/2017/03/13/dropbox-for-the-software-developer.html) without which I simply could not function
* [Deckset](https://www.decksetapp.com/) for converting Markdown files to simple presentations
* Jekyll for blogging; so much of everything I do comes from Open Source but I wanted to call out Jekyll because, well, I'm using it right this second

# Computer, Desk and Phone

Here are the physical things I use:

* I run almost exclusively on a series of Mac laptops:
  * An old 15&quot; Macbook Pro which runs the family media server
  * A Macbook Air which is my blogging / writing / backup dev box
  * A 15&quot; Macbook Pro which is my secondary backup dev box but too slow for anything intensive
  * A 13&quot; Macbook Pro, last generation, maxed out on ram and SSD to be as fast as possible (note to Apple - we need vastly more than 16 gigs)
  * I also have an Intel NUC as a Linux box but I don't use it all that much
  * All my Macs are configured with [Ansible](http://fuzzyblog.io/blog/osx/2016/11/20/ansible-for-configuring-your-mac-so-much-better.html)
* Apple iPhone 6; I didn't find the iPhone 7 improvements in the small form factor (non plus) to be enough to justify the upgrade and now I'm hoping it lasts thru the iPhone 8 launch
* I built my own desk ([pic 1](http://fuzzyblog.io/blog/assets/desk1.jpg), [pic 2](http://fuzzyblog.io/blog/assets/desk2.jpg), [pic 3](http://fuzzyblog.io/blog/assets/desk3.jpg)) about 18 months ago and I love it.  I have an 8 foot long desk with plenty of space.
* I use iPads to monitor long running jobs using Transmit's late StatusBoard app.  Pity that went away.
  
# Recording and Music

I don't do a lot of this but what I do use is:

* ScreenFlow for recording screencasts
* iTunes for music
* Audacity for audio editing on the rare occasion when I need that 
* Bose Quiet Comfort Headphones Wired for pair programming
* Bost Quiet Comfort Headphones BlueTooth for listening to music and isolating out the world in coffee shops


</description>
        <pubDate>Tue, 13 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/06/13/the-tools-i-use.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/06/13/the-tools-i-use.html</guid>
        
        <category>osx</category>
        
        <category>tools</category>
        
        <category>development</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>To Compress or Not to Compress - An S3 Question</title>
        <description>I'm finally at what I refer to derisively as the &quot;Turd Polishing&quot; stage of a new SAAS application that I hope to roll out over the next month or so.  I've validated the:

* the market
* the concept
* figured out how to get initial users over the OOBE (out of box experience)

and now I'm at the stage of looking at the underlying crawler / indexer and how it acquires data.  Up to this stage I've been archiving daily crawl data to a flat directory on my hard disc and when I realized that I had over a million html archive files in a single directory, well, oops.  So yesterday was devoted to moving them into date named directories like:

* page_archives/2017-06-08
* page_archives/2017-06-09

and so on.  I'm currently archiving about 10 gigabytes per day spread across 200k to 240k individual files.  There is a backing database table that stores per object metadata so I can fetch back the information that I need.  My long term plan has always been to store this information in S3 and looking at the data last night I had the epiphany *I need to compress it* -- or do it?

Here's what I know:

* 200K files per day
* 10 gigs per day, every day so 300 gigs per month
* 6,000,000 put requests per month; dramatically lower read volume

The data I'm archiving are simple html files and a quick test with gzip shows dramatic compression:

    206729 Jun  9 06:41 ad69734d630423333479b0a954ab52baf056c16d.html
    24881 Jun  9 06:41 ad69734d630423333479b0a954ab52baf056c16d.html.gz

Yep - that's a 10x difference in size, 200K down to 24K.  Surely it must be worth it to compress, right?  Normally this is exactly where I'd be rolling up my sleeves and implementing but, happily something wasn't sitting right with me so I turned to the much maligned AWS calculator.  I ran two calculations, each of which is screen shotted below:

## No Compression
![s3_no_compression.png](/blog/assets/s3_no_compression.png)


## Compression
![s3_compression.png](/blog/assets/s3_compression.png)

Yep.  That's right -- the grand price difference of a 10 fold difference in size is all of $6 or 16.67%.  Now a 16 % cost savings shouldn't be dismissed but at the current scale I'm running it, it is a rounding error and can be ignored.

## The Moral of the Story

Everyone always says that you can never understand your bottlenecks without benchmarking and that has always proven to be true for me.  I think guessing about cost structures in today's cloud follows a similar rule -- benchmark first and then implement, what you think may be pricey may actually not be.</description>
        <pubDate>Fri, 09 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/06/09/to-compress-or-not-to-compress-an-s3-question.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/06/09/to-compress-or-not-to-compress-an-s3-question.html</guid>
        
        <category>aws</category>
        
        <category>hosting</category>
        
        <category>s3</category>
        
        <category>pricing</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Adding Cron to a Dockerized Rails Application Using Clockwork</title>
        <description>If you talk to any computer scientist they will easily tell you that 50 plus years into the computer age, scheduling is NOT a solved problem.  Even something as simple as cron which is decades old can still be challenging under different environments (example - cron and RVM is a bloody nightmare).  And then when you add containers into the mix, well ...  In this blog post I outline how I'm handling a daily scheduled process using Docker and Clockwork (a cron like gem for Ruby).

# Why is Cron and Docker a Problem

Docker represents a simplified computing environment where you generally don't have a full Linux stack -- instead the model is generally a single root process.  Now there are people who challenge that approach, notably the good folks at Phusion, but this is generally regarded as a best practice.  So you're not your own application stack and then cron as well.

# Enter Clockwork

Clockwork is a long standing Ruby gem which acts as a persistent daemon with its own scheduler that executes a simple DSL allow your models to be easily executed.  Here's an example:

    every(1.day, 'Midnight.job -- PageArchive.update_everything', :at =&gt; '00:00') do |job|
      PageArchive.update_everything
    end
    
Unlike traditional cron syntax, I've never found an issue with reading the Clockwork dsl, this says:

* every day
* execute at midnight
* run PageArchive.update_everything

# Adding Clockwork to your Rails Application

Here are the steps to add clockwork to your Rails application:

Add the clockwork gem to your Gemfile: 

&gt; gem 'clockwork'

Create a clock.rb file in lib:

&gt; touch lib/clock.rb

Write one or more clockwork expressions in clock.rb.  Here's an example from my application:

    require 'clockwork'
    include Clockwork

    require File.expand_path('../../config/boot', __FILE__)

    require File.expand_path('../../config/environment', __FILE__)

    require 'clockwork'

    include Clockwork

    module Clockwork

      every(1.day, 'Midnight.job -- PageArchive.update_everything', :at =&gt; '03:58') do |job|
        PageArchive.update_everything
      end

      every(1.week, 'Weekly Job -- PageArchive.update_bing', :at =&gt; '00:00') do |job|
        PageArchive.update_bing
      end

    end
    
To test this you can just use the command line:

&gt; bundle exec clockwork lib/clock.rb

Once you set that running then you need to simply watch it to make sure that tasks execute.  Yes, provided that your syntax is correct, they certainly should but I've seen enough scheduled jobs fail over the years that I always feel better when I actually see them run.

**Note:** If you were previously using a Rake task to run your daily jobs then you need to refactor that as something like a class method that can be called from the Rake task.  This allows you to continue using the Rake task but also make your code easily executable from Clockwork.

# The Dockerfile

In this example I have a simple Rails app which exists to populate a page archive database consisting of data harvested daily from the Internet.  Here's the Dockerfile:

    FROM ruby:2.3.1-alpine

    RUN apk update &amp;&amp; apk add build-base nodejs mariadb-dev tzdata

    RUN mkdir /app
    WORKDIR /app

    COPY Gemfile Gemfile.lock ./
    RUN bundle install --binstubs

    COPY . .

    CMD bundle exec clockwork lib/clock.rb
    
The secret to making the scheduling work is to execute the clockwork executable as the root command in the container. This causes the Clockwork executable to be launched when the container initializes.  At that point Clockwork will run the command until it finishes and then remain running, waiting for its next invocation.

# Conclusion and Issues

As you can see, when you have a scheduling process as the root process in your container, it provides an easy way to handle your scheduled job needs and Clockwork really does make it easy.  Still this doesn't mean that scheduling for your application is necessarily solved:

* What if your needs are large enough that you have to have multiple machines?
* How do you log issues with the scheduled jobs?
* What happens when a deploy occurs while your container is running a job?  How do you ensure that the day's work actually got processed?

## Multiple Machines

If I needed to have multiple machines involved, I would likely implement some kind of a work queue where they scheduler is solely responsible for setting up a queue of the work to be done and then containers on other machines are responsible for processing the data in the queue.  This approach is also useful for the deploy issue covered below.

## Logging Issues with Scheduled Jobs

There are enough issues with containers and logs that addressing it here is really out of the scope of this blog post.  I did want to point it out, however, as logging around scheduled jobs is usually necessary and needs to be thought through.

## Deploy Conflicts

There are at least three strategies that I'd use for handling deploy conflicts:

1. Start your jobs early and simply don't deploy while they are running.  This is the least desirable strategy but it does actually work provided the execution time of the job is only a few hours (i.e. until when people need to deploy).  Obviously this works poorly with a globally distributed labor pool and a CI server that deploys code automatically.
2. Change the code so that jobs become idempotent i.e. the job knows that a particular data item can be processed, for example, only once per day and then allow the scheduled process to be run multiple times per day.  This decreases the chance of a deploy shutting things down fully for an entire day since statistically it becomes less likely that you'll conflict with something every single time that it is running allowing at least one of the runs to complete.
3. Use the multiple machines strategy mentioned above so that you have a work queue and multiple asynchronous processing engines on the data.  As long as the queue is atomic in nature this avoids duplication issues and the additional parallelism that multiple containers bring should process the data more quickly thereby leading to fewer deploy conflicts.

Of each of these strategies, the third is likely the best but incurs the most application level changes.  Please keep in mind that you can iteratively evolve your application towards the right strategy for your needs.

</description>
        <pubDate>Thu, 11 May 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/05/11/adding-cron-to-a-dockerized-rails-application-using-clockwork.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/05/11/adding-cron-to-a-dockerized-rails-application-using-clockwork.html</guid>
        
        <category>rails</category>
        
        <category>docker</category>
        
        <category>cron</category>
        
        <category>clockwork</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Building a Rails API App Which Accepts Data from JavaScript</title>
        <description>A friend recently described a web tracking problem to me and we discussed looking at it via Google Analytics or by rolling our own approach.  Google Analytics is tremendously powerful but I looked at this as a chance to improve my JavaScript skills which are, admittedly, not my strongest part of my technical skill set so I went with the roll my own approach.  As with most things I do, I'm writing this out in full to clarify my own thinking and understanding of the problem as well as to make this information publicly available.

# Problem

The problem at hand was to record tracking data as people viewed pages on an e-commerce web site.  The goal was to correlate the tracking data and try and resolve the per visitor url paths that were traversed.  I also wanted to experiment with browser fingerprinting so I used the FingerprintJS2 library.

# Solution

The solution was two fold:

* JavaScript that runs in the browser and executes an HTTP get with the details of the url that the user visited
* A Rails API server that accepts the details of the url that was visited and logs it to a database

# Part 1: JavaScript

I'm not a JavaScript expert by any means so this js code was largely pulled together from StackOverflow and other sources (references are at the end).  I ended up writing this twice, first starting with jQuery and then realizing that for something embeddable you really want to eliminate every dependency -- so getting rid of jQuery itself is a desirable goal.

## Browser Fingerprinting

A browser fingerprint is an SHA / MD5 style hash which reflects a unique profile of a browser based on the browser's uniquely exposed capabilities, platform, fonts and other attributes.  While not necessarily as unique as a fingerprint, browser fingerprint is actually a robust approach to tracking a user and since it is based on readily available [open source libraries](https://github.com/Valve/fingerprintjs2), it isn't something that needs to be developed from scratch.  And, happily, it even gets good marks on my new [Should I Gem](http://shouldigem.com/report_cards/146?url=https%3A%2F%2Fgithub.com%2FValve%2Ffingerprintjs2) tool.

Here's all it takes to calculate a browser fingerprint:

    &lt;!-- bring in the library --&gt;
    &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/fingerprintjs2/1.5.1/fingerprint2.min.js&quot;&gt;&lt;/script&gt;
    &lt;!-- the hash will be in the variable result --&gt;
    &lt;script type=&quot;text/javascript&quot;&gt;
        new Fingerprint2().get(function(result, components){
        }
    &lt;/script&gt;
    
Now that we know how to calculate a browser fingerprint, the next step is the sending it to our API service either by jQuery or a native XHR call.

## The jQuery Approach

Here's the jQuery code:

    &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/fingerprintjs2/1.5.1/fingerprint2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/javascript&quot;&gt;
        new Fingerprint2().get(function(result, components){
     
        $(document).ready(function(){

             $.post('http://localhost:3820/api/log_it', {
               &quot;api_key&quot;: &quot;foobarbaz&quot;,
               &quot;url&quot;: window.location.href,
               &quot;fingerprint&quot;: result
             }, function(serverResponse){
             })

         })
     
       });
    &lt;/script&gt;

## The XHR Approach

Here's the XHR code:

    &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/fingerprintjs2/1.5.1/fingerprint2.min.js&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/javascript&quot;&gt;
        new Fingerprint2().get(function(result, components){
          var xhr = new XMLHttpRequest();
          var dest_url = &quot;http://localhost:3820/api/log_it?&quot; + &quot;api_key=&quot; + &quot;foobarbaz&quot; + &quot;&amp;&quot; + &quot;url=&quot; + window.location.href + &quot;&amp;&quot; + &quot;fingerprint=&quot; + result;
          xhr.open('GET', dest_url);
          xhr.onreadystatechange = function(e) {
            if(xhr.readyState === 4)
              console.log(&quot;got result: &quot;, xhr.responseText);
          };
          xhr.send();
       });
    &lt;/script&gt;
    

# Part 2: Rails API Server

I've become an increasing fan of microservices as an architectural paradigm and Rails API servers are something I now generate regularly.  Here's how I did that:

&gt; rails new --api tracking_api --database=mysql --skip-spring --skip-listen --skip-sprockets

You'll notice that I'm skipping spring and listen which gets rid of the [disaster that is the evented file watcher on OSX](http://fuzzyblog.io/blog/osx/2017/03/20/getting-around-osx-bash-fork-issues.html) / [Github Issue](https://github.com/puma/puma-dev/issues/56) (this causes huge issues with overly aggressive process launching).

## Getting Past CORS Restrictions

Once this is done we need to add the rack-cors gem with:

&gt; gem 'rack-cors'

in the Gemfile.  In case you've been following what I've done with [ShouldIGem.com](http://www.shouldigem.com), you might be curious how rack-cors fared -- it got an [A](http://shouldigem.com/report_cards/141?url=https%3A%2F%2Fgithub.com%2Fcyu%2Frack-cors).

The rack-cors gem addresses the problems with Cross Origin Scripting (CORS) by allowing domain X to post or get to domain Y.  While there are a bunch of other approaches to getting around CORS, the rack-cors gem was by far the best solution I found.

Note: There are security implications if you open rack-cors to everything; please keep that in mind.

## Writing the Api

After a bundle install, I generated an Api controller with:

&gt; bundle exec rails g controller api

I then wrote a simple method to capture the data and save it to a PageView object in the database:

    class ApiController &lt; ApplicationController
  
      def log_it
        PageView.create_page_view(params[:fingerprint], request.remote_ip, params[:url], request.user_agent)
      end

    end
    
I then needed to write a route for this as follows:

    Rails.application.routes.draw do
      get '/api/log_it', to: 'api#log_it'
    end
    
The PageView object is simply an ActiveRecord model which logs the view to the database and it is simple enough that I'm not going to cover it here.

Note: This is a simplified controller.  I have deliberately omitted handling the api key parameter which limits calls against the API to only those with valid API keys.

## Testing this with Wget or Curl

As I wrote about previously, I'm a huge believer in using curl or wget to test APIs.  Here is a simple wget statement that exercises the API as you tail a log file to make sure everything works:

&gt; wget &quot;http://localhost:3820/api/log_it?api_key=foobarbaz&amp;url=http%3A%2F%2Flocalhost%3A3400%2F&amp;fingerprint=e82eaadd&quot;

## Testing this From the Browser

To test this from the browser just embed it in an HTML page and tail the logs on the API service.  Here's what those logs look like:

    Started GET &quot;/api/log_it?api_key=foobarbaz&amp;url=http://localhost:3400/&amp;fingerprint=e82eaadd998e1aae0309b781029f8edb&quot; for 127.0.0.1 at 2017-05-09 18:35:32 -0400
    Processing by ApiController#log_it as */*
      Parameters: {&quot;api_key&quot;=&gt;&quot;hyde314159&quot;, &quot;url&quot;=&gt;&quot;http://localhost:3400/&quot;, &quot;fingerprint&quot;=&gt;&quot;e82eaadd998e1aae0309b781029f8edb&quot;}
       (0.2ms)  BEGIN
      SQL (0.4ms)  INSERT INTO `page_views` (`created_at`, `updated_at`, `date_created_at`, `fingerprint`, `ip_address`, `user_agent`, `url`, `url_base`) VALUES ('2017-05-09 22:35:32', '2017-05-09 22:35:32', '2017-05-09', 'e82eaadd998e1aae0309b781029f8edb', '127.0.0.1', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36', 'http://localhost:3400/', 'localhost/')
       (0.8ms)  COMMIT
    Completed 204 No Content in 4ms (ActiveRecord: 1.4ms)

# Conclusions

This was an interesting exercise which taught me a bit of JavaScript, illustrated how to use the browser fingerprinting and finally taught me a way around CORS restrictions -- the rack-cors gem.  Hopefully this blog post illustrated for you how to integrate external JavaScript with a Rails api.

# References

* [http://stackoverflow.com/questions/38290552/using-rest-api-and-send-post-request](http://stackoverflow.com/questions/38290552/using-rest-api-and-send-post-request)
* [http://stackoverflow.com/questions/1034621/get-current-url-in-web-browser](http://stackoverflow.com/questions/1034621/get-current-url-in-web-browser)
* [http://test-cors.org/](http://test-cors.org/)
* [https://github.com/monsur/test-cors.org](https://github.com/monsur/test-cors.org)
* [https://github.com/jpillora/xdomain](https://github.com/jpillora/xdomain)
* [https://learn.jquery.com/ajax/working-with-jsonp/](https://learn.jquery.com/ajax/working-with-jsonp/)
* [http://stackoverflow.com/questions/29751115/how-to-enable-cors-in-rails-4-app](http://stackoverflow.com/questions/29751115/how-to-enable-cors-in-rails-4-app)
* [https://github.com/cyu/rack-cors](https://github.com/cyu/rack-cors)
* [http://stackoverflow.com/questions/20035101/no-access-control-allow-origin-header-is-present-on-the-requested-resource](http://stackoverflow.com/questions/20035101/no-access-control-allow-origin-header-is-present-on-the-requested-resource)</description>
        <pubDate>Wed, 10 May 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/05/10/building-a-rails-api-app-which-accepts-data-from-javascript.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/05/10/building-a-rails-api-app-which-accepts-data-from-javascript.html</guid>
        
        <category>rails</category>
        
        <category>javascript</category>
        
        <category>api</category>
        
        <category>cors</category>
        
        <category>fingerprint</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Testing Out New Programming Languages Using Docker</title>
        <description>I am, as you likely know, an unabashed Ruby fan -- love the language; loathe the performance but still a huge, huge damn fan.  Ruby is the first language I ever fell in love with and like many first loves, they never really fade.  Still, even so, there times when you just hunger for something new.  Here's how to get a new programming language up and running with Docker for a handful of different environments.  I deliberately chose languages that can boot into a console or REPL (replace-evaluate-print-loop) since that's the easiest way to get going in terms of experimenting with a language.

# Installing Docker

The easiest way to install Docker is to follow the official directions [here](https://docs.docker.com/engine/installation/).  You want Docker Community Edition in likely one of these platforms:

* [Mac](https://docs.docker.com/docker-for-mac/install/)
* [Windows](https://docs.docker.com/docker-for-windows/install/)
* [Ubuntu](https://docs.docker.com/engine/installation/linux/ubuntu/)

Please note that Docker install urls change frequently so you may need to look around a bit.  They work today, end of April 2017, how long that lasts is unclear.

# Docker Basics

There are a few basic Docker things that you need to know, particularly, if you run Docker on OSX.  Docker is a container technology, similar to a virtual machine (albeit faster, smaller and very differently implemented) that allows you to run programs.  The best way I understand Docker is to think of it as a portable runtime that is bound to a software stack and allows you to achieve build once, run anywhere status.  The software stack can be your own code, 3rd party code such as a database or some combination.

## The ps and kill Commands

Just like your operating system has a ps command, so too does Docker:

![docker_ps.png](/blog/assets/docker_ps.png)

Unlike your operating system where a simple number denotes a process id, Docker uses a git like hash for each process.

Just like your operating system has a kill command, so too does docker which you run with: 

&gt; docker kill hash

The general syntax shown above of **docker** *command* is what you'll see below.  Normally there will be other options as well which you can see illustrated below.


## The Magic Eval Statement

If you get this message:

docker ps
Cannot connect to the Docker daemon at tcp://192.168.59.103:2376. Is the docker daemon running?

after running a docker ps or any docker command then its an issue of not being able to find the Docker daemon and then you need this bit of magic:

&gt; eval $(docker-machine env ${C_DOCKER_MACHINE})

My bash skills are laughably bad so I'm not going to even try and explain that.  Just think of it as a magic spell given to you by an experienced wizard -- you don't have to understand how it works as long as it does.

## The images Command

Run the command docker images to find all the Docker images on your system.  You'll be surprised at how big some of these can be and they often will hang out on your system, just eating your disc space.  Just as an example, a prototype I played with using the tleyden5iwx/open-ocr-preprocessor OpenOcr image over a year ago is still on my system using 1.3 gigs of disc sapce.

## Disc Space Usage / Where Does All This Go?

The location of all your images varies by [operating system](http://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine).  On OSX it is stored under the virtual machine:

&gt; ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2

Please keep in mind that you can use up a considerable amount of disc space just by experimenting with images.  Here's my system:

    du -hc ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
     21G	/Users/sjohnson/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2
     21G	total

# Testing Different Languages

## Ruby

In case you're not a rubyist, I thought I'd start with this one.  [Ruby on Docker](https://hub.docker.com/r/library/ruby/tags/)

docker run --rm --name ruby -it ruby:2.4-alpine

## Elixir 

Elixir is a next generation functional language designed by some of the former Rails core members.  [Elixir on Docker](https://hub.docker.com/_/elixir/)

docker run --rm --name elixir -it elixir:1.4-slim

Here's what this process looks like while it runs:

![docker_elixir.png](/blog/assets/docker_elixir.png)

## PHP 7.1

PHP is, well, php.  Do I really need to say any more?  [PHP On Docker](https://hub.docker.com/r/library/php/tags/)

docker run --rm --name php -it php:7.1-alpine

## Python 2.7

Python is a dynamic scripting language similar to Ruby.  Python 2.7 is the mainstream version of Python.  [Python on Docker](https://hub.docker.com/_/python/)

docker run --rm --name python27 -it python:2.7-slim

## Python 3

Python 3 is a newer version of Python that hasn't been as widely adopted as the Python 2 family.

docker run --rm --name python33 -it python:3.3-slim

## R

I really, really like R for statistical analysis and data crunching.  [R on Docker](https://hub.docker.com/_/r-base/)

docker run --rm --name r-base -it r-base:3.4.0

R can be a bit different from other languages due to its mathematical orientation.  Here's a sample R one liner you can use to test it:

&gt; print(sample(1:10))

[More R Examples](http://www.rexamples.com/)

## Perl

Perl in many ways is the grandfather to all modern web development.  Unlike the languages above, Perl isn't a REPL based language so you'll have to pass a script in when you run it.  [Perl on Docker](https://hub.docker.com/_/perl/)

 docker run -it --rm --name my-running-script -v &quot;$PWD&quot;:/usr/src/myapp -w /usr/src/myapp perl:5.20 perl your-daemon-or-script.pl

## Julia

Julia is a high performance language used for finance and math that runs on top of the Java VM.  [Julia on Docker](https://hub.docker.com/_/julia/)

docker run -it --rm -v &quot;$PWD&quot;:/usr/myapp -w /usr/myapp julia julia script.jl arg1 arg2

# Learning More About Docker

Most of what I know about Docker, I learned from The [Dive into Docker](https://diveintodocker.com/courses/dive-into-docker) course by Nick Janetakis.  Recommended.</description>
        <pubDate>Wed, 26 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/docker/2017/04/26/testing-out-new-programming-languages-using-docker.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/docker/2017/04/26/testing-out-new-programming-languages-using-docker.html</guid>
        
        <category>docker</category>
        
        <category>programming</category>
        
        <category>elixir</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>OSX MySQL Disc Space Usage and Location</title>
        <description>Your permanent storage for a SAAS app is always one of your single biggest per user costs.  There are two ways to get this info -- at the SQL layer with a query and at the filesystem layer.  Each method is explained.

# Getting Disc Space From MySQL With a Query

You can run this query:

    select table_schema, sum((data_length+index_length)/1024/1024) AS MB from information_schema.tables group by 1;

and you'll get back a per database assessment of disc space used.  That's the easiest way to get this information.  Here's what it looks like:

    | seira_crawler_development         | 354.68750000 |
    | seira_crawler_test                |   0.23437500 |
    | seira_watch_development           |   0.75000000 |
    | seira_watch_site_development      |   0.12500000 |
    | seira_watch_web_app_1_development |   2.82812500 |
    | seira_watch_web_app_2_development |   1.21875000 |

# Where Is Your Physical Database Instance?

Run this mysql query from your command line:

    mysql -uUSER -p -e 'SHOW VARIABLES WHERE Variable_Name LIKE &quot;%dir&quot;'

You'll see something like this:

    mysql -uUSER -p -e 'SHOW VARIABLES WHERE Variable_Name LIKE &quot;%dir&quot;'
    Enter password:
    +---------------------------+---------------------------------------------------------+
    | Variable_name             | Value                                                   |
    +---------------------------+---------------------------------------------------------+
    | aria_sync_log_dir         | NEWFILE                                                 |
    | basedir                   | /usr/local/Cellar/mariadb/10.1.19                       |
    | character_sets_dir        | /usr/local/Cellar/mariadb/10.1.19/share/mysql/charsets/ |
    | datadir                   | /usr/local/var/mysql/                                   |
    | innodb_data_home_dir      |                                                         |
    | innodb_log_arch_dir       | ./                                                      |
    | innodb_log_group_home_dir | ./                                                      |
    | innodb_tmpdir             |                                                         |
    | lc_messages_dir           |                                                         |
    | plugin_dir                | /usr/local/Cellar/mariadb/10.1.19/lib/plugin/           |
    | slave_load_tmpdir         | /var/folders/76/mqchlgzs6x5cv2f11jh3jkjw0000gn/T/       |
    | tmpdir                    | /var/folders/76/mqchlgzs6x5cv2f11jh3jkjw0000gn/T/       |
    | wsrep_data_home_dir       | /usr/local/var/mysql/                                   |
    +---------------------------+---------------------------------------------------------+

The path datadir is where your data is stored.  For my system I want to look into this path: /usr/local/var/mysql/.

# Assessing Disc Space Usage

Running an ls -l on the path in datadir will show you all your databases on your development machine.  Doing a du -h on the path to any database on your system will show you a result like this:

    du -hc /usr/local/var/mysql/seira_crawler_development/
    400M    /usr/local/var/mysql/seira_crawler_development/
    400M    total

That's the metric for my backing crawler data archive.  The data for a single user instance is:

    du -hc /usr/local/var/mysql/seira_watch_web_app_1_development/
     12M    /usr/local/var/mysql/seira_watch_web_app_1_development/
     12M    total

# Scaling the Numbers Up

If you want to assess your disc space needs for a SAAS app, take your per user disc space needs and then multiply it by the number of users that you are projecting to handle and then add the disc space for any system level databases (such as the crawler data above).

# Adding the Numbers to a Dashboard

If your SAAS app supports an admin tool of any type then you want to watch the growth of this number pretty carefully since unexpected database growth can lead to system outages and the like.  Here's a quick Ruby snippet to get this information as a float which you could log, graph, etc:

    def self.database_size
      sql = &quot;select table_schema, sum((data_length+index_length)/1024/1024) AS MB from information_schema.tables where table_schema='#{Rails.configuration.database_configuration[Rails.env][&quot;database&quot;]}' group by 1;&quot;
      results = ActiveRecord::Base.connection.select_all(sql)
      return results.first[&quot;MB&quot;].to_f
    end
    
  I'm getting the name of the current database in an environment agnostic way using: **Rails.configuration.database_configuration[Rails.env][&quot;database&quot;]**.  That value was read from the configuration file.  You could also get it from: **ActiveRecord::Base.connection.current_database** (which to use depends on whether or not you are using ActiveRecord or a different storage abstraction).
  
  Note: I'm feed this, via a JSON feed, to my new Cartazzi engine and I'm pretty pleased with this as an admin tool.

# References

* [Finding Your MySQL Data Directory](http://stackoverflow.com/questions/17968287/how-to-find-the-mysql-data-directory-from-command-line-in-windows)
* [mysqldiskusage utility (not always available)](https://dev.mysql.com/doc/mysql-utilities/1.5/en/mysqldiskusage.html)
* [Calculating Disc Space Usage with a Query](https://dba.stackexchange.com/questions/14337/calculating-disk-space-usage-per-mysql-db)
* [Getting Current ActiveRecord Configuration](http://stackoverflow.com/questions/10001583/how-to-check-the-database-name-that-activerecord-uses)</description>
        <pubDate>Fri, 21 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/mysql/2017/04/21/osx-mysql-disc-space-usage-and-location.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mysql/2017/04/21/osx-mysql-disc-space-usage-and-location.html</guid>
        
        <category>osx</category>
        
        <category>mysql</category>
        
        <category>mariadb</category>
        
        <category>saas</category>
        
        <category>ruby</category>
        
        <category>hyde</category>
        
        
        <category>mysql</category>
        
      </item>
    
      <item>
        <title>Returning to a Better Diff Tool</title>
        <description>So this morning I needed to diff some fairly complex html and see the differences. This is part of a page archiver I'm building and the command:

&gt; diff 2094f14b81ddd5d6cf8eab7a487e97e4f34935c8.html 68e0f2de414da40d33a1728d6c5c592d12fc54ae.html

just didn't give me anything really intelligible (or I'm too dumb to read raw diff output; its possible).  I've always maintained that the reason I write a blog is to capture my own knowledge so I turned to a [post](http://fuzzyblog.io/blog/software_engineering/2017/02/07/on-merging-files-diff-alternatives-on-the-mac.html) I wrote a few months ago on this very topic.

Ah Ha!  I need to use OpenDiff, a part of Xcode.  Unfortunately this gave me:

    opendiff 2094f14b81ddd5d6cf8eab7a487e
    97e4f34935c8.html 68e0f2de414da40d33a1728d6c5c592d12fc54ae.html
    xcode-select: error: tool 'opendiff' requires Xcode, but active developer directory '/Library/Developer/CommandLineTools' is a com
    mand line tools instance
    
Messing around with an Xcode install always seems to mean rebooting your machine and I have an active crawl going on my development box so that was out of the question.  Looking back to my diff article, one of the other tools was Meld which is a part of [HomeBrew](https://brew.sh/).  The beauty of anything HomeBrew is always the installation model.  Brew even helpfully tells you when the installation model changes which it did this time so I now needed to use:

&gt; brew install caskroom/cask/meld

A very, very quick command:

&gt;  meld 2094f14b81ddd5d6cf8eab7a487e97e4
f34935c8.html 68e0f2de414da40d33a1728d6c5c592d12fc54ae.html

gave me exactly the info I needed:

![meld_diff.png](/blog/assets/meld_diff.png)
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/04/18/returning-to-a-better-diff-tool.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/04/18/returning-to-a-better-diff-tool.html</guid>
        
        <category>software_engineering</category>
        
        <category>diff</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Rails Idiocy - When Params In Your Controller Are Nil</title>
        <description>So I got up early and got cracking on some code and then I hit an absolute wall -- the kind of thing that just stops you cold.  I was debugging a controller in my side project and I got a nil on accessing a member on the params hash.  And since this was something I'd done previously I thought it odd but added debugger to my controller and evaluated params:

    [12, 21] in /Users/sjohnson/Dropbox/fuzzygroup/hyde/seira_watch_web_app/app/controllers/instructors_controller.rb
       12:   end
       13:
       14:   def create
       15:     @instructor = Instructor.new# =&gt; (params[:instructor])
       16:     debugger
    =&gt; 17:     if params[:instructor][:url] =~ /http/
       18:     else
       19:       flash[:error] = &quot;Please specify a valid url&quot;
       20:       redirect_to edit_instructor_path(@instructor) and return
       21:     end
    (byebug) params
    nil
    
A decade plus in Rails and Ghu only knows how many controllers and I don't think I've ever seen nil for params.  I mean params is effectively a god object that is always there.  How the hell does it become nil?

This was when I put my keyboard down, went over to my editor and started doing some writing.  I knew in my gut that this was an oddball issue and what I really needed was another set of eyes.  And, an hour later, my buddy [Nick](http://www.nickjanetakis.com/blog/), gave me a hand over Google Hangouts pointing out that I had something wrong my strong params method:

```ruby
def instructor_params
  params.require[:instructor].permit(:url)
end
```
    
The above code is lexically correct and won't cause any errors but it will absolutely screw everything up and make params goto nil.  This should have been:

```ruby
def instructor_params
  params.require(:instructor).permit(:url)
end
```

The difference between [ ] and () is of course very, very real but it is subtle, particularly from a visual angle where you can easily mistake them.</description>
        <pubDate>Wed, 12 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/04/12/rails-idiocy-when-params-in-your-controller-are-nil.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/04/12/rails-idiocy-when-params-in-your-controller-are-nil.html</guid>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Executing Word Counts with Bash</title>
        <description>I'm in the process of starting a new writing project and I wanted to make the process of getting metrics easier so I rolled together this bash script:

    #!/bin/bash
    echo &quot;Word Count:&quot;
    cat *.md | wc -w
    echo &quot;Page Count:&quot;
    cat *.md | wc -w | xargs -I{} expr {} / 250
    
If you're looking to do this yourself then these commands all go into a single text file and a chmod +x command on the file makes it executable.  

Here's the output of this script:

    ./wordcount
    Word Count:
        3827
    Page Count:
    15
    
The interesting tool here is **expr** which is a command line math tool.  The 250 is a metric that I use for the number of words per printed page.  I know in the modern era that a printed page is perhaps a bit of an anachronism but I find it very comforting to know that I wrote 15 pages today (which is what I actually turned out today).
    
What this does is:

* take a collection of markdown files 
* count all the words in the collection of markdown files
* count all the words in the markdown files and then feed that value into xargs which then feeds the value into expr and divides it by 250

# Cross References:

Here are the places where I sourced some of these techniques:

* [Executing word counts using cat and wc](cat *work* | wc -w)
* [An example using using Xargs to feed into expr](http://stackoverflow.com/questions/13182070/best-way-to-divide-in-bash-using-pipes)</description>
        <pubDate>Wed, 12 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/bash/2017/04/12/executing-word-counts-with-bash.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/bash/2017/04/12/executing-word-counts-with-bash.html</guid>
        
        <category>bash</category>
        
        <category>linux</category>
        
        <category>shell</category>
        
        
        <category>bash</category>
        
      </item>
    
      <item>
        <title>Color Palette Design Tools</title>
        <description>Well Google released their new color palette tool for [Material](https://material.io/color/) and I thought I should point out some of the other tools for this: 

* [Sankk.in](https://www.sankk.in/material-mixer/)
* [Flat UI Colors](http://flatuicolors.com/)
* [MaterialPalette](https://www.materialpalette.com/)
* [Adobe Color Wheel](https://color.adobe.com/create/color-wheel/)
* [Material Design Bootstrap](https://mdbootstrap.com/css/colors/)
* [Color-Hex](http://www.color-hex.com/color-palettes/)

Thanks to this [Hacker News thread](https://news.ycombinator.com/item?id=14085425) for surfacing some of these; the last two I found when I went looking for bootstrap versions.

</description>
        <pubDate>Tue, 11 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/design/2017/04/11/color-palette-design-tools.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/design/2017/04/11/color-palette-design-tools.html</guid>
        
        <category>design</category>
        
        <category>color</category>
        
        
        <category>design</category>
        
      </item>
    
      <item>
        <title>Rails Tip - When Puma Doesn't Run on the Specified Port</title>
        <description>So I just noticed this particular oddness:

    12:12 $ bundle exec rails s -p4000
    =&gt; Booting Puma
    =&gt; Rails 5.0.1 application starting in development on http://localhost:4000
    =&gt; Run `rails server -h` for more startup options
    Puma starting in single mode...
    * Version 3.7.0 (ruby 2.3.1-p112), codename: Snowy Sagebrush
    * Min threads: 5, max threads: 5
    * Environment: development
    * Listening on tcp://0.0.0.0:3000

As you can see Puma is being started with a port 4000 directive but it is actually listening on the default 3000 port.  

A bit of googling led me to this bit of text in a [Github issue](https://github.com/puma/puma/issues/1200):

&gt; After updating the gem version and bundling: gem 'puma', '~&gt; 3.6.2' [More...](https://github.com/puma/puma/issues/1200#issuecomment-278606118)

I edited my Gemfile and replaced:

&gt; gem 'puma', '~&gt; 3.0'

with:

&gt; gem 'puma', '~&gt; 3.6.2'

A quick bundle install and then another rails s -p4000 gives:

    bundle exec rails s -p4000
    =&gt; Booting Puma
    =&gt; Rails 5.0.1 application starting in development on http://localhost:4000
    =&gt; Run `rails server -h` for more startup options
    Puma starting in single mode...
    * Version 3.6.2 (ruby 2.3.1-p112), codename: Sleepy Sunday Serenity
    * Min threads: 5, max threads: 5
    * Environment: development
    * Listening on tcp://localhost:4000

As with all http testing, a curl -I is always handy for figuring out if things are working:

    curl -I http://localhost:4000/
    HTTP/1.1 200 OK
    X-Frame-Options: SAMEORIGIN
    X-XSS-Protection: 1; mode=block
    X-Content-Type-Options: nosniff
    Content-Type: text/html; charset=utf-8
    ETag: W/&quot;c06ad854cb732d5ed93f13e0e4a601ac&quot;
    Cache-Control: max-age=0, private, must-revalidate
    X-Request-Id: 85a07f99-f8b3-4692-aaa5-4f7c8327d901
    X-Runtime: 0.032369</description>
        <pubDate>Wed, 05 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/04/05/rails-tip-when-puma-doesn-t-run-on-the-specified-port.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/04/05/rails-tip-when-puma-doesn-t-run-on-the-specified-port.html</guid>
        
        <category>rails</category>
        
        <category>puma</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Setting Up a Router LeveL VPN Take 2</title>
        <description>Three days ago [I wrote up how to configure a Router level vpn](http://fuzzyblog.io/blog/vpn/2017/03/29/setting-up-a-router-level-vpn-to-secure-your-browsing.html) to secure your Internet browsing so your ISP can't know where you're visiting and sell your Internet history.  That tutorial is still very relevant but the commercial VPN provider I used, [StrongVPN](http://www.strongvpn.com/), prevented me from using Netflix.  My initial plan had been to turn the VPN on / off at the router level when we wanted to use Netflix but my kids apparently use Netflix very regularly when I'm at work so that's kind of a non starter.  This meant that I needed a different VPN solution -- one that I ran myself and not a commercial provider.

The reason that Netflix blocks VPN access is that they don't want customers in other countries buying the US version of Netflix to get around country specific content licensing.  And since any VPN provider is an ongoing business with a persistent set of IP ranges, Netflix can easily block any centralized VPN provider just by noting &quot;Ok - StrongVPN is using xxx.yyy.vvv.zzz&quot; so using any commercial provider is always going to play cat and mouse.  The one I used, StrongVPN, had a history of being Netflix compliant but that's apparently no longer the case.  

The approach that I took is one that's currently popular among networking folk that I know online:

* Take a cheap cloud computing instance like an AWS t2.micro or a Digital Ocean droplet
* Configure an open source VPN tool on top of it
* Use that for all Internet access including router level access like I linked to above

There are a number of these solutions now available and they generally operate using a provisioning engine like Ansible to spin up the cloud instance, install the VPN and create the needed infrastructure.  Here are the ones I looked at:

* Hacker News Discussion Threads on this Topic: [AutoVPN](https://news.ycombinator.com/item?id=13249523), [Algo](https://news.ycombinator.com/item?id=13998493), [Streisand](https://news.ycombinator.com/item?id=13996417)
* **Recommended** [Algo](http://github.com/trailofbits/algo) | [Blog](http://blog.trailofbits.com/2016/12/12/meet-algo-the-vpn-that-works/)
* **Recommended** [openvpn-install](http://github.com/Nyr/openvpn-install)
* [Streisand](https://github.com/jlund/streisand)
* [Pritunl](http://pritunl.com)
* [Tinc](http://www.tinc-vpn.org)

Of the options above I tried three of them: Streisand, Algo and openvpn-install.  Streisand was very promising but it failed to install completely on either Google Cloud or AWS.  Worse it left a dangling cloud instance on AWS meaning that had I not been watchful, I'd have been paying for that until I noticed.  Algo did far, far better on installation but its post install documentation left me confused enough that I had to file a [Github issue](https://github.com/trailofbits/algo/issues/320) to find out what to do next (which someone answered almost immediately -- thank you!).

Of the options above I ended up using Algo and openvpn-install.  This is likely confusing and, to a VPN expert, actually stupid but here's why:

* Algo handled provisioning the AWS instance and securing it perfectly.  It also generated VPN configurations specific to iOS devices which is useful since we're an Apple household.  
* Algo is a deliberately cut down VPN system focused solely on the IKEv2 spec.  Sadly while this works client side on OSX devices, my router only supports OpenVPN, PPTP and L2TP vpn protocols so I can't use Algo at the router level.  Please be aware that this is a feature not a limitation -- the real solution is for my router vendor, Asus, to support IKEv2.
* Once I had an AWS instance running, there is nothing stopping you from running openvpn-install directly on that AWS instance so you have an OpenVPN protocol that works on a router that only supports OpenVPN -- and that's what I did.
* And since this VPN is now running on my own box, not a commercial provider, it actually does support Netflix which makes my kids happy.
* This should end up being quite a bit cheaper than a commercial provider since a t2.micro not is pretty minimal but untiL i see a bill I won't really know.

# Process

Here's what to do.  Please bear in mind that this requires a full Ansible dev stack complete with Boto so if you're not a Linux or OSX person who's fairly technical, well, you're likely out of luck.

1.  Start by going to [Algo](http://github.com/trailofbits/algo) and downloading it per the instructions.
2.  Run the Algo setup process and answer all the questions.  You likely want to pick a data center for your VPN closest to you for best performance.  It will take a fair bit (roughly 20 minutes to run).  At the end you'll get a nicely formatted although mildly unclear set of instructions.  If you don't want to use openvpn-install also then you can stop here and just configure your local vpn.  Or you can continue to install openvpn-install.
3. Ssh into the algo box:
&gt; ssh -i config/algos.pem ubuntu@xxx.yyy.vvv.zzz
4. Become root:
&gt; sudo su - 
5.  Run the openvpn-install process: 
&gt; wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh
6. At the end your OpenVPN settings will be in **/root/client.ovpn**   Copy that file locally and then use it to configure a VPN connection at your router or client side.  See my [earlier tutorial](http://fuzzyblog.io/blog/vpn/2017/03/29/setting-up-a-router-level-vpn-to-secure-your-browsing.html) for details on setting up your router level connection.  Either way you should now have a VPN that supports Netflix.  

# Post Installation Steps

After you install, you should goto [whoer.net](https://whoer.net) and check your params.  My local ISP is in Indiana but here's my whoer report once my VPN was running on my router:

![/blog/assets/whoer.png](/blog/assets/whoer.png)

As you can see this reflects the location of my VPN which is located in Ohio.

# Comments

Please bear in mind that this is a sub optimal approach -- I'm running two separate VPN servers now (thought I could shut down the Algo one) but it supports my use case which boiled down to:

* Securing my family's Internet browsing
* Fast and easy installation -- I spent much more time writing these two blog posts than I did dealing with VPN configuration
* Worked router side with a router that didn't support Algo's protocols; ideally I'd have replaced my router with something that ran IKEv2 directly but I didn't feel like that pain right now
* Supported Netflix which made my kids happy
</description>
        <pubDate>Fri, 31 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/vpn/2017/03/31/setting-up-a-router-level-vpn-take-2.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/vpn/2017/03/31/setting-up-a-router-level-vpn-take-2.html</guid>
        
        <category>vpn</category>
        
        <category>privacy</category>
        
        <category>security</category>
        
        <category>router</category>
        
        
        <category>vpn</category>
        
      </item>
    
      <item>
        <title>Setting Up a Router Level VPN to Secure Your Browsing</title>
        <description>Well the fools in Congress really did it -- they made it [legal for any ISP to sell your browsing habits](https://arstechnica.com/tech-policy/2017/03/for-sale-your-private-browsing-history/) - [Hacker News Discussion](https://news.ycombinator.com/item?id=13981184).  If you think this isn't a privacy issue then consider these examples:

* Visiting babycenter.com?  Maybe you're pregnant.
* Visiting cars.com?  Maybe you're buying a new car.
* Visiting celiac.com?  Maybe you have medical issues around gluten.
* Visiting kink.com?  Maybe you're a sexual deviant.  

# How To Setup a Router Side VPN

This is an **abomination** of everyone's privacy and the only even partial answer is to start using a VPN immediately.  Here's how I did it.  I have an Internet connection from a local fiber ISP named [NineStar](https://www.ninestarconnect.com/residential/residential-internet/).  They provided us with an ASUS RT-N66U router.  Here's the step by step illustration of how to do this using this router.  I suspect that most current routers will now support this type of VPN setup but you need to check your router.

1.  Sign up with a VPN provider. This usually costs between $5 and $10 per month.  I used [StrongVPN](http://www.strongvpn.com/) just now but I also looked at [Mullvad](http://www.mullvad.net) and [Private Internet Access](http://www.privateinternetaccess.com/).  All of these are good; I've previously used Mullvad and they kept my browsing safe for years.  One of the reasons I went with StrongVPN was their wide support for mobile devices including [iOS, Google Play and more](/blog/assets/vpn09.png).
2.  Go into your router's control panel and find the VPN option:&lt;br/&gt;
![/blog/assets/vpn01.png](/blog/assets/vpn01a.png)
3.  Select the VPN client option:&lt;br/&gt;
![/blog/assets/vpn02.png](/blog/assets/vpn02b.png)
4.  Set the VPN client parameters:&lt;br/&gt;
![/blog/assets/vpn03.png](/blog/assets/vpn03c.png)
5.  The parameters you really need to set are description (which VPN service you are using), VPN Server, User Name and Password.  You should also turn on Auto-Reconnection.

# Disadvantages

Using a VPN can be a really nerdy thing and there can be disadvantages.

## Turkey Anyone?

For example I was initially assigned to a VPN server that was located in Istanbul, Turkey.  This had the amusing impact of Google thinking that I was Turkish and giving me results from google.tr:

![/blog/assets/google_turkey.png](/blog/assets/vpn04.png)

The solution for me was to log into StrongVPN and then change my VPN server's location to one in Atlanta.  Here's that process:

1.  Login to the [Customer Area](https://intranet.strongvpn.com/services/intranet).
2.  Click on Change Server:
![/blog/assets/vpn05.png](/blog/assets/vpn05.png)
3.  See what servers are available:
![/blog/assets/vpn06.png](/blog/assets/vpn06.png)
3.  Choose a country:
![/blog/assets/vpn07.png](/blog/assets/vpn07.png)
4.  Choose a server:
![/blog/assets/vpn07.png](/blog/assets/vpn07.png)
5. Click the Change Server button:
![/blog/assets/vpn08.png](/blog/assets/vpn08.png)

## Netflix, Sigh

The biggest issue for most people with a VPN will be that it kills Netflix entirely.  If you google for [Netflix VPN](https://www.google.com/search?q=Netflix+VPN&amp;oq=Netflix+VPN&amp;aqs=chrome..69i57j0l5.419j0j7&amp;sourceid=chrome&amp;ie=UTF-8), the 570,000 search results for that simple query should indicate that there's something going on here.  The issue is that VPNs make content access look like it comes from a different location thus allowing a Netflix customer in say Thailand to get the U.S. version of Netflix.  And since this violates all kinds of content licensing, Netflix aggressively prevents the use of a VPN.  StrongVPN is supposed to work with Netflix (that's one of the reasons I chose it) but, right now, at least for me, it seems to be failing.

For right now I'm going to go with the &quot;Turn VPN on / off&quot; when we want to use Netflix.  I'm sure there are better approaches and I'll likely figure one out but for now this will work given that most of our media content is on a local [Plex](http://www.plex.tv) server and not on Netflix.  

I know the Netflix problems absolutely suck but the privacy and security that this provides is absolutely worth it.


</description>
        <pubDate>Wed, 29 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/vpn/2017/03/29/setting-up-a-router-level-vpn-to-secure-your-browsing.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/vpn/2017/03/29/setting-up-a-router-level-vpn-to-secure-your-browsing.html</guid>
        
        <category>vpn</category>
        
        <category>privacy</category>
        
        <category>security</category>
        
        <category>router</category>
        
        
        <category>vpn</category>
        
      </item>
    
      <item>
        <title>Getting Started with Ansible when You Know Literally Nothing</title>
        <description>An old friend, let's call him GC, emailed me earier today about ansible:

&gt; Hi Scott,

&gt; I'm trying out ansible... But I'm a little at a loss as to what I do next.

&gt; Goal: We have been working on an x86 assembly language interpreter available on the web, for student usage.

&gt; We build and test it locally, push to GitHub... but then we have to remember to login to our pythonanywhere servers, do the appropriate pull, and restart the web server.

&gt; This seems like exactly what ansible is for. But I've now installed it locally, and at pythonanywhere, and... well, I'm watching nice videos about the wonderful way ansible can automate this, and I can run ansible-console and see zillions of commands, and...

&gt; Well, I *think* I need a playbook, and I *guess* I can invoke it from my makefile.

&gt; On the remote, I need to:

&gt; Get to the proper directory.

&gt; Run git pull origin (dev or master)

&gt; Restart the web server.

&gt; I also think I need some sort of SSH key.

&gt; Any guidance you can offer on getting going with this would be greatly appreciated!

&gt; gc

Ansible is the best devops tool I've ever used but its yaml based syntax and mixture of different concepts - playbooks, roles, tasks and inventories can make this hard so let's break this down bit by bit and construct a conceptual model for this.

**Note:** If you want a set of introductory slides on Ansible, [here are a set of slides](http://fuzzyblog.io/blog/assets/ansible_basics.pdf) that I gave in Fall 2016 at an Elixir Meetup.

# Definitions

Let's start with some definitions:

* Playbook - A collection of logical tasks that are designed to accomplish a purpose.  Think of a playbook as an executable of limited scope.  Playbooks execute roles.
* Role - A specific thing that you want the playbook to accomplish.  If you had a Playbook that had as its goal to install Ruby then it might have roles of: download source, compile source and then copy the final executable into pace.
* Tasks - A task is a collection of things that a role does.
* Inventory - an inventory is a collection of hosts that you want to execute a playbook on.  This is generally just an ASCII file formatted as a .ini file

# What Actually Is Ansible?

Part of what makes Ansible confusing is that its not an imperative, from start to finish scripting language.  Ansible is instead a *state description system* where what you're doing is describing the state of the system that you want to achieve, as a series of yaml documents, and then Ansible itself reconciles the target system with that state description until it matches.  If you ever did expert systems development then think of Ansible as a problem solving engine and that's actually pretty close to the truth.

# Solving Our Problem

There are at least three ways that we could solve this problem:

* **Local to Remote**

* **Remote to Remote**

* **On Remote**

The difference between these two is **where** Ansible is executing and the reason that's important is that Ansible operates via SSH automation at its core.  This means that the underlying concept of *connection* is key to Ansible.  You can operate Ansible either via a connection from your local machine to a series of remote hosts (your inventory) or you can operate it on a remote host solely (the connection: local approach).  My suspicion is that either approach could be made to work so let's explore both.

## Local to Remote

With a local to remote approach, the ansible code executes initially on your local machine but connects to each of the machines identified in your inventory.  This connection is done through SSH automation.  I know a lot of my readers are people from the Ruby community so perhaps this analogy might help -- if you're at all familiar with [Capistrano](http://capistranorb.com/) then think of this aspect of Ansible as a vastly more generalized Capistrano.

## Remote to Remote

A Remote to Remote approach is actually very similar to local to remote.  The only real difference is that you use a dedicated ansible box on your network to handle running your Ansible code.  This can have some pretty dramatic performance improvements since your remote box is at the same &quot;level&quot; as your ansible code itself.

## On Remote

Another approach would be to run the ansible process solely on the remote machine directly.  I've done less of this but it should certainly be possible.

## Choosing Local to Remote

Local to Remote feels like the cleanest match for this problem given my limited knowledge -- I know only what was in the email printed above (slight readability / formatting changes).  

## Starting with a Make File

GC is an outstanding, low level software developer so his thinking of ansible as something you run directly from the makefile doesn't surprise me at all.  In an earlier life he implemented the HyperAwk programming language for me using Borland's Turbo Pascal (yes I've known him that long).  HyperAwk was a variant on the Awk programming language that read directly from binary word processing files (Word, WordPerfect, Ami Pro, Samna) and then constructed hypertext documents from them.  I miss HyperAwk but I digress.

Let's start with a sample makefile.  Here's one from the [Gnu folks](https://www.gnu.org/software/make/manual/html_node/Simple-Makefile.html):

    edit : main.o kbd.o command.o display.o 

    main.o : main.c defs.h
            cc -c main.c
    kbd.o : kbd.c defs.h command.h
            cc -c kbd.c
    command.o : command.c defs.h command.h
            cc -c command.c
    display.o : display.c defs.h buffer.h
            cc -c display.c
    insert.o : insert.c defs.h buffer.h
            cc -c insert.c
    search.o : search.c defs.h buffer.h
            cc -c search.c
    files.o : files.c defs.h buffer.h command.h
            cc -c files.c
    utils.o : utils.c defs.h
            cc -c utils.c
    clean :
            rm edit main.o kbd.o command.o display.o \
               insert.o search.o files.o utils.o
             
And while GC's makefile is certainly different, conceptually it should be much the same - a series of steps which result in the production of a binary file.  In GC's case that binary is checked into a version control system and that's what our Ansible playbook will operate on.

What we're going to need to do is invoke our Ansible playbook at the end of the clean: stage with a line something like this:

    ansible-playbook -i /Users/sjohnson/me/fuzzygroup/ansible/gc01/inventories/hosts /Users/sjohnson/me/fuzzygroup/ansible/gc01/playbook_deployer.yml

As I don't know anything about the paths in GC's build system, I'm specifying absolute paths to everything but there's nothing stopping GC from embedding his ansible code into the build process itself -- that's a choice that he could certainly make.  Obviously he would change the /sjohnson/ to something on his local system.

## The Inventory File

The file /Users/sjohnson/me/fuzzygroup/ansible/gc01/inventories/hosts is just an ASCII file that might look something like this:

    [production]
    ansible_ssh_host=ec2-99-39-100-178.us-east-1.compute.amazonaws.com        ansible_ssh_private_key_file=/Users/sjohnson/.ssh/aws.pem
    
If we wanted this to execute on say 10 different hosts then we'd simply enumerate all 10 hosts here.  And if our local machine's SSH keys are setup to reach the remote box directly then we wouldn't even have to specify a .pem file.  The example above shows how I generally configure ansible to talk to AWS since that's a very, very common use case.

## The Roles We Need

Here are the roles that we're likely to need:

* git_pull
* web_server_restart

Ansible like a lot of open source tools can be used in many different ways.  I've looked at a lot of Ansible code on the web and I've written probably close to 10K lines of Ansible by now and I have a pretty opinionated approach to Ansible development by this point.  I treat Ansible code just as if it was &quot;real&quot; source code and I use a very disciplined approach to the files and structure of an ansible &quot;application&quot; -- don't kid yourself, Ansible is a real application development tool even if it doesn't seem like it.  If you have any doubts on Ansible as a development environment, please read [this post of mine](http://fuzzyblog.io/blog/aws/2017/03/06/using-ansible-as-a-development-tool-with-rails.html).

Before we delve too deep into the specific ansible code, let's look at the file structure that makes up what we need to do.  Here is tree's output:

    ~/me/fuzzygroup/ansible/gc01
    20:19 $ tree
    .
    ├── docs
    │   └── readme.txt
    ├── inventories
    │   └── hosts
    └── roles
        ├── git_pull
        │   └── tasks
        │       └── main.yml
        └── web_server_restart
            └── tasks
                └── main.yml

## The Playbook

Here is the playbook that we'll need:

    - hosts: production
      become: yes
      remote_user: ubuntu
      vars:
        - checkout_path: &quot;/home/gc/code&quot;
        - repo: &quot;git://foosball.example.org/path/to/repo.git&quot;
        - version: &quot;release-0.22&quot;
      roles:
        - { role: git_pull, tags: git}
        - { role: web_server_restart, tags: web_server}
        
Each bit of this has the following purpose:

* hosts - identifies a group of hosts in the inventory file. 
* become - tells Ansible to do its operations using sudo 
* remote_user - tells ansible what user to run as on the remote host
* vars - defines a series of variables to be used inside roles
* roles - defines the roles to be called in order

## The Checkout Role

Ansible uses what are called modules to interface with things and, happily, there is a [Git module](https://docs.ansible.com/ansible/git_module.html).  We can use that to handle the checkout and that reduces the complexity down to just a few lines: 

    # https://docs.ansible.com/ansible/git_module.html
    - name: Check out from the Git Repo
      git:
        repo: &quot;{{ repo }}&quot;
        dest: &quot;{{ checkout_path }}&quot;
        version: {{ version }}

## The Web Server Restart

Restarting the web server is a hard to define thing since web servers vary considerably so all I can do here is provide an example based on what I use for a web server -- Apache:

    # https://docs.ansible.com/ansible/shell_module.html
    - name: Restart web server
      shell: 
        cmd: &quot;apache2ctl restart&quot;
        
# Example Repo

All the code referenced above is located in this [git repo](https://github.com/fuzzygroup/ansible_gc).

# Learning More

This is barely touching the surface of Ansible.  Not only did I completely ignore Ansible's idempotent nature but I also skipped a ton of other important things such as AWS automation, the *censored* nature of Ansible Galaxy, python Boto support and more.  Here are some pointers:

* [My Blog on Ansible](http://fuzzyblog.io/blog/category.html#ansible)
* [Jeff Geerling](https://www.jeffgeerling.com/) / [His Github](https://github.com/geerlingguy)
* [Ansible for Devops](https://leanpub.com/ansible-for-devops)
* [Ansible for AWS](https://leanpub.com/ansible-for-aws)

The last two resources are ebooks and they are both excellent.</description>
        <pubDate>Wed, 22 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2017/03/22/getting-started-with-ansible-when-you-know-literally-nothing.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2017/03/22/getting-started-with-ansible-when-you-know-literally-nothing.html</guid>
        
        <category>ansible</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Gluten Warning - Mushrooms May Not Actually Be Gluten Free</title>
        <description>![http://startcooking.com/public/IMG_0474.JPG](http://startcooking.com/public/IMG_0474.JPG)

I don't often write about food issues here -- I have a [recipe blog](http://fuzzyblog.io/recipes/) for that.  But sometimes I find something so egregious that I feel compelled to write something.  My wife suffers, quite badly, from [Celiac disease](https://celiac.org/celiac-disease/understanding-celiac-disease-2/what-is-celiac-disease/).  And, as with any auto-immune condition, it is always a running battle where the symptoms change and you're always in responsive mode.

Lately my wife has been having odd stomach issues and we finally managed to correlate it to mushrooms.  Our whole kitchen is gluten free and the few gluten based ingredients we do have are physically **stored**, and **used**, on a different **floor** of the house.  So we knew it wasn't a normal cross contamination issue.  We also hadn't eaten out in weeks so that was off the table as well.  A bit of googling turned up this [Very Well post](https://www.verywell.com/are-mushrooms-gluten-free-562814):

&gt; Answer: There's no question that plain mushrooms ought to be gluten-free — after all, they're a fresh vegetable, right? But lots of people report getting glutened after eating fresh mushrooms ... enough people that it's worth looking into how mushrooms are grown.

&gt; In fact, once you learn how they're grown, you'll understand why many people react, especially those who are particularly sensitive to trace gluten.

&gt; You see, mushroom spores are grown either directly on gluten grains, or on a medium that's derived at least in part from gluten grains. Rye is used most commonly for this purpose, but growers also use wheat and occasionally on a combination of the two grains (barley, the third gluten grain, doesn't seem to be in common use for growing mushrooms). And this cultivation method leads to gluten cross-contamination on the finished fungi.

It should be noted that the article goes on to say that the presence of gluten in mushrooms is below the 20 ppm standard that defines gluten.  But my wife is admittedly very, very sensitive to ingredients and she can taste the presence of things that I can't so it doesn't surprise me that she has problems with mushrooms.  But, even so -- what have things come to when a vegetable (ok a fungus) -- in its &quot;natural&quot; state -- contains gluten? 

This only emphasizes to me just how **very, very careful** you have to be when you have celiac or any food related issue -- what you think you're buying may not be exactly what you expect.


</description>
        <pubDate>Tue, 21 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/gluten_free/2017/03/21/gluten-warning-mushrooms-may-not-actually-be-gluten-free.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/gluten_free/2017/03/21/gluten-warning-mushrooms-may-not-actually-be-gluten-free.html</guid>
        
        <category>warning</category>
        
        <category>gluten_free</category>
        
        <category>food</category>
        
        
        <category>gluten_free</category>
        
      </item>
    
      <item>
        <title>Getting Around OSX Bash Fork Issues</title>
        <description>If you've ever seen this error under OSX:

    -bash: fork: Resource temporarily unavailable
    -bash: fork: Resource temporarily unavailable
    -bash: fork: Resource temporarily unavailable
    -bash: fork: Resource temporarily unavailable
    -bash: fork: Resource temporarily unavailable
    -bash: fork: Resource temporarily unavailable
    -bash: cannot make pipe for command substitution: Too many open files
    
then you know that it usually signals &quot;Oh crap - restart the box&quot;.  If you're a Rails developer then you may be hitting this due to problems with the new [Puma development server and the evented listener](https://github.com/puma/puma-dev/issues/56).  

The solution is to change the listener in config/environments/development.rb from:

&gt; config.file_watcher = ActiveSupport::EventedFileUpdateChecker

to:

&gt; config.file_watcher = ActiveSupport::FileUpdateChecker

You might also want to kill the listen gem as well (read the linked issue above).</description>
        <pubDate>Mon, 20 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/03/20/getting-around-osx-bash-fork-issues.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/03/20/getting-around-osx-bash-fork-issues.html</guid>
        
        <category>osx</category>
        
        <category>bash</category>
        
        <category>rails</category>
        
        <category>fork</category>
        
        <category>mac</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Disabling Spring in Rails</title>
        <description>I now have more than a decade at building Rails apps and, since most of that time was a consultant, I think its fair to say that I've seen, worked on and built from scratch a lot of Rails applications.  My single least favorite part of Rails is the monolith application pattern.  While it can be incredibly productive to have every aspect of your source code in a single place, monoliths are an inherent anti-pattern:

* They are brittle
* They are fragile
* Gems can conflict with each other
* Ruby is a dynamic, late bound, garbage collected language -- the more lines of code you have the more likely you are to have some kind of conflict

Monoliths seem to work great for building BaseCamp but I am immensely unhappy with them and I've spent the past several months going down a service oriented architecture (SOA) approach for my side project.  

Spring is an application pre-loader which loads a chunk of your Rails code into memory in order to speed up development.  By having things preloaded, for example, you can get into rails console faster, or run tests faster, etc.  And this works fine when you have a monolith application pattern.  In my case, however, I'm partitioning my data at the user level into individual databases and using an environment variable to specify the database at load time with this trick in database.yml:

&gt; database: cartazzi_web_app_&lt;%= ENV['CARTAZZI_USER_ID'] %&gt;_development

And this works perfectly -- until Spring rears its ugly head.  Since Spring has things cached, you can go into rails console and execute a User.first and find the wrong user.  It becomes even more confusing when you check the database with:

&gt; Rails.application.config.database_configuration[Rails.env][&quot;database&quot;]

And it reports back a database that you **know** isn't the one where the user you just found doesn't exist.  What's going on is that the Rails console is talking to its in memory configuration which reports back something from earlier, not your current configuration.  And then you scratch your head for quite a bit until you cobble together enough brain cells to recall &quot;oh crap -- SPRING!&quot;.

The solution is to disable Spring entirely.  Here's how:

1.  At the shell level, do this: **pkill -f spring** 
2.  Edit your Gemfile and comment out all the spring references.  
3.  At the shell level, do this: **bin/spring binstub --remove --all**  [Stack Overflow Reference](http://stackoverflow.com/questions/30302021/rails-runner-without-spring).

There are other ways around this including an environment variable which temporarily disables it but given the number of spring related processes I keep having to kill, I think getting it out entirely is better.</description>
        <pubDate>Mon, 20 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/03/20/disabling-spring-in-rails.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/03/20/disabling-spring-in-rails.html</guid>
        
        <category>rails</category>
        
        <category>monolith</category>
        
        <category>hyde</category>
        
        <category>spring</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Questioning the Nature of Gem'ified JavaScript</title>
        <description>In 2017 you can't be doing web development and not at least be doing some JavaScript.  I bloody well hate JavaScript and even I'm dipping my toe into the JS water.  Way, way, way back in the mid 90s I got very badly burned by JavaScript compatibility issues and I've never looked back but it is time.  So I've been mucking about with d3 and it has been a hellish weekend indeed.  When I learn a new technology I like to start with the simplest possible example so I started with the [Overfitted Rails / D3 example](http://www.overfitted.com/blog/?p=302) because its only about 10 files total.  And this example works perfectly in Rails 4.2.  Kudos to the author.  Now while it works perfectly in Rails 4.2, it entirely fails to render the world's simplest bar graph in [Rails 5](https://github.com/fuzzygroup/rails5_d3_seed) and nothing I do makes it work.  This has the feel of some type of &quot;Rails Magic&quot; that changes from 4 to 5 so something that was implicit is now explicit and Ghu only knows what the *censored* it is.  I've put probably 3+ hours into making this example work and I'm now at the point of wanting to take a hammer to a Macbook -- clearly time to move on.  

The saddest part of all this is that the author of the overfitted example is clearly a bright, bright guy as based on his [Stack Overflow profile](http://stackoverflow.com/users/1583239/qwwqwwq) - top 12%, better than [me](http://stackoverflow.com/users/409644/fuzzygroup) certainly but there's no obvious way to contact him and no comments on his blog so, sigh...

My next stopping point on this d3 quest was [Greg Park's Adding a Live D3 Visualization to Rails](http://gregpark.io/blog/live-d3-rails-plot/).  This is a lot more complex than Overfitted but all the bits are there and [there's a full repo](https://github.com/gregoryjpark/live-d3-example).  So the first step was a quick fork so I could remove the postgres dependency.  An install showed that everything worked in Rails 4.  The moment of truth was to try it in [Rails 5](https://github.com/fuzzygroup/live-d3-example) -- and it worked brilliantly!  Thank you Greg!

I'm not a JS expert in any way -- it would more more accurate to say &quot;I'm a self professed JavaScript hater who feels he has no alternative but to try and use it (finally)&quot; so my first thought was &quot;Why not use the [d3rails gem](https://github.com/iblue/d3-rails)&quot;.  Given that I had this in git, the path was clear:

* take my rails 5 exmaple
* make a branch
* delete the vendor/javascripts/d3.js file
* update gemfile and add d3-rails
* bundle install
* try the app and see if the graphs display

And, what do you know, the result was the same as my [Font Awesome issues recently](http://fuzzyblog.io/blog/css/2017/03/16/when-your-font-awesome-etsy-icon-doesn-t-display.html) -- the graphs didn't display.  This is really making me question gems that wrap JavaScript libraries.  I would be certain that the problem here is that something changed in d3 and d3-rails isn't current enough so something is broken.  But given that I've now hit this twice in the same week, it feels like either a CDN or vendor/assets/javascripts is the right approach for JavaScript inclusion.  Our historical method of wrapping JavaScript into gems for &quot;inclusion&quot; into the rails ecosystem is really feeling broken right now.  Thoughts?</description>
        <pubDate>Sun, 19 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/03/19/questioning-the-nature-of-gem-ified-javascript.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/03/19/questioning-the-nature-of-gem-ified-javascript.html</guid>
        
        <category>rails</category>
        
        <category>javascript</category>
        
        <category>d3</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>When Your Font Awesome Etsy Icon Doesn't Display</title>
        <description>I'm a fan of [Font Awesome](http://fontawesome.io/) and I think enough of it that it was actually the **first** bit of software I purchased for use on my side project.  

I recently had the situation where most of my Font Awesome icons would show up but a few wouldn't.  I knew it wasn't a layout problem because the code was all machine generated and the Amazon icon was showing up just a few lines earlier.  An example of an icon which wouldn't show up is the Etsy icon.

As with almost all my crazy CSS / Bootstrap issues, I reached out to my buddy [Dv](http://dasari.me/) and a quick pairing session set it right.  The first solution we found was to add the following snippet to custom.scss in /app/assets/stylesheets:

    .fa-etsy:before {
      content: &quot;\0045&quot;
      font-family: georgia, serif
    }

And that fixed the Etsy icon -- but it was really a work around and the credit goes to [Hayley.cc](https://hayley.cc/2016/10/05/Font-Awesome-No-Etsy-icon-No-problem/) (brilliant but still a work around).  But only fixing the Etsy icon clearly wasn't enough.

Dv did some digging and came up with the thesis that the [font-awesome-rails gem](https://github.com/bokmann/font-awesome-rails) wasn't up to date and suggested I replace it with the CDN.  So by adding this:

    &lt;link href=&quot;https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css&quot; rel=&quot;stylesheet&quot; integrity=&quot;sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN&quot; crossorigin=&quot;anonymous&quot;&gt;
    
The problem was entirely fixed.  Given the problems with CloudBleed, a month ago, I'm mildly paranoid with a CDN but this did simply and brilliantly fix the problem.

Thank you Dv!</description>
        <pubDate>Thu, 16 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/css/2017/03/16/when-your-font-awesome-etsy-icon-doesn-t-display.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/css/2017/03/16/when-your-font-awesome-etsy-icon-doesn-t-display.html</guid>
        
        <category>css</category>
        
        <category>fontawesome</category>
        
        
        <category>css</category>
        
      </item>
    
      <item>
        <title>Developer Employee Transitions When You're an AWS Shop</title>
        <description>So I find myself advising a former employer on how to lock out an employee with fairly pervasive access (and, yes, I am the employee).  Here was my advice:

1.  The safest option would be to move to white listing **all** ip addresses needed for SSH login.  That would be an absolute ban on any incoming SSH logins from the old employee and, while inconvenient, is a wonderful means to lock down a system.
2.  [Re-issue a new SSH pem file](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair).
3.  Delete the old SSH pem file on your machine and then verify login to at least 2 systems with the new PEM file.  I'd make sure to verify access to the primary db server using the new PEM file but that's just me.
4.  [De-authorize the old PEM file](https://stackoverflow.com/questions/31912037/how-to-delete-deactivate-the-current-pem-file-and-create-new-one-in-aws).
5.  [Delete the API keys that the employee had access to](https://docs.aws.amazon.com/cli/latest/reference/apigateway/delete-api-key.html).  Make sure that you delete them in all regions where they were valid.
</description>
        <pubDate>Tue, 14 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/03/14/developer-employee-transitions-when-you-re-an-aws-shop.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/03/14/developer-employee-transitions-when-you-re-an-aws-shop.html</guid>
        
        <category>aws</category>
        
        <category>management</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Ansible Error with AWS, AMI Creation and Encrypted</title>
        <description>Here is a bizarre Ansible / AWS error I just found:

    An exception occurred during task execution. To see the full traceback, use -vvv. The error was: TypeError: __init__() got an unexpected keyword argument 'encrypted'
    fatal: [localhost]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;module_stderr&quot;: &quot;Traceback (most recent call last):\n  File \&quot;/tmp/ansible_prCPvG/ansible_module_ec2_ami.py\&quot;, line 560, in &lt;module&gt;\n    main()\n  File \&quot;/tmp/ansible_prCPvG/ansible_module_ec2_ami.py\&quot;, line 552, in main\n    create_image(module, ec2)\n  File \&quot;/tmp/ansible_prCPvG/ansible_module_ec2_ami.py\&quot;, line 381, in create_image\n    bd = BlockDeviceType(**device)\nTypeError: __init__() got an unexpected keyword argument 'encrypted'\n&quot;, &quot;module_stdout&quot;: &quot;&quot;, &quot;msg&quot;: &quot;MODULE FAILURE&quot;}
    
A quick google revealed [this solution](https://github.com/ansible/ansible-modules-core/issues/1773):

&gt; pip install boto --upgrade

which I had to execute as:

&gt; sudo pip install boto --upgrade</description>
        <pubDate>Tue, 14 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/03/14/ansible-error-with-aws-ami-creation-and-encrypted.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/03/14/ansible-error-with-aws-ami-creation-and-encrypted.html</guid>
        
        <category>aws</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Dropbox for the Software Developer</title>
        <description>An old friend asked me a great question recently:

&gt; I saw you mentioning your use of Dropbox the other day. I’ve been using GitHub as my “cloud device”: when would I find Dropbox a better option?

One of the single best decisions that I made in 2016 was to adopt [Dropbox](http://www.dropbox.com/) as the key part of my personal computing environment.  I last talked about this back on [November](http://fuzzyblog.io/blog/postmac/2016/11/05/life-in-a-postmac-world.html), see the section &quot;Conclusion - A Synchronized Computing Environment For Me&quot; but it really does bear further description.

My use for Dropbox is exactly what it is designed for -- synchronization.  I've always had multiple machines but one has always been my &quot;primary&quot;&quot; machine.  Since all my code was on Github, I could generally get to my code from any machine but what about my spreadsheets?  Or my documents?  Or just about any other file that I needed.  Here's what I did:

1.  Signup for a 1 Terabyte Dropbox account (roughly $50 / year so dirt cheap).
2.  Install Dropbox on all my machines including my ChromeBook, my Linux box, my iPhone and iPad.
3.  Symlink ~/Dropbox to ~/me so I have an easier path to type.
4.  Move all the files that matter to me into ~/me under a logical file structure related to my usage context - ~/me/blogging for all my blogs, ~/me/Documents for Documents, ~/me/consulting for consulting work, ~/me/fuzzygroup for anything I do online and so on.
5.  Move important things like SSH keys under ~/me and then use symlinks to insert them back into their correct location.
6.  Create a ~/me/transfer directory to copy transitory data from one machine to anther.  
7.  Move all the git repos that matter underneath ~/me.

My working set of data for Dropbox is about 40 gigabytes and I suspect that could be made smaller but I haven't seen the need.  This approach has meant that literally no matter what computing device I am on, I always have access to the data that actually matters to me.  True I don't have, for example, my iTunes music synchronized or my Photos but in terms of the data that I, as a software developer, actually rely on, this is about as perfect an environment as I could dream up.  The only objection I have is that on one of my machines, oddly the most powerful, Dropbox refuses to fully sync, always stalling out on two files that never complete.  Since there are no user facing logs on Dropbox for OSX, well, I have no idea what they are.  And, worse, it spikes my CPU past 100% always running up the fan and down the battery life.  But, other than that, I'm ridiculously happy with this as an approach.

I know that this was supposed to be what Apple was going to give us with iCloud but iCloud:

* Doesn't seem to work particularly well
* Removes local file storage of documents into the cloud solely thus absolutely screwing you over if you don't have internet access
* Has no cross platform support -- I really wanted all my data on my ChromeBook and my Linux box 

It is astonishing to me that the cross machine synchronization environment that I've been searching for most of my professional life is nothing more than a simple Dropbox subscription but that's all it took.

I still do full OS level backups periodically but I don't generally worry about it terribly since the data that matters is always safe being mirrored across all my computing devices and into the Dropbox cloud.
</description>
        <pubDate>Mon, 13 Mar 2017 00:00:00 -0400</pubDate>
        <link>http://fuzzyblog.io/blog/dropbox/2017/03/13/dropbox-for-the-software-developer.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/dropbox/2017/03/13/dropbox-for-the-software-developer.html</guid>
        
        <category>dropbox</category>
        
        
        <category>dropbox</category>
        
      </item>
    
      <item>
        <title>Leveling Up as a Developer Part 2 - Interstitial Time</title>
        <description>I recently wrote about [Leveling Up as a Developer](http://fuzzyblog.io/blog/software_engineering/2017/02/24/leveling-up-as-a-developer.html).  Leveraging Interstitial Time is another approach to improve your skills.  As a developer there are at least two very different types of learning that you have to do:

* **Deep, Focused Learning**.  The classic example here is the syntax of a language.  Some of us can do that by simply reading a book but most of us need the focused, &quot;muscle memory&quot; effort of actually doing it.  
* **Awareness Learning**.  A huge part of the IT or Information Technology space is simply being aware that something exists.  Whether it is a tool, a particular library, an algorithm or merely a way to approach a problem, awareness is often all you need.

We all know how to do deep focused learning -- you sit, generally alone, in a quiet space with a computer and lots of time and you work at it.  Some of us have a set of problems that we like to solve to teach ourselves.  Others focus on working through tutorials.  And there are certainly other approaches.

But how do you tackle *awareness learning*?  Given how vast the world of IT is in 2017, how can you possibly be aware of everything you need to know to be an effective software engineer?

The remainder of this post talks about awareness learning and my approach to it.

# Understanding Interstitial Time

Just as an interstitial ad is the ad that shows up between page transitions, I view interstitial time as the *time between times*.  A good example of this came up last night -- I had to peel two pounds of shrimp for dinner.  This was easily 30 minutes of time when my hands were occupied and I really couldn't do much.  Given that my hands were nasty dirty, I couldn't even turn the pages of a book while I did it.  To me that's interstitial time -- normally it would be entirely wasted.  What I do, however, is turn interstitial time into awareness learning because while I can't do much, I can **listen** -- and that's at the heart of awareness learning.

# Required Items

These are the things you need:

![smart_phone](/blog/assets/smart_phone.jpg)

Some type of smart phone; doesn't have to be iOS, android is just fine.

![bluetooth_headset.jpg](/blog/assets/bluetooth_headset.jpg)

A bluetooth headset.  Personally I favor the [Plantronics M180](https://www.amazon.com/Plantronics-Universal-Cancelling-Wireless-Bluetooth/dp/B010XDJTWS) but any one will work.  This one has good volume, easy pairing and a 7 hour battery.

[![icatcher.jpg](/blog/assets/icatcher.jpg)](https://itunes.apple.com/us/app/icatcher-podcast-player/id414419105?mt=8)

You need some kind of podcasting listening tool.  Personally I Use [iCatcher](https://itunes.apple.com/us/app/icatcher-podcast-player/id414419105?mt=8) on iOS but [OverCast](https://itunes.apple.com/us/app/overcast-podcast-player/id888422857?mt=8) by Marco Arment is also excellent and I recently bought [Castro](https://itunes.apple.com/app/apple-store/id1080840241?mt=8&amp;ign-mpt=uo%3D4) but haven't used it as well.  There are great ones on Android as well but none that I have personally used.

![podcast_feed.jpg](/blog/assets/podcast_feed.jpg)

The most important thing is some podcast feed that's going to download new things to learn right into your phone on an ongoing basis.  Thankfully, in 2017, that essentially means any podcast on the planet -- since they pretty much all have an RSS feed.

From top to bottom this is a smart phone, a bluetooth headset and a podcast feed or podcast feeds of things that for your type of software engineering you should be aware.  I'm a generalist with a focus on web and back end so I find [Software Engineering Daily](https://softwareengineeringdaily.com/) to be outstanding.  But here are some more great learning sources by category:

* DevOps - [Datanaughts](http://packetpushers.net/datanauts-podcast/)
* Big Data - [Drill to Detail](https://www.drilltodetail.com/) 
* Online Marketing - [Noah Kagan Presents](http://okdork.com/podcast/)
* Machine Learning - [Talking Machines](http://www.thetalkingmachines.com/)
* Data Science - [Data Skeptic](https://dataskeptic.com/podcast)
* JavaScript - [JavaScript Jabber](https://devchat.tv/js-jabber)
* Ruby - [5by5](http://5by5.tv/rubyonrails)
* Elixir - [Elixir Fountain](https://soundcloud.com/elixirfountain)

I think you can see that there is a podcast for almost any time of IT learning.  For example I just googled for Embedded System podcast and found [Embedded.fm](http://embedded.fm/) which is 189 episodes long on Embedded Systems and is currently covering Soft Robotics.

Just google for what you need to learn about and then &quot;podcast&quot; and you're very likely to find something.

# Pulling Off Awareness Learning On an Ongoing Basis

The key to this is that you need to develop a **habit** around applying interstitial time to learning.  Here's all you need to do:

* Get the things above
* Keep your headset and phone regularly charged
* Whenever you've got at least 5 minutes doing something crappy -- commuting, dishes, peeling shrimp, raking leaves, building your kids tree house, widening your driveway with a pickaxe, sitting with your kids while the fall asleep -- it can be anything, you **listen** and **learn**.  And, yes, I did build a tree house while listening to Ruby podcasts and I widened a driveway doing the same thing.
* Keep at it!

The last one, Keeping at it, is the hard part.  If you work in IT then your days tend to be long and there are times that your brain just hurts and you don't want to listen to, well, YMITC (yet more IT crap).  It doesn't matter, this only works if you stick with it.  So you suck it up and build it into a habit where you just keep at it.

And that's pretty much it but let me finish up with a concrete example.

# An Example

I was recently listening to Software Engineering Daily, their podcast on [Heroku AutoScaling](https://softwareengineeringdaily.com/2017/02/28/heroku-autoscaling-with-andrew-gwozdziewycz/) and Andrew Gwozdziewycz gave their algorithm for determining when to auto scale.  He described it this way:

&gt; Time point: 35:40 Autoscaling like I said is a little bit more complicated.  The only data we use for auto scaling is your response time, the p95 response time, the number of requests that you're serving be it errors or successful requests and the number of dynos that you currently have.  And what we do is utilize Little's Law, this thing from queueing theory that basically  says (gives example of bank tellers and queues) - think of the arrival rate as through put and think of the latency as the time a teller takes to make a deposit or withdrawal.  These things are interrelated into this relationship -- Little's Law.  We look at the actual number of bank tellers versus the theoretical number if the latency is zero.  If you're app starts responding greater than that latency then we'll scale it up to get that latency down.  We put that into an exponential weighted moving average and we plot a regression line and the slope of that regression line gives us whether or not its ok to scale up or scale down.  (Time point: 38:50)

**Note:** I may have munged the description a bit but if you start listening at minute 35 and go thru it once or twice, you'll pick it up.

Just having heard that much, I can think of about a dozen different ways to apply that algorithm to my daily work as I'm sure you can.  

And that's why I listen to podcasts so avidly -- trying to come up with something like this on your own is truly hard but just knowning that this exists often gives you the critical first step to solving this type of problem. </description>
        <pubDate>Sun, 12 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/03/12/leveling-up-as-a-developer-part-2-interstitial-time.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/03/12/leveling-up-as-a-developer-part-2-interstitial-time.html</guid>
        
        <category>software_engineering</category>
        
        <category>learning</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>When Sidekiq Makes You Nuts Check Your Types</title>
        <description>Much of what I do at the job I'm in the process of leaving is large scale data processing and it is all done through queues and [Sidekiq](http://www.sidekiq.org).  I generally do it with a method call that looks like this:

&gt; CompileWorker.perform_async(id)

Today though I had to do it and pass in a date to my method call so I had this:

&gt; CompileWorker.perform_async(id, date)

And I got crazy, crazy, crazy results -- my object would get about 10 lines into the call and then ... nothing.  I'd test my method line by line in rails console on production and it would work perfectly.  I even went so far as to isolate it down to one Sidekiq thread and one redis server (on local host no less to make sure that no other sidekiq worker was messing with things).  And nothing made any difference.  Finally I realized that the crucial difference was actually that I was passing in a date.  Here's the docs:

&gt; Don't pass symbols, named parameters or complex Ruby objects (like Date or Time!) as those will not survive the dump/load round trip correctly. [More](https://github.com/mperham/sidekiq/wiki/Best-Practices)

And, sure enough, when I removed the date by treating it as an internal constant, it worked perfectly.  Here's the workaround I came up with:

```ruby
def compile(date=)
  if date.is_a?(String)
    date = Date.parse(date)
  end
end
```

And then I call it as 

```ruby
CompileWorker.perform_async(id, date.to_s)
```

This allows my internal use of the compile method where a date is passed in normally to function without issue but to work correctly when called in an asynchronous manner.

I actually don't have a problem with Sidekiq working in this fashion but it does bother me that while the Sidekiq documentation warns about this, there's no error thrown or alert raised.  Now it might be impossible to pull off an error message but this is a damn easy mistake to make.  I knew about the ActiveRecord object limitation in Sidekiq but didn't have a clue that it extended to a simple date.</description>
        <pubDate>Wed, 08 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/03/08/when-sidekiq-makes-you-nuts-check-your-types.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/03/08/when-sidekiq-makes-you-nuts-check-your-types.html</guid>
        
        <category>rails</category>
        
        <category>sidekiq</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Using Ansible as a Development Tool with Rails and AWS for Large Scale Data Processing Automation</title>
        <description>Man it was a hell of a busy week last week.  Here's an illustration:

    ~/me/appdatallc/ansible-clean [master|✚ 1…1]
    01:34 $ find . -name '*.yml' | xargs wc -l
    ...
    283 total

    ~/me/appdatallc/ansible_monthly [master|✔]
    find . -name '*.yml' | xargs wc -l
    ...
     1066 total

Yep.  Starting last week, from Monday morning to Friday afternoon, working with [Winston](http://winstonkotzan.com/) to teach him [Ansible](https://www.ansible.com/), I authored almost 1,300 lines of Ansible. 

The really interesting part of this was using Ansible as a **development tool** with Rails.  The use case in question was automating production of large scale data processing jobs.  Example what these jobs did is highly proprietary but they basically had a shared architecture like this: 

* Execute on a developer's workstation
* Verify state of the application by calling an API server side
* Change the thread count as needed for the right amount of concurrency
* Clear the rails log
* Set the right redis server to isolate sidekiq from other concurrently executing jobs
* Clear the Sidekiq log
* Make an AMI of the EC2 instance
* Launch N instances of the AMI to do the needed data processing
* Fill the Sidekiq queue
* Display the count of items in the Sidekiq queue

There were a series of 8 different data processing jobs, 6 of which matched the above list and two of which were slightly different.  Each of the stages above was represented by a small Ansible playbook and the coordination between each of the playbooks was handled by a bash script which called each of the stages in succession.  

Classically Ansible is a devops tool for provisioning boxes but last week really illustrated to me the power of **Ansible as a development tool**.  Ansible's idempotent, state based approach of modeling the world as a succession of yaml files can definitely be funky but the model works.  

# Adding Status Tracking

Early Friday I realized that once these jobs were running, the developer running them would need to understand the status of the job on a highly discrete level.  Historically I've done this by directly querying the database and just *understanding* the objects involved and the tables that represent them.  But that comes from a huge amount of internal domain knowlege that Winston didn't have.  

This status would need to include:

* Amount of data left in the sidekiq queue 
* Amount of records produced for each job 

Sure in an idealized world this would be a pretty, graphical dashboard available on the web to all people in the company.  Practically speaking, the following is sufficient:

TASK [run_rake_task_and_show_output : debug] *****************************
ok: [monthly-categorization] =&gt; {
    &quot;msg&quot;: [
        &quot;in LoadError rescue statement&quot;,
        &quot;Queue Size = 0&quot;,
        &quot;Total categorization records = 271408&quot;,
        &quot;Total distinct entities in categorization = 6783&quot;
    ]
}

The ability to run this at any point in the process is hugely useful and here's an example of how simple that code can be.  There are three key pieces:

* Ansible Playbook - a playbook which defined a Rake namespace and task to be executed
* Ansible Role - essentially a function which executes the Rake namespace and task, capturing its output and displaying it
* Rake Task - the business logic of what to analyze

## Ansible Playbook

```ansible      
    - hosts: monthly-categorization
      become: yes
      remote_user: ubuntu
      vars:
        - rake_task: &quot;monthly:echo_stats&quot;
      roles:
        - { role: run_rake_task_and_show_output }
```        
    
## Ansible Role

```ansible    
    - name: run a rake task on the target box
      shell: 
        chdir: /var/www/apps/rails_app/current
        cmd:  RAILS_ENV=production bundle exec rake {{ rake_task }}
      register: result

    - debug:
        msg: &quot;{{ result.stdout.split('\n') }}&quot;
```        
        
My first pass on all this had the output being listed as a jumbled mess (typical to captured output by Ansible).  Winston correctly pointed out something to the effect of &quot;Looks a bit like arse&quot;.  Well a quick google led to this [StackOverflow post](http://stackoverflow.com/questions/34188167/ansible-print-message-debug-msg-line1-n-var2-n-line3-with-var3) where this technique:

&gt; msg: &quot;{{ msg.split('\n') }}&quot;

could be applied.  In our case we had it as result so it was just a matter of replacing msg with result.stdout.  And, almost magically, that jumbled mess came into razor sharp focus.  My thanks to Winston for recognizing that this was an issue.  I was so close to the problem that I didn't even perceive it.
    
## Rake task being called by Playbook:

```ruby    
    task :echo_stats =&gt; :environment do
      j = Job.current
      data_shard = ShardedData.shard_for_date(&quot;service&quot;, j.date)
      distinct_entities = data_shard.select(&quot;entity_id&quot;).distinct.count
    
      puts &quot;Queue Size = #{Sidekiq::Queue.new.size}&quot;
      puts &quot;Total categorization records from #{j.current.id} = #{data_shard.count}&quot;    
      puts &quot;Total distinct entity_ids in categorization from #{j.current.id} = #{distinct_entities}&quot;    
    end
```
    
A more complex example of this based on Ruby meta-programming is shown below:

```ruby
    task :echo_other_stats =&gt; :environment do
      tables = []
      tables &lt;&lt; &quot;rating_estimates&quot;
      tables &lt;&lt; &quot;type_estimates&quot;
      tables &lt;&lt; &quot;category_estimates&quot;
    
      stats = {}
    
      tables.each do |table|
        stats[table] = table.classify.constantize.where(:job_id =&gt; Job.current.id).count
      end
    
      stats.each do |k,v|
        puts &quot;Table: #{k} = #{v} for Processing Run #{ProcessingRun.current.id}&quot;
      end
    end
```

    
This approach was based on some internal analysis logic where we had an array of table names that we used for generating some SQL code dynamically.  It took about 5 minutes to convert that list of tables into this.  The table.classify.constantize call takes the name of the table and first converts it to a model name (classify) and the converts that model name to a constant that represents the class itself.  Once you have a class that inherits from ActiveRecord you can then call a .where statement to get a count.  Finally you inject the original table name and the count back into a hash to store the results.

# Conclusion

I've been managing this job production process for almost two years now and most of my approaches to making it better have evolved around different rake tasks and some fairly bad internal documentation.  By pulling Ansible into this, for the first time, we actually have a solution which:

* Goes end to end 
* Automates tasks like AMI creation and instance launching historically done with the AWS developer console
* Allows use of N instances to process the data 
* Fully uses our existing code base without change -- I can't emphasize this point enough
* Displays status on the running jobs at any point
* Is broken down into small, modular chunks -- each of our playbooks are maybe 10 lines in total

Ansible isn't how I expected to solve this problem but its approach really, really worked -- keep it in mind.</description>
        <pubDate>Mon, 06 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/03/06/using-ansible-as-a-development-tool-with-rails.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/03/06/using-ansible-as-a-development-tool-with-rails.html</guid>
        
        <category>ansible</category>
        
        <category>aws</category>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>shell</category>
        
        <category>bash</category>
        
        <category>sidekiq</category>
        
        <category>devops</category>
        
        <category>meta_programming</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Things Winston Taught Me - Better Git Shortcuts</title>
        <description>As I discussed in my article on what [Ganesh taught me](http://fuzzyblog.io/blog/learning/2017/03/02/things-ganesh-taught-me.html), working with another engineer is a great way to learn new things.  Last week I also worked with [Winston](http://winstonkotzan.com/) and he showed me some git short cuts you can add to your bash profile.

# How I Use Git

I've tried a number of git UI tools over the years but I keep coming back to the command line.  Here's my work flow:

* git status
* git add some_file
* git commit -m &quot;a commit message&quot;
* git push origin master

I'm pretty much a straight command line guy.  And because I try very hard to avoid rollup commits, this can make the commit process tedious.

# How Winston Uses Git

Winston showed me his process: 

* gs
* ga 
* gc &quot;a commit message&quot;
* gpc 

These abbreviations are a heck of a lot shorter than what I've been typing so that's clearly a win.  My one objection to his approach is that ga adds EVERYTHING.  My preference is atomic commit messages where one commit = the changes to one file so I really would prefer to have two aliases for add:

* ga some_file
* gaa (adds everything)

Here's how Winston originally implemented this:

    alias gundo='git reset --soft HEAD~1'
    function gc() {
      git commit -m &quot;$*&quot;
    }
    alias gcurrentbranch='git rev-parse --abbrev-ref HEAD'
    alias ga='git add .'
    alias gs='git status'
    alias gl='git log --oneline'
    alias gpc='git push origin $(gcurrentbranch)'
    
Here is my version:

    alias gundo='git reset --soft HEAD~1'
    function gc() {
      git commit -m &quot;$*&quot;
    }
    function ga() {
      git add &quot;$*&quot;
    }
    alias gcurrentbranch='git rev-parse --abbrev-ref HEAD'
    alias gaa='git add .'
    alias gs='git status'
    alias gl='git log --oneline'
    alias gpc='git push origin $(gcurrentbranch)'
    function ghelp() {
      echo &quot;ga (add one) gc (commit) gpc (push) gaa (add all) gs (status) gl (log) gcurrentbranch (show branch) gundo (be careful!)&quot;
    }
    
The only changes I made were:

* add a ga function which does a single file add
* change the name of the previous ga, which added everything, to gaa (git add all)
* add a ghelp function to tell me what's available

Just using this for a few days has really streamlined my workflow.  Thanks Winston!</description>
        <pubDate>Fri, 03 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/git/2017/03/03/things-winston-taught-me.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/git/2017/03/03/things-winston-taught-me.html</guid>
        
        <category>git</category>
        
        <category>terminal</category>
        
        
        <category>git</category>
        
      </item>
    
      <item>
        <title>Things Ganesh Taught Me</title>
        <description>I am an engineer and I've worked with other engineers from all over the world.  People can talk about cultural differences all they want but the bottom line is that software engineering is a very binary profession -- either the code compiles or it doesn't.  And this simple truism generally tends to make engineers far more similar than they are different.  One of my favorite things about working with a new engineer is watching their tooling and then saying &quot;Hey - what's that you just did / please show me how&quot;.  And if its any good, I try hard to incorporate it into my daily workflow.  

Last week I had the privilege of pairing with [Ganesh from Imaginea](https://www.linkedin.com/in/itsgg/) all week long and I picked up some good tricks documented below.  Tomorrow or early next week, I'll document what I picked up from [Winston](http://winstonkotzan.com/).

# Keep

This is an interesting command line tool which lets you designate particular command line strings as things to be &quot;kept&quot;:

![keep](https://raw.githubusercontent.com/OrkoHunter/keep/master/data/keep.gif)

Install it with:

&gt; pip install keep

Save a command with:

&gt; keep YOUR_COMMAND

List your saved commands with:

&gt; keep list

[More on Keep](https://github.com/orkohunter/keep)

# ScreenFlick

I was meeting with Ganesh as part of a transition plan so there was a lot of screen recording to capture knowledge.  While I have traditionally used ScreenFlow, he was using [ScreenFlick](http://www.araelium.com/screenflick) which:

* Is only $29
* Works natively in MP4
* Has a cool remote feature so you can use your phone to pause a recording, make some changes and then get back to the recording

I haven't tried this yet so no clue about noise reduction or other key issues but this is *interesting*.

![ScreenFlick](http://d2l5v8ibvnnoh9.cloudfront.net/assets/screenflick/darken-4a4f14e94a7bba4d3a51486e8bed54cb4b48ed56513ee89089cd41de29a3b4d4.png)

[Araelium](http://www.araelium.com/) the company offering ScreenFlick has a number of cool apps.  Definitely worth checking out.  Even I find their [MySQL Query tool](http://www.araelium.com/querious) interesting.

https://github.com/orkohunter/ping-me

# Filmora

I can't fully remember if I found [Filmora](https://filmora.wondershare.com/mac-os-x-el-capitan/) through Ganesh but it showed up on my box at that time so I think so.  I'm still digging into this one.

# Prompt Envy

I didn't get this from Ganesh but looking at his bash prompt and seeing it show git commit status gave me a case of *prompt envy*.  I've had this in and out over the years like 20 different times but it always breaks.  This time around I looked for someone else's version and found it as a brew package:

&gt; brew install bash-git-prompt 

[Source](https://gist.github.com/trey/2722934).  And it is awesome!  I now have a prompt which shows me branch, status and number of files waiting to be committed.

# Color Coded Terminal Login

The best trick that Ganesh showed me was that his ssh logins to remote boxes actually change the background color of the terminal.  He's doing it using AppleScript with the standard OSX Terminal.  Unfortunately I wasn't able to pull off making that work with my preferred terminal client -- iTerm.  But I've wanted this for so many years now that I'm going to keep at it until it works.

# Thanks!

Thank you Ganesh!  Hugely appreciated.</description>
        <pubDate>Thu, 02 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/learning/2017/03/02/things-ganesh-taught-me.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/learning/2017/03/02/things-ganesh-taught-me.html</guid>
        
        <category>terminal</category>
        
        <category>podcasting</category>
        
        <category>screen_recording</category>
        
        <category>keep</category>
        
        
        <category>learning</category>
        
      </item>
    
      <item>
        <title>Increasing Linux Open File Limits</title>
        <description>So [Winston](http://winstonkotzan.com/) found this log message in our logs today:

    2017-03-02T18:18:31.561Z 32313 TID-osjfdq3bg ERROR: !!! ERROR HANDLER THREW AN ERROR !!!
    2017-03-02T18:18:34.696Z 32313 TID-osjfdq3bg ERROR: Too many open files @ rb_sysopen - /proc/meminfo

We were running a large data processing job using sidekiq for running our queue and this was causing the queue to stop being processed.  As the resident *nix nerd, this one was on me.  A quick google on increasing open file limits gave me a tutorial from [Easy Engine](https://easyengine.io/tutorials/linux/increase-open-files-limit/) and the information was good but I think it could be documented better so here is the process I used.

# Step 1: Diagnosis 

Figure out what your limits actually are.  Given that these can be set on a per user basis you want to first diagnose them.  I'm using the user ubuntu so here's the command line you need:

&gt; sudo su - ubuntu -c 'ulimit -aHS' -s '/bin/bash'

Here's the result:

    sudo su - ubuntu -c 'ulimit -aHS' -s '/bin/bash'
    core file size          (blocks, -c) 0
    data seg size           (kbytes, -d) unlimited
    scheduling priority             (-e) 0
    file size               (blocks, -f) unlimited
    pending signals                 (-i) 122314
    max locked memory       (kbytes, -l) 64
    max memory size         (kbytes, -m) unlimited
    open files                      (-n) 1024
    pipe size            (512 bytes, -p) 8
    POSIX message queues     (bytes, -q) 819200
    real-time priority              (-r) 0
    stack size              (kbytes, -s) 8192
    cpu time               (seconds, -t) unlimited
    max user processes              (-u) 122314
    virtual memory          (kbytes, -v) unlimited
    file locks                      (-x) unlimited
    
Clearly a limit of 1024 sounds low.  

# Step 2: Fixing

The configuration file that you need to edit is: /etc/security/limits.conf.  Edit this with:

&gt;  sudo vi /etc/security/limits.conf

Add to the end of this file the following content:

    *         hard    nofile      999999
    *         soft    nofile      999999
    root      hard    nofile      999999
    root      soft    nofile      999999
    
Save the file and exit.

# Step 3: Log Out

For these changes to take effect you need to log out.  Do that now with **exit**.

# Step 4: Login and Re-Diagnose

Log back into the server and run the diagnosis step again:

&gt; sudo su - ubuntu -c 'ulimit -aHS' -s '/bin/bash'

This time you should see something like:

    core file size          (blocks, -c) 0
    data seg size           (kbytes, -d) unlimited
    scheduling priority             (-e) 0
    file size               (blocks, -f) unlimited
    pending signals                 (-i) 122314
    max locked memory       (kbytes, -l) 64
    max memory size         (kbytes, -m) unlimited
    open files                      (-n) 999999
    pipe size            (512 bytes, -p) 8
    POSIX message queues     (bytes, -q) 819200
    real-time priority              (-r) 0
    stack size              (kbytes, -s) 8192
    cpu time               (seconds, -t) unlimited
    max user processes              (-u) 122314
    virtual memory          (kbytes, -v) unlimited
    file locks                      (-x) unlimited
    
    
# Step 5: Ansible

Ideally this should be implemented using Ansible level so that any new boxes have these settings.  Unfortunately that is left as an exercise for the reader.
 




</description>
        <pubDate>Thu, 02 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/03/02/increasing-linux-open-file-limits.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/03/02/increasing-linux-open-file-limits.html</guid>
        
        <category>linux</category>
        
        <category>sys_admin</category>
        
        <category>sidekiq</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>An Engineer's Guide To Having Your Credit Card Stolen</title>
        <description>So the one credit card that my wife and I rely on was recently stolen.  Personally I put the blame on [CloudFlare / CloudBleed](https://www.google.com/search?q=cloudbleed&amp;ie=utf-8&amp;oe=utf-8) since it occurred just after I purchased the HumbleBundle for Arduino (Feb 22nd) and that's:

* the only different credit card transaction done in the past few weeks
* HumbleBundle was affected by CloudBleed

But, that said, I don't know to 100% certainty and this wasn't Humble's fault so I'll still keep purchasing from them.  But I do feel pretty good in thinking that this was due to CloudFlare and I certain, after having reviewed more technical details, that CloudBleed is a lot worse than the [CloudFlare](http://www.cloudflare.com/) company is admitting.  If you haven't yet started the login change process you might want to think about it.  Even I am gearing up to change all my damn passwords.  Sigh.

Now that we just received new credit cards, I thought outlining the process of what to do when you're a developer might be interesting for someone out there.  

1.  If you're not using a digital wallet to manage your secure crap (passwords / credit card data / pins) then I'd highly recommend it.  I've tried a bunch but the one I like best is [EnPass](https://www.enpass.io/) which runs on iOS, OSX, Windows and Unix, uses DropBox for sync from a small company in India.  **Recommended**.
1.  [Github](http://www.github.com/settings/billing).  Github is keys to the kingdom so it is always my first stop.  Drop down the menu next to your profile picture and choose settings and then billing.  Then do the credit card credentials three step jig.
2.  [GoRails](http://gorails.com/users/edit).  [GoRails](http://gorails.com/) is a screencast service about Rails.  I have a legacy account grandfathered in at $9 / month and I don't want to lose it so it was my #2 stop.
3.  [Elixir Sips](http://www.elixirsips.com). ElixirSips is a screen cast service for Elixir and Josh Adams does a great job on it as it was my #3 stop.
4.  [Amazon](http://www.amazon.com/).  How can you not change the big A?  I suspect we all know how to do this by now.
4.  [AWS](https://console.aws.amazon.com/billing/home/#paymentmethods).  Your AWS card can be different from your Amazon card and mine had contact details that were 3 years out of date so make sure to check them.  This is also a great opportunity to shut down any AMIs, instances or snapshots you're not actually using.  You should also check your Cost Explorer while you're at it. I'm finding Route 53 Hosted Zones to be oddly pricey but more on that later...
5.  VPN.  We don't all use VPNs but if you are trying to crawl geographically different information like the Google Play store world wide, a VPN is an awesome testing tool.  Everyone's usage here is different so YMMV.
6.  Apple.  This can easily be done under iOS via Settings / Wallet &amp; Apple Pay.
7.  [DropBox](https://www.dropbox.com/account/profile).  I hate DropBox's performance.  I despise how it runs my CPU at 99% constantly and turns my bloody MacBook Pro fan on but other than my code editor it is probably the one app I just can't live without at this stage.  DropBox keeps all my files up to date across three MacBooks, a Linux desktop, an iPad and an iPhone.  
8.  [BackBlaze](http://www.backblaze.com/).  I actually am no longer a current BackBlaze subscriber but I know lots and lots of us are so if you use BackBlaze or another cloud backup tool I'd update that.
9.  [Meh](http://www.meh.com/account).  If you're reading this and you haven't ever used Meh then you really, really should.  Its an outstanding Nerd resource.
10. [Freshbooks](http://www.freshbooks.com/#/profile).  Since I'm going back to full time freelancing shortly, decent accounting software is called for an Freshbooks is pretty great.  
99. Search your email for the word receipt and look at anything in the last 60 days and you'll likely be ok.

Now, print this out for your wife / significant other so they know where your card has been updated.
</description>
        <pubDate>Thu, 02 Mar 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/misc/2017/03/02/an-engineer-s-guide-to-having-your-credit-card-stolen.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/misc/2017/03/02/an-engineer-s-guide-to-having-your-credit-card-stolen.html</guid>
        
        <category>misc</category>
        
        <category>credit_card</category>
        
        <category>cloudflare</category>
        
        <category>cloudbleed</category>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>Working with the Gem Ecosystem Part 2 - Updating Gems and Writing Generators</title>
        <description>In case you haven't figured out yet that I write stuff here as much for myself as for anyone else, well, its true.  Almost every single day I find myself going back to my own blog as a reference tool.  Today I'm writing a part two to my [Working with the Gem Ecosystem](http://fuzzyblog.io/blog/ruby/2016/08/17/working-with-the-gem-ecosystem.html) post.

My thanks are extended to [Nick](http://www.nickjanetakis.com/blog/) who helped clarify the Gem update process and pointed out the need to embed the branch name in the Gemfile.  He picked this up from his work on the [Orats gem](https://github.com/nickjj/orats) which is very useful if you're into Rails and Docker.

# Generators

I'm a big believer in custom generators and I've always found the process of writing them to be convoluted at best.  I recently found [pattern_generator](https://github.com/sungwoncho/pattern_generator) which makes writing generators drop dead easy.  I'm writing a project now where a major part of the process is writing custom data parsers for all kinds of web sites -- udemy, leanpub, pluralsight, instagram, etc.  These are all PORO (plain old ruby objects) i.e. no ActiveRecord backed and I wanted a custom generator which:

* created my class
* created my template
* filled out the boilerplate structure

Happily pattern_generator is just plain perfect. Here's all I did

* fork the gem
* clone it to my computer
* open it in an editor
* create a directory
* add my template class
* add my template spec
* replace the class name with erb style output tags

And while this worked great, I ran into issues when I started making changes ...

**Note:** The issues I had here are mine and mine alone.  [Sung Won Cho](https://github.com/sungwoncho) did a great job on pattern_generator and I thank him for it.

# And Now Back to Gems

My problem came when I added my fork of pattern_generator to my project's Gemfile and did a bundle install.  At first the gem came down perfectly but I noticed a few bugs.  I then updated my code and re-bundled and *nothing*.  I didn't get any of my changes.  Here's how I had it in my Gemfile initially:

```ruby
gem 'pattern_generator', :git =&gt; 'https://github.com/fuzzygroup/pattern_generator.git'
```

Naturally this was located in a development only section of the Gemfile since we don't want the memory overhead of this in production.

My first thought was that this was tied to a bundle update versus bundle instlal so I did:

&gt; bundle update

And, again, nothing.  Next I tried:

&gt; bundle update --source=https://github.com/fuzzygroup/pattern_generator.git

I would have expected a bundle update pattern_generator to work but I did not find that to be the case.  But even with specifying the source explicitly I still did not get the right version of my code.  Happily Nick and I were about to pair anyway so I asked him and he honed right in on the branch i.e. specify the branch in the Gemfile.  Here's how that looks:

```ruby
gem 'pattern_generator', :git =&gt; 'https://github.com/fuzzygroup/pattern_generator.git', :branch =&gt; &quot;master&quot;
```

And that worked perfectly.  I wrote several new parsers today and then used my [SyncDotRake](https://github.com/fuzzygroup/sync-dot-rake) toolkit to merge the parsing code into a new Service Oriented API.  Overall it was a damn good day.  Thanks Nick!

**Note:** I next updated the gem on the following day and **could not get** it to update at all.  I finally tried:

&gt; bundle update pattern_generator

and that actually seemed to work consistently and reliably.
</description>
        <pubDate>Mon, 27 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/02/27/working-with-the-gem-ecosystem-part-2.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/02/27/working-with-the-gem-ecosystem-part-2.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>generators</category>
        
        <category>pattern_generator</category>
        
        <category>hyde</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Setting Up Rails with Rspec From the Start</title>
        <description>So this morning I was working on my side project and I realized that one aspect of it alone is now 26 plus PORO (plain old ruby objects) models and growing rapidly.  Given that I'm building this along the lines of a service oriented architeture where I want to be able to replace components prototyped in Ruby down the road with Elixir this would make sense so it was time to isolate it into a standalone http service as a separate Rails API stack.

And this brings up the need to generate a stack with RSpec from the start.  Here's how to do this:

&gt; rails new hyde_page_parser -T --skip-active-record --skip-action-cable --skip-spring --api

I want this to **NOT** include: 

* test_unit so -T gets rid of that
* ActiveRecord so --skip-active-refactor
* ActionCable so --skip-action-cable 
* Spring so --skip-spring 

That gets me a baseline project to which I can add RSpec.  Using [Nrakochy's](http://nrakochy.github.io/rspec/rails/2015/05/27/How-To-Setup-Rspec-Instead-Of-Test-Unit-Rails/) instructions, all you need to do is: 

1.  Add gem 'rspec-rails' to a development, test group in Gemfile
2.  Run bundle install
3.  Run bundle exec rails g rspec:install

</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/26/setting-up-rails-with-rspec-from-the-start.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/26/setting-up-rails-with-rspec-from-the-start.html</guid>
        
        <category>rails</category>
        
        <category>rspec</category>
        
        <category>hyde</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Multi Line Comments in Ruby - Finally</title>
        <description>I've wanted multi line comments in Ruby forever and I just, thanks to [this Stack Overflow post](http://stackoverflow.com/questions/2989762/multi-line-comments-in-ruby), found out that they exist thanks to =begin and =end:

```ruby
=begin

A long comment
that spans two
lines

=end
```

Here's a practical example where I'm keeping an example of how to run an instance method on a class in an easy to use copy and paste fashion (copy it and just drop it in Rails console).

```ruby
=begin

url = &quot;https://www.etsy.com/listing/253953555/tacos-tshirt-perfect-for-tacos-lover?ref=finds_l&quot;
parser = Page.new(url)
parser.parse

=end
```

I'm a fan of keeping code snippets around for easy copy and paste so you can get back to stuff quickly and multi-line comments make that so, so much easier.  Sure, I'd prefer /* and */, but, honestly, this will make me happy.  Matz made a choice to not have multi-line comments and while I don't personally agree, long ago, I tied my 
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/02/26/multi-line-comments-in-ruby-finally.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/02/26/multi-line-comments-in-ruby-finally.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Understanding Low Level Index Issues in MySQL and Rails</title>
        <description># The Problem

I had a weird thing recently -- a table with 313 million rows had 30+ second queries on a unique index -- that's way, way too long.  Here's how I went about troubleshooting this. 

A shout out of thanks to [Ganesh](https://github.com/itsgg) and to [Winston](https://github.com/wakproductions) who both helped out.  Notable mention to Ganesh who actually figured out the core issue; I'm really just the scribe here.

# Viewing MySQL Indexes

Any performance problem always starts with an explain on the query:

```BASH
explain select * from line_items where company_id=37 and document_identifier = 'RCON2170' and period = '2008-12-31'\G
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE
        table: line_items
         type: ALL
possible_keys: NULL
          key: NULL
      key_len: NULL
          ref: NULL
         rows: 314459869
        Extra: Using where
```

Given that I **know** that there's an index I found this *puzzling*.  The next step was to use a FORCE INDEX syntax on the query to ensure that this isn't an optimizer issue:

&gt; select * from line_items FORCE INDEX (index_line_items_fin_document_identifier_period) where company_id=37 and document_identifier = 'RCON2170' and period = '2008-12-31'

And that gave the same disappointing performance.  Pity. Using FORCE INDEX would have sucked but it would have been an easy fix at least.  Onward!

This is where Ganesh rose to the challenge and recommended using SHOW INDEXES FROM table_name.  I haven't used that in years and that's likely a bad, bad, bad thing on my part (sorry).  Here's what that gave us:

&gt; SHOW INDEX FROM line_items\G

```BASH

MariaDB [data_production]&gt; SHOW INDEX FROM line_items\G
*************************** 1. row ***************************
        Table: line_items
   Non_unique: 0
     Key_name: PRIMARY
 Seq_in_index: 1
  Column_name: id
    Collation: A
  Cardinality: 313591995
     Sub_part: NULL
       Packed: NULL
         Null:
   Index_type: BTREE
      Comment:
Index_comment:

*************************** 2. row ***************************
        Table: line_items
   Non_unique: 0
     Key_name: index_line_items_fin_document_identifier_period
 Seq_in_index: 1
  Column_name: company_id
    Collation: A
  Cardinality: 200
     Sub_part: NULL
       Packed: NULL
         Null: YES
   Index_type: BTREE
      Comment:
Index_comment:

*************************** 3. row ***************************
        Table: line_items
   Non_unique: 0
     Key_name: index_line_items_fin_document_identifier_period
 Seq_in_index: 2
  Column_name: document_identifier
    Collation: A
  Cardinality: 200
     Sub_part: NULL
       Packed: NULL
         Null: YES
   Index_type: BTREE
      Comment:
Index_comment:

*************************** 4. row ***************************
        Table: line_items
   Non_unique: 0
     Key_name: index_line_items_fin_document_identifier_period
 Seq_in_index: 3
  Column_name: period
    Collation: A
  Cardinality: 200
     Sub_part: NULL
       Packed: NULL
         Null: YES
   Index_type: BTREE
      Comment:
Index_comment:
4 rows in set (0.01 sec)

```

The very, very curious thing is that we see **3 copies** of the index!  And you may notice that the cardinality of the index is incredibly low -- 200 versus the 313591995 cardinality of the primary key index.

[Cardinality](https://www.ibm.com/developerworks/data/library/techarticle/dm-1309cardinal/) is one of those key database concepts that most of us never have to worry about -- essentially it is a measure of uniqueness in the index.  Indices perform better when they are unique and this is way, way, way too low -- particularly for a unique index which incorporates 3 columns.  The actual cardinality here should equal that of the primary key index since this is a unique index.  

# Understanding How Indexes Get Corrupted

This is the kind of thing that should NEVER happen so what went wrong?  Here's where you have to guess a bit since we lack enough history to recreate things exactly.  Here's what I think happened:

1.  We had this running on a 100GB partition where the database was roughly 54 gb and, yes, most of that space was dedicated to this one table.  
2.  An ALTER TABLE (via a Rails migration) started the index creation and then it failed part way through due to a lack of disc space.  
3.  And since it likely took a long time to run the ALTER TABLE, the timestamp for the migration was NEVER inserted into the schema_migrations table (deploy timeout).  
4.  This meant the next time that a deploy happened, the ALTER TABLE was run again.  And again.  And again until the timestamp finally made it into the schema_migrations table (even though technically the index was never fully created).

# Fixing All This

The solution to fixing this was actually pretty simple and had four parts:

1. Run an alter table statement which **deleted** the original bad migration: **ALTER TABLE line_items DROP INDEX index_line_items_fin_document_identifier_period;**.
2. Delete the time stamp for the migration from the table schema_versions: **DELETE FROM schema\_migrations WHERE version=20161115210810;**.
3. Do a full deploy which will cause the migration to run again.  Given that this is a multi hour migration, the chance of your SSH connectivity staying up long enough to complete is slim.  Just know that and accept that.
4. Manually insert the timestamp back into the schema_migrations table: **INSERT INTO schema_migrations (version) VALUES (20161115210810);**.

**Note 1:** A better way to do this, that I just thought of, is to **NOT** do this as a deploy but instead manually scp the migration to the server (in fact it should be in db/migrate it should be there), delete the timestamp from schema_migrations and then run db:migrate directly on the server.  This would avoid step 4 entirely.  

**Note 2:** Very **long running** migrations are generally best run directly on the server without a deploy (in my not so humble opinion).  I used to do this all the time in my AppData hey day but I haven't had to in ages so it took a while to remember it.

**Note 3:** If you don't like doing an scp of a migration file then deploy to one node in your cluster of servers that is NOT a db role.  That prevents the migration from running automatically and then you can log in via ssh and start the migration manually.  Migrations **only** run automatically when you have at least one machine identified as a db role being deployed to.

</description>
        <pubDate>Fri, 24 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/24/understanding-low-level-index-issues-in-mysql.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/24/understanding-low-level-index-issues-in-mysql.html</guid>
        
        <category>mysql</category>
        
        <category>index</category>
        
        <category>performance</category>
        
        <category>rails</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Leveling Up as a Developer</title>
        <description>I have spent the past week handing off about 450,00 lines of ruby / ansible code (as measured by rake stats) that are an interface to about 6 terabytes of MySQL data handled by n AWS instances where n varies between 15 and several hundred to a **single** developer -- who happened to be more junior I was.  He's not a bad guy by any means but, at times, it felt like I had hit him with an intellectual anvil.  We had only one week for the transition.  

Given how hard transitions and taking on a whole new code base can be, I thought I'd write out some recommendations for *Leveling Up as a Developer*.
 
# Its All About Immersion
 
About 9 months ago, I realized that the future of computing lay, imho, in AWS.  After having resisted cloud hosting for years, I finally buckled down and immersed myself in it. During the first six months of that 9 month period, well, I pretty much read constantly about little else but AWS.  There aren't lots of great AWS books from what I can tell so I read blog post after blog post after blog post.  And then when I learned something I'd try it and then [write it down](http://fuzzyblog.io/blog/category.html#aws).
 
# You Need to Write It Down
 
The way I got good at learning things came back in 2002 when I learned to blog.  By writing things down in such a way that I could understand them, I taught them to myself.  Personally my writing preference has always been blogging since it gives something back to the public commons and, if I do write it well, I always, always, always learn something new from the writing process itself.  See Notes 1, 2 and 3 at the bottom of [this post](http://fuzzyblog.io/blog/rails/2017/02/24/understanding-low-level-index-issues-in-mysql.html) as an example.  Writing it down helped me come up with those three important points.
 
# You Need a Side Project
 
I think every single developer out there needs a side project.  It doesn't have to be grandiose in nature or even make money but your day job doesn't always allow you to learn the things that you need.  A side project is **yours**.  Want to play with backbone?  Then write a backbone front end to it.  Need to experiment with Elixir after you spent your own cash on the conference and the training and your boss denied it as too risky?  Just use it on your side project.
 
# You Need to Write Code Every Single Day

I'm a big believer in incrementalism and the fact that you get better bit by bit.  On my current side project I have a commitment to myself that I put in at least one hard core hour per day every single day and I've been sustaining that since December 27th when I got serious about it.  If you want to level up then coding isn't just a 9 to 5 thing; its an every day thing.
 
# Read, Read, Read

You need to read until you're sick of it and then you need to read some more.  This tends to go beyond immersion.  Immersion is when you're picking up something entirely new.  Reading is part of taking software development seriously as your craft.
 
# Realize This Basic Fact
 
I've now been working in software development since '87 and it took me years and years to learn this basic fact: 
 
&gt; Any software engineer increases their skills in direct proportion to the **number** of **different** codebases that they work on.
 
The problem here is that when you're an employee the number of different codebases you're generally exposed to is, well, **one**.  The single best thing I ever did for my engineering skills was to move to being a freelancer.  I can cite which experience, since 2010, the start of my focus on freelancing, taught me what:
 
 * AppData - Large scale data *everything*
 * HowAboutWe - ActiveRecord optimization
 * StreamSend - Alternative ActiveRecord sharding architecures for decreasing hosting costs
 * LiveText - How to take a code base without any tests and grandfather them in without breaking things
 * FI Navigator - Devops
 * Interana - SAML
 * SigStr - MongoDB i.e. the world is not entirely MySQL
 * AddApp - Primary market research and ideation
 * Inside Network - How to write a billing system end to end that scales to millions of dollars and operates with zero downtime over a multiyear period
 * MDDX - Complex mathemetical workflows in Ruby
 * LinkedIn - Low level optimizations for an intensive Ruby workload
 * Trazzler - Geopositioning
 
Had I been stuck in 1 or even maybe 3 different jobs since 2010, I'm absolutely certain that I would never have picked up all these things.
 
# Learning is Multi-Medium These Days

I'm an admitted book nerd:

![book_nerd](/blog/assets/book_nerd.jpg)

You should know that these book shelves *now* go floor to ceiling and this is mostly computer science / high tech industry case studies with a bunch of cook books.  But even I have to admit that there are now very high bandwidth learning tools including:

* YouTube.  Ignore the cat videos and search for conference proceedings.
* Podcasts.  I highly, highly recommend [Software Engineering Daily](https://softwareengineeringdaily.com/).  It is a fantastic learning tool for staying up to date on things.
* Udemy Courses.  I've learned a ton from Udemy but I got the best computer science basics from [this course on Interviewing](https://www.udemy.com/break-away-coding-interviews-1/learn/v4/t/lecture/3948990?start=0).  Also see my buddy Nick's stuff on [Docker](https://www.udemy.com/the-docker-for-devops-course-from-development-to-production/).  Outstanding.
 
# Apply It For Yourself

Seemingly any technology you want to get good at these days there's a way to apply it for your personal use. Let's say for example that you want to learn Ansible.  Here are two personal applications that would teach you quite a bit:

* I now configure my MacBook using [Ansible](https://github.com/fuzzygroup/ansible-macbook-pro) - the link is to my repo for this.
* I know other people who configure their [Plex servers using Ansible](https://github.com/Phunky/ansible-plexmediaserver).</description>
        <pubDate>Fri, 24 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/02/24/leveling-up-as-a-developer.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/02/24/leveling-up-as-a-developer.html</guid>
        
        <category>software_engineering</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Advice on Complex Caching Schemes</title>
        <description>It is reputed to have been said:

&gt; There are only two hard things in Computer Science: cache invalidation and naming things. [Link](https://martinfowler.com/bliki/TwoHardThings.html)

Personally I'd generalize this to:

&gt; There are only two hard things in Computer Science: caching things and naming things. (Me/Today)

Or the JWZ variant:

&gt; I had a performance problem so I implemented a cache.  Now I had two problems.  (Me/Today | [JWZ Variant](http://regex.info/blog/2006-09-15/247))

I recently spoke with a client that wanted to implement a fairly complex scheme for caching the results of API operations.  They had a memcached implementation using [AWS ElastiCache](https://aws.amazon.com/elasticache/) and wanted to extend this to CloudFront.  My advice was simple: 

&gt; Hell No!

Any kind of cache is always **finicky** and **hard to debug**.  The more complexity in your caching architecture, the absolutely harder it is to debug and the less likely it is to work reliably.  I've implemented a lot of caches over the years and I've tried to be trickier and tricker and, generally, when I bothered to track cache hits, the trickier ones never seemed to work quite right.  Part of it may have been logic errors and part of it may been misunderstanding how caches get called in this new crazy multi core world but I think my point stands -- *complex caching schemes are hard to get right*. 

Here was my advice to the client:

1.  Use [memcached](https://memcached.org/) solely (as part of ElastiCache); don't bring in cloudfront.  Memcached has that brilliantly simple, automagical cache invalidation where you a) set when the object is available until allowing it to just disappear *automatically* and b) if enough other objects crowd it out, it is expired using a FIFO architecture.  Those two properties make the cache invalidation issues pretty much disappear.
2.  Take the incoming API call params and a) put them in a hash, b) call hash.to_json to get a stringified version, c) call a SHA1 routine on that stringified version.  That gives you a dirt simple cache key that you can store into memcached.
3.  If their implementation of ElastiCache allows having multiple caches then set up a new cache for this new data and separate it logically from the current cache as this is high volume but low value.
4.  If their implementation of ElastiCache doesn't allow multiple caches then simply increase the size of the current cache.  Your goal here is to ensure that this new cache (which is on low value data) doesn't crowd out the current cache citizens (which are higher value data).
5.  Look into their current implementation of ElastiCache and see if they have any metrics on cache hit ratio and the like.

That's 458 words on caching and I could honestly write a book on it but I think I'll stop here.

**Sidebar 1**: (Not part of the 458 words).  If you never watched Brad Danga's video on Live Journal architecture then I absolutely recommend it unfortunately it seems to have expired from YouTube and the best I can do are the [slides](http://danga.com/words/2005_oscon/oscon-2005.pdf) (my favorite version) / [More Slides](http://danga.com/words/).  Brad is my favorite Internet Architect and if he had a baseball card, well, I'd treasure it.  Recommended.

**Sidebar 2**: (Not part of the 458 words).  If you haven't used ElastiCache then I highly recommend it.  It is truly the simplest way you'll ever setup caching infrastructure.  AWS really knocked it out of the park on this one. 
</description>
        <pubDate>Fri, 24 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/02/24/advice-on-complex-caching-schemes.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/02/24/advice-on-complex-caching-schemes.html</guid>
        
        <category>software_engineering</category>
        
        <category>caching</category>
        
        <category>aws</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Rails Humor The Sound of Crickets when rails g Is Used</title>
        <description>So I'm starting work on a new project that promises to be interesting.  This is pure green field development -- one of my very, very favorite things.  I did a skeleton application generation like this:

```ruby
rails new exploder --skip-active-record --skip-action-cable --skip-spring --api
```

Then I did the normal repository initialization dance: 

    git init
    git add .
    git commit -m &quot;First commit&quot;
    git remote add origin git@github.com:fuzzygroup/exploder.git
    git push -u origin/master
    
No surprises so far.  Time for the first model:

    rails g model TextCommon
    
And I got ... nothing. Hm... Here's app/models:

    ls -l app/models/
    total 0
    drwxr-xr-x@ 3 sjohnson  staff   102B Feb 20 08:32 concerns/
    
*blink*  Oh right -- skip-active-record.  This is going to be purely an API and one that processes a lot of text.  But, in an amazing lack of foresight, apparently there's no such thing as a model generator when you don't have ActiveRecord installed.  Sheesh.  

I'll say this much though -- it did make me laugh out loud when I really, really needed it.  Off to write a poro (plain old ruby objects) model generator so test files get properly set up and such...</description>
        <pubDate>Mon, 20 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/20/rails-humor-the-sound-of-crickets-when-rails-g-is-used.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/20/rails-humor-the-sound-of-crickets-when-rails-g-is-used.html</guid>
        
        <category>rails</category>
        
        <category>humor</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>ASCII Rules; Binary Drools - Markdown Over PowerPoint FTW</title>
        <description>When an engineer leaves an organization there is a vital need to capture knowledge from that engineer.  In short that engineer needs to give a presentation or presentations.  And if they've been there a long time, say 7 years or so, capturing that amount of knowledge is damn near impossible.  Or so I would have thought.  Over the past 48 hours I have authored over 243 slides organized into 22 presentations outlining my knowledge on a project that I am transitioning out of.  Yeah -- the 7 years is me.  

My secret weapon for this was actually MarkDown.  Here's what I did:

* Buy a copy of [DeckSet](http://www.decksetapp.com/) or use [MARP](https://github.com/yhatt/marp)
* Write your slides in a limited subset of markdown where --- delimits a slide and ## is a slide title
* Write in your programming editor
* Present with DeskSet or MARP (MARP doesn't really have a presentation mode but it works kind of ok and is cross platform)
* Use [ScreenFlow](http://www.telestream.net/screenflow/overview.htm) to record your screen
* Pray hard that you have a brilliant friend like [Nick Janetakis](https://nickjanetakis.com/) who knows how to set your audio options correctly so your voice doesn't sound horrible and then thank him profusely for listening to 4 different versions of audio until it was just right while he tells you exactly what you need to adjust.  Or you could just buy his Docker courses(https://nickjanetakis.com/products/); I did and they are awesome.

Seriously - 265 slides in 48 hours.  There is no way I could have done that with PowerPoint.  Being able to work in my code editor and focus on pure content made all the difference.

Note: I used to build corporate enterprise knowledge management systems back in the 90s and sold them to clients like the U.S. Navy, etc.  This, with about $130 of software, was a better knowledge capture solution than what we used to ship for about $250,000 / installation.  My how the world has changed.

Oh and when you are working in a pure ASCII medium you can manipulate your data in bulk like this script (below) which compiled a full table of contents and did a slide count.  Yeah its not elegant but I wrote it in about 10 minutes.

```ruby
directories = Dir.glob(&quot;*&quot;)

slide_ctr = 0
outline_entries = []

directories.each do |directory|
  if directory =~ /^[0-9]/
    filename = File.join(directory, &quot;notes.md&quot;)
    next if !File.exists?(filename)
    markdown_content = File.readlines(File.join(directory, &quot;notes.md&quot;))
    outline_entries &lt;&lt; filename
    markdown_content.each do |markdown|
      if markdown =~ /^##/
        outline_entries &lt;&lt; &quot;  #{markdown}&quot;
        slide_ctr = slide_ctr + 1
      end
    end
  end
end

outline_entries.each do |outline_entry|
  puts outline_entry
end

puts &quot;Total slides: #{slide_ctr}&quot;
```

Thank you [John Gruber](http://daringfireball.net/) for MarkDown.  Thank you, Thank you, Thank you, Thank you!</description>
        <pubDate>Sun, 19 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/02/19/ascii-rules-binary-drools-markdown-over-powerpoint-ftw.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/02/19/ascii-rules-binary-drools-markdown-over-powerpoint-ftw.html</guid>
        
        <category>markdown</category>
        
        <category>writing</category>
        
        <category>udemy</category>
        
        <category>deckset</category>
        
        <category>software_engineering</category>
        
        <category>knowledge_capture</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Visual Studio Code for a Rails Guy</title>
        <description>Editors are among the stickiest of software tools for a developer.  Want to get me to change editors -- good luck!  I've been using [TextMate](https://macromates.com/) since 2006 now and only switched off the long obsolete TextMate 1 to TextMate 2 about a year ago.  Editors are just that sticky.  Here are some of the things I've tried and rejected since the point I realized that I'm stuck in legacy land:

* Sublime (didn't feel right)
* Atom (too slow)
* VIM (been there; done that; got the t-shirt; ;wore the t-shirt until it was threadbare; wrote several hundred k lines of code in it from 2002 to 2005; no thanks; only server side from now on and even then there's swearing; love it conceptually; hate to configure it)
* MacVIM (didn't feel right)

Again and again I walk away from TextMate but I keep coming back.  However, recently, [Nick](http://nickjanetakis.com/blog/) has convinced me to try Visual Studio Code and I'm **intrigued**.  As someone who literally came of computing age with Microsoft IDEs and then fled into *nix ecosystems screaming, I never expected myself to once again be in a Microsoft produced editor and I would have bet money that that editor would NOT run on Linux -- I would be wrong, 1000% wrong.  Visual Studio Code: 

* Feels Great
* Runs on Linux
* Performs Pretty Well
* Has An Enormous Plugin Library

# Installation on OSX and Linux

Start with the installation instructions [here](http://code.visualstudio.com/docs/setup/setup-overview).  Here are the Mac and Linux downloads:

* [Mac](http://code.visualstudio.com/docs/setup/mac)
* [Linux](http://code.visualstudio.com/docs/setup/linux)

# Installation on Linux

I had some post install issues here so I'm going to cover this in a later post.

# Summary

Visual Studio Code really, really feels good.  I am still falling back to TextMate but that's mostly from convenience and muscle memory.  I think I have to keep trying Visual Studio Code.  The fact that its available on OSX and Linux is a huge, huge win in its favor.  And since my Linux box has more memory than any OSX box I own, well, perhaps the memory overhead of Electron won't matter so much.</description>
        <pubDate>Fri, 17 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/17/visual-studio-code-for-a-rails-guy.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/17/visual-studio-code-for-a-rails-guy.html</guid>
        
        <category>rails</category>
        
        <category>hyde</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Troubleshooting Rails and Sidekiq</title>
        <description>I recently had a problem with Rails and [Sidekiq](http://sidekiq.org/) that confused the heck out of me for an embarrassingly long time so I thought I'd document.  Sidekiq is an asynchronous job processor that allows you to defer a long running routine so a collection of other computing resources take care of it.  In this case it was a data extraction routine which should have written some records to the database.  I deployed the code to AWS, made an image and then launched 180 instances to process the data.  Each of the instances was configured with Sidekiq on boot so the minute that AWS launched the instance they would all start churning thru the data with the very best threading that [Mike Perham](http://www.mikeperham.com/) can bring to bear on the problem.  Mike is the author of Sidekiq, a great member of the Open Source community and the best threaded programmer I've ran across in years and years.

This all started with a bug where the data sharding call was being made but data was ending up in the wrong table.  The error, naturally, was in my application code but the true error was in my debugging approach.  I fixed the code, verified it locally and locally via Sidekiq and then deployed.  Then I tried it on the box where I made the AMI from.  And I got back a hash from the async call indicating that SideKiq got it -- but nothing happened.  *blink*  More changes and debugging calls and such were made and I still never saw the data flow into the database tables.  Then I got the bright idea -- take Sidekiq out and call the method directly without the async and lo and behold it worked.  Sigh.

At this point I went away from the screen and worked on documentation for a while while I just thought about it.  A second debugging session finally solved it.  Here's what was going on:

* 1 box, the source of the AMI, is being used to load the data into SideKiq
* 180 other boxes are sitting idly saying &quot;Got something for me to do?  No -- I want to do it!  Me too!&quot;
* The bug was STILL in the 180 boxes because I never shut them down and relaunched them from the AMI
* Even though I was working on a box that was shoving the data into Sidekiq and had the **right, working** code, one of the other boxes was simply picking it up first

The solution was to shut down the 180 boxes, re-make the AMI, relaunch the 180 boxes and feed Sidekiq the data again.
</description>
        <pubDate>Fri, 17 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/17/troubleshooting-rails-and-sidekiq.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/17/troubleshooting-rails-and-sidekiq.html</guid>
        
        <category>rails</category>
        
        <category>sidekiq</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>OSX to Linux in 10 Minutes 01 - Installing Albert on Ubuntu Mac</title>
        <description>One of the most pleasant aspects of OSX is the tool [Alfred](https://www.alfredapp.com/).  Alfred is a simple launcher that you can toggle with CTRL+SPACE and the best way I can describe it is that it makes the god awful OSX search engine (Spotlight) actually usable.  Honestly I've used Alfred so long now that I don't even know if it is its own search engine or a Spotlight front end or what.  What I do know is that I absolutely love it.

I started the day with the realization that I need to work more on my desktop Linux box.  Honestly I have a 32 gig Intel NUC running a Dell Ultra Widescreen monitor and I don't use it since I'm in an OSX rut.  So I turned it on and: 

* The network connectivity was dead
* The only easy way I could get that working again was to reboot it

Sigh.  All is not necessarily well in Ubuntu Land when the first thing you have to do is restart it but then again my brand new MacBook Pro that cost me about 3 grand, well, it shuts down constantly.  So win some, lose some I guess.

Anyway I rebooted and the very first action I did was press CTRL+SPACE and then swore up and down when Alfred wasn't there for me.  So let's fix that, shall we?

# Installation

A little bit of [Google fu](http://www.webupd8.org/2015/01/albert-fast-lightweight-quick-launcher.html) told me that the desktop Linux equivalent is called Albert.  Here's how to install it:

```bash
sudo add-apt-repository ppa:nilarimogard/webupd8
sudo apt-get update
sudo apt-get install albert
```

After that, if you're running [Ubuntu-Mate](https://ubuntu-mate.org/), then you'll find it in Applications / Accessories.  Once you run it, it recognizes that it is on first launch and you need to set a command keystroke to summon it and it will ask you if you want to.  Select this option and then you can map it to anything.  I used CTRL+SPACE.

You probably want this to start automatically when your Linux box starts up so here's some Terminal Fu to do that.  

```bash
cp /usr/share/applications/albert.desktop ~/.config/autostart/ &amp;&amp; chmod +x ~/.config/autostart/albert.desktop
```

Thank you to [rhoconlinux](https://github.com/albertlauncher/albert/issues/11) for contributing this!

Once you have this installed then you can press CTRL+SPACE on your Linux desktop and search for things in much the same way that you can on OSX.  **Recommended**.

# Further Reading:

* [Home Page](https://github.com/albertlauncher/albert)
* [Docs](https://albertlauncher.github.io/docs/)
* [Quick Read](http://www.webupd8.org/2015/01/albert-fast-lightweight-quick-launcher.html)
* [Patreon](https://www.patreon.com/albertlauncher)
* [PayPal](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=W74BQPKPGNSNC)


**Note**: No screenshots yet; still figuring that out on Linux.
</description>
        <pubDate>Fri, 17 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx_to_linux/2017/02/17/osx-to-linux-in-10-minutes-01-installing-albert-on-ubuntu-mac.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx_to_linux/2017/02/17/osx-to-linux-in-10-minutes-01-installing-albert-on-ubuntu-mac.html</guid>
        
        <category>osx_to_linux</category>
        
        <category>osx</category>
        
        <category>postmac</category>
        
        <category>linux</category>
        
        <category>ubuntu</category>
        
        
        <category>osx_to_linux</category>
        
      </item>
    
      <item>
        <title>Getting Rid of Smart Quotes on OSX</title>
        <description>I don't know about any other developer but I bloody well hate smart quotes.  I'm sorry but if I type in a &quot;foo&quot; mark then I expect it to remain &quot;foo&quot; not some bastardized quote version that is going to get messed up into curly quotes when I copy and pasted.  I have long that this was either application behavior or behind the scenes OSX behavior that I was doomed to forever have to cope with.

In reading a [Hacker News thread on Amazon Chime](https://news.ycombinator.com/item?id=13641301), I found out that wasn't true: 

&gt; If you are on OSX, that is a feature of OSX, not Slack. You can turn it off in Keyboard settings IIRC. [Hacker News](https://news.ycombinator.com/item?id=13644973)

When I went to do this myself though I couldn't find it initially so here's the whole flow:

# Start at the Apple Menu
![curly_quotes_begone_01.png](/blog/assets/curly_quotes_begone_01.png)

# Now You're At System Preferences
![curly_quotes_begone_01.png](/blog/assets/curly_quotes_begone_02.png)

# Now You're At Keyboard
![curly_quotes_begone_01.png](/blog/assets/curly_quotes_begone_03.png)

# Now You Select Text
![curly_quotes_begone_01.png](/blog/assets/curly_quotes_begone_04.png)

# Now You Turn off the Use Smart Quotes and Dashes Checkbox
![curly_quotes_begone_01.png](/blog/assets/curly_quotes_begone_05.png)

And the final step is to close out of System Preferences which should automatically save your settings
</description>
        <pubDate>Thu, 16 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/02/16/getting-rid-of-smart-quotes-on-osx.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/02/16/getting-rid-of-smart-quotes-on-osx.html</guid>
        
        <category>osx</category>
        
        <category>mac</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Reversing CSS Minification</title>
        <description>For the first time in a very, very long time, I find myself actually writing a somewhat &quot;modern&quot; web app.  By which I mean that it has to use CSS and use it both well and effectively.  I am essentially a back end developer wandering around a series of increasingly complex front end tasks.  As with all web development one of the keys is looking at the source i.e. *Use the source Luke*.  The problem with looking at the CSS source is that generally CSS content is minified which means that all line breaks, extra spacing and such are removed rendering it hard, hard, hard to understand.  

The trick here is to reverse the minification.  Here are a few tools:

* [Web Based Reverse Minifier](http://mrcoles.com/blog/css-unminify/)
* [Visual Studio Code Plugin](
https://marketplace.visualstudio.com/items?itemName=HookyQR.beautify)

The easiest one to use is the web based tool for Mr. Coles.  Just paste your CSS into a text box on the web page and click a form button.  Here's a screenshot:

![css_reverse_minifier.png](/blog/assets/css_reverse_minifier.png)

A virtual hat tip to [Nick](http://nickjanetakis.com/blog/) who found these for me.  Thanks man!

One general note about reverse engineering CSS is that its actually pretty **hard**.  I was looking for examples of good looking page footers and the one I found was, well, mind bending in its complexity.  I then simply googled for [bootstrap footer](https://www.google.com/search?tbs=li:1&amp;q=bootstrap+footer) and found [bootsnipp.com](http://www.bootsnipp.com/) where I found this [footer](http://bootsnipp.com/snippets/1KEEq) which was perfectly designed for use as a template.  

I still think reversing CSS minification is useful but finding things designed to be used as templates is perhaps more useful.
</description>
        <pubDate>Wed, 15 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/css/2017/02/15/reversing-css-minification.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/css/2017/02/15/reversing-css-minification.html</guid>
        
        <category>css</category>
        
        <category>hyde</category>
        
        
        <category>css</category>
        
      </item>
    
      <item>
        <title>Bash on OSX versus Bash on Ubuntu and Upgrading Your Bash</title>
        <description>Bash is one of those things that you just don't think about all that much -- until you want something that is apparently new.  In my case I was looking to use the ** syntax which, mysteriously, I found doesn't work at all on OSX (even if you turn it on).  This let me to use bash --version.  

Here's the result for Linux:

```bash
bash --version
GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;

This is free software; you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
```

Here's the result for OSX:

```bash
bash --version
GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin15)
Copyright (C) 2007 Free Software Foundation, Inc.
```

Intellectually I've always known that OSX lagged behind on Open Source tooling but 6 years behind???  Really???  Apparently this is a GPL v3 issue so I get it but **sigh**.

# Upgrading Bash on OSX

I will admit that I am absolutely terrified to try upgrading Bash on OSX since I eat, sleep and breathe in the terminal.  The research I've done shows me that you can upgrade your Bash to a version 4 series with just this brew command:

```bash
brew install bash
```

And [HomeBrew](https://brew.sh/) is awesome so I suspect it works; I'm just chicken.

More on Upgrading Bash: [ClubMate](http://clubmate.fi/upgrade-to-bash-4-in-mac-os-x/) | [Stack Overflow](http://apple.stackexchange.com/questions/193411/update-bash-to-version-4-0-on-osx)

# Sidebar: The ** Syntax Explained

The ** syntax allows the file globbing to operate recursively with a single command.  For example:

```bash
wc -l **/*.rb
```

would count all the lines of ruby code across all subdirectories.  If you haven't enabled ** then you can do this with:

```bash
shopt -s globstar
```

I first the ** syntax on Stack Overflow [here](http://stackoverflow.com/posts/14689618/revisions). Thank you [Michael Wild](http://stackoverflow.com/users/159834/michael-wild).

</description>
        <pubDate>Wed, 15 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2017/02/15/bash-on-osx-versus-bash-on-ubuntu.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2017/02/15/bash-on-osx-versus-bash-on-ubuntu.html</guid>
        
        <category>osx</category>
        
        <category>linux</category>
        
        <category>bash</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Rails in 10 Minutes - How to Add a Fav Icon to Your Rails App</title>
        <description>I am a huge, huge believer in what are called &quot;favicons&quot;.  These are the tiny icons that your web application declares to the world:

&gt; &quot;here's a visual representation of **this site** to use when someone creates a bookmark link to me on their browser link bar&quot;

I know that sounds perhaps beyond nerdy but that's what they do.  I use a LOT of web sites and rely on them constantly so what I do is drag links from them to my browser link bar and when a fav icon exists it means I can use just an icon instead of text.  Here is, for right now, what my browser link bar looks like:

![browser_link_bar](/blog/assets/browser_link_bar.png)

Here's how to add a favicon to your Rails app.

1.  Come up with a favicon in either .ico or .png format.  I can't tell you which is better but I know that both of these work.
2.  Place that icon file in app/assets/images
3.  In your application.html.erb template in app/views/layouts you want to add this html snippet:

```ruby 
&lt;%= favicon_link_tag asset_path('favicon.png'), :rel =&gt; 'icon', :type =&gt;  'image/png' %&gt;
```
or

```ruby 
&lt;%= favicon_link_tag asset_path('farmer.ico'), :rel =&gt; 'icon', :type =&gt; &quot;image/x-icon&quot; %&gt;
```

Do *add* / *commit* / *push* / *deploy* dance and check in production

**Note 1:** Once upon a time you had to always name these favicon.ico but that's no longer necessary as your html tells the browser what to look for.

**Note 2:** [Helpful Stack Overflow post I cribbed from](http://stackoverflow.com/questions/13827325/correct-mime-type-for-favicon-ico).

    

</description>
        <pubDate>Tue, 14 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/02/14/how-to-add-a-fav-icon-to-your-rails-app.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/02/14/how-to-add-a-fav-icon-to-your-rails-app.html</guid>
        
        <category>rails</category>
        
        <category>hyde</category>
        
        <category>design</category>
        
        <category>user_interface</category>
        
        <category>ui</category>
        
        <category>rails_in_10_minutes</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>How to Make a Side Project Interesting</title>
        <description>Taking a quick look at the different development directories on the various hard discs that I use for code related stuff shows that I've started over 20 different side projects since 2006.  Generally speaking that averages out to about 2 a year.  My current side project, while not at release stage yet, has now sustained over 45 days of continuous coding -- at least 2+ hours every day.  This morning, after making sure my wife was up, I had an mild epiphany as I walked back to my basement office.  The reason that I've been able to keep working is that I've been **interested**.

Sure building projects is interesting but that's actually not what I mean.  A very big reason that I've been able to keep working on this so consistently is that I've looked at the what bothers the crap out of me about building data intensive SAAS apps (which this is) and I've figured out how to **minimize** those bad things.  I know exactly what was technically wrong with the [last SAAS app](http://www.appdata.com) I built and I don't want that to hit me again.  When I built my last SAAS app, I looked at it after the day I launched it and I thought to myself, &quot;Whelp - I can spend the next decade of my life actually managing it&quot; and here it is 3/4 of a decade later and, until recently, I was tied to it by a ball and chain.  

I don't want to go into exactly what bothered the crap out of me -- it is going to be different for everyone.  Your crap isn't my crap and vice versa.  But if you want to make side projects interesting then figure out what technically bothered you in prior projects and address it.  Now the trick here is to avoid what is referred to as [The Second System Effect](https://en.wikipedia.org/wiki/The_Mythical_Man-Month#The_second-system_effect).  This is the situation where you get so involved in fixing the crap that you over engineer.  And, lest you think that second system effect is just engineering lore, I can absolutely attest to it.  On at least one startup, I spent so much time at the framework layer that I never really got to the product.

In my case, since the crap that bothered me, was at the data layer, that was a serious risk.  What I did though was to tackle it with simple, common sense techniques that I know for a fact [scale and scale brilliantly](http://fuzzyblog.io/blog/startup/2016/09/01/what-matt-mullenweg-taught-me.html).
</description>
        <pubDate>Sun, 12 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/side_project/2017/02/12/how-to-make-a-side-project-interesting.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/side_project/2017/02/12/how-to-make-a-side-project-interesting.html</guid>
        
        <category>side_project</category>
        
        <category>hyde</category>
        
        
        <category>side_project</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day - curl</title>
        <description>While I try hard to write things described as &quot;* of the day&quot; regularly there are times when life throws you a curve ball.  Yesterday's curve ball was the intersection of sick child, snow and a failed sewage lift pump led to, well, personal hell at home.  After a very busy work day I spent the evening that I had allocated to writing, well, playing clean up man before the real plumber showed up.  Anyway ... on to curl!

The standard Linux tool curl seems to always be present on Linux / OSX systems so I can skip over the installation issues and I suspect most of us know the basics i.e.:

&gt; curl http://foo.com/bar.html

will fetch the file bar.html from the foo.com website and place it in the current directory.  Given that our goal here was to discuss how curl was used in John Graham Cummings [hncomments](https://github.com/jgrahamc/hncomments/blob/master/hncomments) bash script and that's all we need, I could stop here but let's go for the gusto and give some examples of advanced usage of curl.

# Output Redirection with curl

Normally the output of curl is written to the same filename that was used on the server.  You can use the &amp;gt; character to redirect output to a new file as showing here:

&gt; curl yoururl.com &gt; yoururl.html

[More](https://geekflare.com/curl-command-usage-with-example/)

# Send a Custom Header with curl

If you want to send a custom header with curl then you can inject it with --header:

&gt; curl --header 'Content-Type: application/json' http://yoururl.com

[More](https://geekflare.com/curl-command-usage-with-example/)

# Get Headers with curl

You can get a feel for the Internet http headers with --head:

&gt; curl --head http://fuzzyblog.io

```bash
curl --head http://fuzzyblog.io
HTTP/1.1 200 OK
Server: GitHub.com
Date: Fri, 10 Feb 2017 02:00:34 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 1125
Last-Modified: Mon, 16 Jan 2017 18:12:56 GMT
Access-Control-Allow-Origin: *
Expires: Fri, 10 Feb 2017 02:10:34 GMT
Cache-Control: max-age=600
Accept-Ranges: bytes
X-GitHub-Request-Id: EA7C:71BB:11DF3A8:1883A50:589D1EC2
```

This is particularly useful if you build tools like crawlers / indexers because you can use the intersection of Last-Modified and Control-Length to make a hash key that you use for determining if you should index something.  HTTP head operations are very cheap because only the header is transferred.  [More on Head](https://curl.haxx.se/docs/httpscripting.html#HEAD)

# Testing Advanced Options

If you're looking for help with curl then try the [Online Curl tool](https://curlbuilder.com/) which gives you a user interface that has generates a curl command as its output.

# References

Here are some more references:

* [Me on Using Curl to Test APIs](http://fuzzyblog.io/blog/software_engineering/2017/01/05/scott-s-rule-of-api-development.html)
* [Haxx](https://curl.haxx.se/docs/httpscripting.html#HEAD)
* [GeekFlare on Curl](https://geekflare.com/curl-command-usage-with-example/)
* [Linux.com on Curl](https://www.linux.com/blog/5-basic-curl-command-examples-you-can-run-your-linux-server)
* [Rose Hosting on Curl](https://www.rosehosting.com/blog/curl-command-examples/)
* [The Geek Stuff on Curl](http://www.thegeekstuff.com/2012/04/curl-examples/?utm_source=feedburner)
* [Curl and SendGrid](https://sendgrid.com/docs/Classroom/Send/v3_Mail_Send/curl_examples.html) - This is particularly good since it has examples of http post operations.</description>
        <pubDate>Wed, 08 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/08/linux-tip-of-the-day-curl.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/08/linux-tip-of-the-day-curl.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>curl</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>On Merging Files - Diff Alternatives on the Mac</title>
        <description>There are certain skills as a software developer that take, in my experience, a lifetime to master.  I have now been using version control since the mid 1990s and I still don't consider myself, by any means, a master.  I'm good, don't get me wrong, but I don't feel that I have yet fully internalized all the benefits that version control offers.  So, when I have a chance to really take advantage of version control, I truly relish the opportunity.  

I recently had a refactoring situtation where I was, shall we say, overly ambitious.  And, yes, that's the positive version of what I'd refer to as &quot;Humpty Dumpty&quot; code syndrome where you have so thoroughly broken a critical bit of code that you just can't put it back together.  Specifically:

* I refactored a truly alway bit of two critical routines, one with 8 input variables that returned 6 variables
* No test coverage
* It would not, no matter what I did, do what it did before; specifically it would do the first pass of an N pass algorithm but I could never make it go beyond stage 1
* In the process of doing this I added new and highly desirable functionality everywhere and fixed about 30 critical code mistakes along the way

So, well, **crap**.  The fix was, in the immortal words of Doctor Who, to get all timey-whimey!  Specifically I needed to time travel back to *before* I started and then:

* confirm that this code actually worked in the first place -- maybe this problem existed before I *fixed* it
* figure out how to travel back from the past while bringing with me a working version
* NOT lose any of the cool new features

One of the development practices I am absolutely **adamant** about is only ever having 1 checkout of the code based at a time.  I'm hardcore on this because I've seen what happens when you have multiple versions floating about -- its very, very convenient but you tend to get confused and then you lose track of what's the master and things get screwball.  But, in desparate times, you do what you must and so I went over to github and:

* I found a commit from roughly before I started; I wasn't real specific about this, I just remember &quot;this debacle started last Thursday so goto Wednesday&quot;
* I browsed the code base at that point
* I created a new directory where I wanted this
* I checked out the code; that put a fresh copy of master in place
* I did a git co HASH where HASH represented the commit hash for Wednesday

This put me what's called [*detached head*](http://stackoverflow.com/questions/3965676/why-did-my-git-repo-enter-a-detached-head-state) mode and I was able to modify /run / test, etc.  I then had to: 

* port over my rake task which was doing my tests
* port over the individual changes, one by one, *carefully* to let the old code run correctly

What I found was that the old code did, in fact, run better than the new code.  Sigh.  And after 2 days of fixing things all over the place I had the system back to running order with the exception that I now had 2 code bases that needed to be merged.  *Bigger Sigh*.

# Time for some Diff Magic

So I now had two git checkouts with differences between them.  One of them was master, and did I mention that while this was all going on I got pulled off the project to fix something else and add yet more functionality to the code base for a day and half, and the other was my time travel branch.  When I did a git status in the time travel branch I got this:

```bash
    git status
    HEAD detached at 93bf221
    Changes not staged for commit:
      (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
      (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)

    	modified:   app/models/form.rb
    	modified:   app/models/form2016_q4.rb
    	modified:   app/models/iframe2016_q4.rb
    	modified:   app/models/link.rb
    	modified:   app/models/link2016_q4.rb
    	modified:   app/models/page.rb
    	modified:   app/models/page2016_q4.rb
    	modified:   app/models/page_body.rb
    	modified:   app/models/page_body2016_q4.rb
    	modified:   app/models/site.rb
    	modified:   app/models/unique_url.rb
    	modified:   app/models/unique_url2016_q4.rb
    	modified:   app/models/url_resolver.rb
    	modified:   app/models/url_tool.rb
    	modified:   config/initializers/constants.rb
    	modified:   lib/common_link.rb
    	modified:   lib/common_page.rb

    Untracked files:
      (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)

    	docs/snippets/create_mechanize_page_without_fetch.rb
    	lib/tasks/crawl2016_q4.rake

    no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
```
    
That's a whole lot of changes.  The easiest approach was to start with the entirely new files and just copy them over to the original.  

Now this is where I'm going to make a highly personal engineering choice here -- I'm going to avoid an automated merge process of any type.  When it comes to complex code changes in an automated fashion I honestly don't have a lot of faith.  I've seen automated merges go wrong time after time so I was looking for a tool to make this easier.  A quick bit of googling turned up:

* OpenDiff / FileMerge (part of X-Code)
* [meld](http://meldmerge.org/) - a python tool with an unofficial brew install via *brew install homebrew/gui/meld*
* [Beyond Compare]http://www.scootersoftware.com/download.php from [Scooter Software](http://www.scootersoftware.com/)
* [Diff Merge](http://www.sourcegear.com/diffmerge/) from [SourceGear](http://www.sourcegear.com/)
* [Kaleidoscope](http://www.kaleidoscopeapp.com/)

I ended up using Open Diff for my merge issues:

![opendiff](/blog/assets/open_diff.png)

Here are screenshots from some of the other tools I looked at:

## Diff Merge

![diff_merge](/blog/assets/diff_merge.png)

## Beyond Compare

![beyond_compare](/blog/assets/beyond_compare.png)

## Kaleidoscope

![kaleidoscope](/blog/assets/kaleidoscope.png)


## More Reading

Here are a few good articles on merge tools:

* [GitTower](https://www.git-tower.com/blog/diff-tools-mac)
* [outcoldman](https://www.outcoldman.com/en/archive/2014/05/12/git-dirdiff/)
</description>
        <pubDate>Tue, 07 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/02/07/on-merging-files-diff-alternatives-on-the-mac.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/02/07/on-merging-files-diff-alternatives-on-the-mac.html</guid>
        
        <category>software_engineering</category>
        
        <category>version_control</category>
        
        <category>merge</category>
        
        <category>git</category>
        
        <category>diff</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Linux Tip Of The Day - recode</title>
        <description>As I discussed [yesterday](http://fuzzyblog.io/blog/linux/2017/02/06/linux-tip-of-the-day-jq.html), I'm currently talking about the dependencies in John Graham Cummings, [hncomments](https://github.com/jgrahamc/hncomments/blob/master/hncomments) bash script.  At the end of this I'm hoping that I have all the pieces that I can actually run this myself.  

The recode tool, a utility I had never even heard of, converts files between various character sets.  Given all the troubles I've had over the years with crawling the different app stores and encodings, I'm glad to learn about this.

If you're on Linux then you can install recode with:

&gt; sudo apt-get install recode 

If you're on OSX then install recode with: 

&gt; brew install recode

Use recode --help to get assistance (this is only a subset of the help):

```bash
recode --help
Free `recode' converts files between various character sets and surfaces.

Usage: recode [OPTION]... [ [CHARSET] | REQUEST [FILE]... ]

If a long option shows an argument as mandatory, then it is mandatory
for the equivalent short option also.  Similarly for optional arguments.

Listings:
  -l, --list[=FORMAT]        list one or all known charsets and aliases
  -k, --known=PAIRS          restrict charsets according to known PAIRS list
  -h, --header[=[LN/]NAME]   write table NAME on stdout using LN, then exit
  -F, --freeze-tables        write out a C module holding all tables
  -T, --find-subsets         report all charsets being subset of others
  -C, --copyright            display Copyright and copying conditions
      --help                 display this help and exit
      --version              output version information and exit

Operation modes:
  -v, --verbose           explain sequence of steps and report progress
  -q, --quiet, --silent   inhibit messages about irreversible recodings
  -f, --force             force recodings even when not reversible
  -t, --touch             touch the recoded files after replacement
  -i, --sequence=files    use intermediate files for sequencing passes
      --sequence=memory   use memory buffers for sequencing passes
  -p, --sequence=pipe     use pipe machinery for sequencing passes

Fine tuning:
  -s, --strict           use strict mappings, even loose characters
  -d, --diacritics       convert only diacritics or alike for HTML/LaTeX
  -S, --source[=LN]      limit recoding to strings and comments as for LN
  -c, --colons           use colons instead of double quotes for diaeresis
  -g, --graphics         approximate IBMPC rulers by ASCII graphics
  -x, --ignore=CHARSET   ignore CHARSET while choosing a recoding path
```

If we go back to the source on hncomments then we can see how recode fits in:

&gt; jq -r '.hits | .[] | .author + &quot;\nhttps://news.ycombinator.com/item?id=&quot; + .objectID + &quot;\n\n&quot; + .comment_text + &quot;\n\n---\n\n&quot;' &lt;(echo $j) | sed -e 's/&lt;[^&gt;]*&gt;/ /g;' | **recode** -f html..ascii | mail -s &quot;Latest $q HN comments&quot; $e

Here recode is taking the output from sed and converting it from html character encoding to ASCII.  </description>
        <pubDate>Tue, 07 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/07/linux-tip-of-the-day-recode.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/07/linux-tip-of-the-day-recode.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>jq</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day - jq</title>
        <description>I made either the happy decision to do some late night Hacker News reading last night and I found a really interesting bash script -- **[hncomments](https://github.com/jgrahamc/hncomments/blob/master/hncomments)**.  This is a tool which looks at a data feed from HackerNews, through the algolia search engine, and emails you when comments matching what you are interested in appear.  My interest in it stemmed from its author - [John Graham Cumming (jgrahamc)](http://www.jgc.org/).  He's one of those programmers who I've always admired purely from an intellectual basis.  Anyone who can write books with the intellectual breadth of [Geek Atlas](https://www.amazon.com/gp/product/0596523203?ie=UTF8&amp;tag=jgcorg-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0596523203), [Gnu Make](https://www.nostarch.com/gnumake) and [Introduction to Go](http://shop.oreilly.com/product/0636920035305.do) has to be worth admiring.

Anyway I went to get this running and realized that it requires dependencies of jq, recode, sed and curl -- and I didn't even know what jq was so my thought was *this is a series of Linux Tips of the Day!*.  For the next few days I'll be talking about each of these starting with jq.  So, John, thank you for the idea and here's a tip on JQ.  After I write tips for each of these, I'll follow through with getting this whole thing running.

The Linux tool jq is a command line JSON processor designed to use JSON as part of a Unix pipeline.  In the case of the hncomments bash script, jq takes input from the the [algolia](http://www.algolia.com) output as called by curl.

To install jq on Linux, use:

&gt; sudo apt-get install jq

But you might not have to -- it was already installed for me even on servers I control which have pretty limited tools so this feels like it might be a standard tool.

To install jq on OSX, use:

&gt; brew install jq

Help is available, as it always should be (but often isn't), with --help.  Here's an example:

```bash
sjohnson@FuzzygroupMacbookAir ~ $ jq --help
jq - commandline JSON processor [version 1.5]
Usage: jq [options] &lt;jq filter&gt; [file...]

	jq is a tool for processing JSON inputs, applying the
	given filter to its JSON text inputs and producing the
	filter's results as JSON on standard output.
	The simplest filter is ., which is the identity filter,
	copying jq's input to its output unmodified (except for
	formatting).
	For more advanced filters see the jq(1) manpage (&quot;man jq&quot;)
	and/or https://stedolan.github.io/jq

	Some of the options include:
	 -c		compact instead of pretty-printed output;
	 -n		use `null` as the single input value;
	 -e		set the exit status code based on the output;
	 -s		read (slurp) all inputs into an array; apply filter to it;
	 -r		output raw strings, not JSON texts;
	 -R		read raw strings, not JSON texts;
	 -C		colorize JSON;
	 -M		monochrome (don't colorize JSON);
	 -S		sort keys of objects on output;
	 --tab	use tabs for indentation;
	 --arg a v	set variable $a to value &lt;v&gt;;
	 --argjson a v	set variable $a to JSON value &lt;v&gt;;
	 --slurpfile a f	set variable $a to an array of JSON texts read from &lt;f&gt;;
	See the manpage for more options.
```

Because this is designed to be used as part of a pipeline and you have to data in JSON format for it to work, I'm going to illustrate its use by stealing from the **hncomments** bash script (thank you John!):

```bash
j=`curl -s &quot;https://hn.algolia.com/api/v1/search_by_date?query=$q&amp;tags=comment&amp;numericFilters=created_at_i&gt;$l&quot;`

if [ &quot;$j&quot; == &quot;&quot; ] ; then
  exit 0
fi

n=`jq '.hits | .[] | .created_at_i' &lt;(echo $j) | sort -nr | head -n 1` 
if [ &quot;$n&quot; == &quot;&quot; ] ; then
  exit 0
fi
echo $n &gt; $f
```

For purposes of illustration, let's assume that $q is **docker**.  This means that the curl statement is executing the url at hn.algolia.com and returning it as $j.  The jq tool is then filtering down the json by looking for the hash key &quot;hits&quot; in the json output.  Here's an example of what the start of that output looks like:

&gt; {&quot;hits&quot;:[{&quot;created_at&quot;:&quot;2017-02-06T08:29:47.000Z&quot;,&quot;title&quot;:null,&quot;url&quot;:null,&quot;author&quot;:&quot;schappim&quot;,&quot;points&quot;:null,&quot;story_text&quot;:null,&quot;comment_text&quot;:&quot;Function as a service (like AWS Lambda, Google Cloud Functions and Azure Functions) for Ruby.

More on all of this tomorrow when I look into recode.</description>
        <pubDate>Mon, 06 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/06/linux-tip-of-the-day-jq.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/06/linux-tip-of-the-day-jq.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>jq</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Ruby Regex Performance - Scan versus Match</title>
        <description>I got my start with text processing using a custom version of [Awk](https://en.wikipedia.org/wiki/AWK), HyperAwk, that my first company, NTERGAID, built.  What made HyperAwk unique was its ability to read not just ASCII but Word, WordPerfect, Ami and other binary structures and render them into a generic parseable format where you could look for document structures like bold facing, heading tags and the like.  We used it constantly from roughly 90 to 96 for things like &quot;Transform every issue of [Dr. Dobb's Journal](http://www.drdobbs.com/) from source file to hypertext&quot;.  I even got a [cover story](http://www.drdobbs.com/web-development/the-ddj-hypertext-project/201800824?queryText=%2522Scott%2BJohnson%2522%2Bhypertext) out of it which was a huge personal win.  The engineer on the project, [Gene Callahan](http://gene-callahan.blogspot.com/2011/06/reference-management-software.html), was a Unix guy surrounded by a bunch of DOS heads and he recognized that as our document recognition tasks got harder, what started as a tool, became a language.

Because I started with text processing from an Awk perspective, I'm a *regex kind of guy*.  Yes I know Nokogiri and, yes, I know xpath but for simple data extraction, my goto is always a regex. And when I'm in Ruby I tend to use .scan.  It is easy, convenient and I know it cold.  But is it fast?  I don't think I've ever even thought about it.  I've always known that there are different things like .match but I never ventured there -- until now.

What I just found out is that .match is dramatically faster.  Here's an example:

```ruby
 tpp.parse
{
              :blaahs_count =&gt; 324,
           :blaaaargs_count =&gt; 281,
           :followers_count =&gt; 139,
             :match_seconds =&gt; 0.00841700000455603,
              :scan_seconds =&gt; 0.030331000016303733
}
2.3.1 :043 &gt; tpp.parse
{
              :blaahs_count =&gt; 253,
           :blaaaargs_count =&gt; 66,
             :match_seconds =&gt; 0.006857000000309199,
              :scan_seconds =&gt; 0.02245499999844469
}
```

Note: All benchmarking tests were down with a Benchmark.realtime do block wrapping the code in question.  Only the core matching operations were benchmarked; not the underlying network io.

If you do the math that's between *3.2* and *3.6* times faster.  On other tests I found about a 3 fold increase in speed.  As my old friend Pete Jenney would have said **Great Googlely Moogley!**  I should point out that he was saying that in 1996 long before Google even existed.

As with any benchmark, your results will definitely vary.  I'm working here with pretty big strings (web pages; this is a simple data extractor).

</description>
        <pubDate>Sun, 05 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/02/05/ruby-regex-performance-scan-versus-match.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/02/05/ruby-regex-performance-scan-versus-match.html</guid>
        
        <category>ruby</category>
        
        <category>regex</category>
        
        <category>performance</category>
        
        <category>ntergaid</category>
        
        <category>hyper_awk</category>
        
        <category>gene_callahan</category>
        
        <category>pete_jenney</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day - git-standup</title>
        <description>Over on Twitter, [@kamranahmedse](https://twitter.com/kamranahmedse) pointed out to me that git-standup does similar things to [git-recall](http://fuzzyblog.io/blog/linux/2017/01/30/linux-tip-of-the-day-git-recall.html) which I recently talked about -- and more.  First off -- thank you!  Based on his [github account](https://github.com/kamranahmedse/git-standup), I'd assume that git-standup is his project and he's done really nice work here.

In case you missed the previous post on git-recall, git-standup helps you to understand what's been going on with a codebase by looking at commits either during the same day or a given range of days.  

Install on Linux as follows:

&gt; curl -L https://raw.githubusercontent.com/kamranahmedse/git-standup/master/installer.sh | sudo sh

or you can use npm:

&gt; npm install -g git-standup

or if you are on OSX then you can use brew:

&gt; brew install git-standup

Here are the **very useful** options that git-standup supports:

```bash
$ git standup [-a &lt;author name&gt;] 
              [-w &lt;weekstart-weekend&gt;] 
              [-m &lt;max-dir-depth&gt;]
              [-f]
              [-L]
              [-d &lt;days-ago&gt;]
              [-D &lt;date-format&gt;] 
              [-g] 
              [-h]
```

After you change into a directory where you're working on something git, do this:

&gt; git-standup

and you'll get a view of what happened over the past day.  Where git-standup shines over git recall is in its use of command line options to expand beyond the previous day.  And here's an example of looking back over the past 5 days:

![git-standup.png](/blog/assets/git-standup.png)

</description>
        <pubDate>Sat, 04 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/04/linux-tip-of-the-day-git-standup.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/04/linux-tip-of-the-day-git-standup.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>git</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Redirecting HTTP to HTTPS with AWS and ELB</title>
        <description>&lt;p style=&quot;text-align:center&quot;&gt;A Tale of Four Redirects&lt;/p&gt;

Every technologist worth his or her salt has an internal list of &quot;most hated technologies&quot;.  These are the things that no matter what they do always, always give them trouble.  Whether its because the technology in question is actually hard, a mental block or just plain 'ol bad luck, we all have them.  For me it is always, always [mod_rewrite](http://httpd.apache.org/docs/current/mod/mod_rewrite.html).  [Apache's](http://httpd.apache.org/) mod_rewrite is just plain magic -- when it works (and it can always be made to work), it does in place url transformations that let you change your entire technology stack but let Google keep on serving those old urls.  It really is outstanding.  But Great Ghu does it ever give me trouble at times.

Note: For a basic tutorial on mod_rewrite, please see this [blog post](http://fuzzyblog.io/blog/apache/2017/02/03/apache2-and-mod-rewrite.html).

The situation in question was that I had a client with Rails being served by Apache / [Passenger](https://www.phusionpassenger.com/) (my very **favorite** Rails deployment tool) and the client wanted, correctly, to convert all http based url requests to https.  Here's the transition of the redirects one by one.

# Important - Remove AllowOverride all from Your Apache Conf File

I can't stress this enough.  The Apache directive **AllowOverride all** means that redirects will not work.  Honestly I don't even understand what AllowOverride all even does but I did confirm, experimentally, that if you have this, redirects simply will not work.  So make sure that you've got this turned off.

# Redirect 1

Here was my first attempt:

    RewriteEngine On
    RewriteCond %{HTTPS} off
    RewriteRule ^ https://%{HTTP_HOST}%{REQUEST_URI}
    
This would actually work but our Apache isn't actually serving https at all.  When you use an AWS ELB (Elastic Load Balancer) in conjunction with the ACM (AWS Certificate Manager) then your web server talks http on port 80 and the ELB acts as an intermediary translating it to and from https.  If you're lucky with this redirect then your site may, possibly work.  If you're unlucky then you'll get stuck in an infinite loop of http to https to http and so on.  [Source](http://stackoverflow.com/questions/16200501/http-to-https-apache-redirection)

# Redirect 2

My second attempt actually came straight from the [AWS docs](https://aws.amazon.com/premiumsupport/knowledge-center/redirect-http-https-elb/) and it is **flat out wrong**.

    RewriteEngine On
    RewriteCond %{HTTP:X-Forwarded-Proto} =http
    RewriteRule . https://%{HTTP:Host}%{REQUEST_URI} [L,R=permanent]
    
The problem here is that the 301 redirect which this is generating causes the ELB health check to go poorly and your site is declared offline at which point no traffic gets to it.  This can be very annoying to troubleshoot but you should watch for back end not available messages.

# Redirect 3

On an [AWS forum thread](https://forums.aws.amazon.com/thread.jspa?messageID=745509) I found this redirect which an AWS engineer contributed and claimed would work.  Again it is **flat out wrong**.

    RewriteEngine on
    RewriteCond %{HTTP:X-Forwarded-Proto} !https
    RewriteRule .* https://%{HTTP_HOST}%{REQUEST_URI} [R=301,L]
    
This causes the same health check problem as #2.

# Redirect 4 - The Working One

If you read support forum threads *closely* and **well** then you often, always near the end, find the right answer.  Here's the redirect that worked for me:

    # the final correct redirect
    RewriteEngine on
    RewriteCond %{HTTP:X-Forwarded-Proto} ^http$
    RewriteRule .* https://%{HTTP_HOST}%{REQUEST_URI} [R=301,L]
    
This is from the same thread above (#3).  You'll notice that #2, #3 and #4 all rely on the HTTP:X-Forwarded-Proto http header feature - this prevents problems with the ELB.  The description that the author gave is interesting:

&gt; The Apache block (#2) above caused our ELB health check to fail from the HTTP 301 response. To allow this and other local requests over HTTP while redirecting external requests through the ELB to HTTPS, adjust the rewrite condition to match on http instead of a negative match on https:  

This solution was contributed by [holeinonenv](https://forums.aws.amazon.com/profile.jspa?userID=342382) and, sir, I owe you a beer.  Thank you for your help. It is the only thing he's ever posted on the AWS forums and man am I glad he posted it.</description>
        <pubDate>Fri, 03 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/02/03/redirecting-http-to-https-with-aws-and-elb.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/02/03/redirecting-http-to-https-with-aws-and-elb.html</guid>
        
        <category>aws</category>
        
        <category>apache</category>
        
        <category>elb</category>
        
        <category>http</category>
        
        <category>https</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Linux Tip Of The Day - Using rsync for Backup</title>
        <description>Once upon a time when I had complex file copying to handle, I used **xcopy**.  It has been a lot of years since DOS and xcopy and as of late, I reach for rsync.  Yesterday I had the situation where I wanted to make a command line which would take the contents of two drives and backup from drive A to drive B -- full copy.  It was enough files that, well, I just wouldn't trust any gui operation with it (hundreds of gigs if not a terabyte).

This is a problem that I've tried before to tackle and actually failed at so I grabbed [Nick](http://www.nickjanetakis.com/blog/) via [Google Hangouts](http://hangouts.google.com/) and he and I whipped it together easily.  We started with:

&gt; rsync --help

And then we went thru the options one by one.  I've given the example with the long form versions of the arguments so it is a little bit more understandable:

&gt; rsync --archive --recursive --inplace --progress SRC DEST

So when you replace SRC with the actual source of the files and DEST with the destination you get this command line:

&gt; rsync --archive --recursive --inplace --progress /Volumes/Plex/ /Volumes/PlexBackup

There are an absolutely enormous set of rsync options, too many to go into here but if you have to move lots and lots of files around, rsync can generally do it.

The rsync tool is powerful enough that there is a whole company devoted to supporting and extending it -- [rsync.net](http://www.rsync.net).</description>
        <pubDate>Fri, 03 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/03/linux-tip-of-the-day-using-rsync-for-backup.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/03/linux-tip-of-the-day-using-rsync-for-backup.html</guid>
        
        <category>linux</category>
        
        <category>sysadmin</category>
        
        <category>devops</category>
        
        <category>backup</category>
        
        <category>linux_tip_of_the_day</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Apache2 and Mod Rewrite and Automatic Redirect to https</title>
        <description>**Note:** If you're using AWS and an Elastic Load Balancer (ELB) this is actually tricky and will not work.  I documented that process in my [AWS Redirection](http://fuzzyblog.io/blog/aws/2017/02/03/redirecting-http-to-https-with-aws-and-elb.html) post.

**Note:** You'll need to use sudo for almost all the commands here.  Keep that in mind.

With Lets Encrypt making https actually free for the first time, there is a greater desire to use SSL than there used to be.  This blog post explains how to enable / disable the mod_rewrite aspect of Apache and also defines a simple rule for transforming all inbound requests to https from http:

To enable mod_rewrite:

&gt; a2enmod rewrite

Here's a [Stack Overflow on enabling mod_rewrite](http://stackoverflow.com/questions/869092/how-to-enable-mod-rewrite-for-apache-2-2).

To disable mod_rewrite:

&gt; a2dismod rewrite

The a2dismod is something I've never seen before.  Thanks to [James Lin](http://james.lin.net.nz/2010/07/28/ubuntu-apache-enabledisable-modrewrite/) who wrote about it on his blog.

A simple rule to transform http to https:

    RewriteEngine On
    RewriteCond %{HTTPS} off
    RewriteRule ^ https://%{HTTP_HOST}%{REQUEST_URI}
    
Here's the [Stack Overflow](http://stackoverflow.com/questions/16200501/http-to-https-apache-redirection) where I got this redirect from.
    
That needs to go into your vhost configuration.  After that you need to restart apache:

&gt; service apache2 restart

**Note:** I can’t stress this enough. The Apache directive AllowOverride all means that redirects will not work. Honestly I don’t even understand what AllowOverride all even does but I did confirm, experimentally, that if you have this, redirects simply will not work. So make sure that you’ve got this turned off.

To test your side after, assuming url foo.bar.com:

&gt; wget https://foo.bar.com/

-or-

&gt; curl -v http://foo.bar.com

Here's a [Stack Overflow](http://stackoverflow.com/questions/3252851/how-to-display-request-headers-with-command-line-curl) on curl and request headers that was helpful.

I always **strongly recommend** testing this kind of change with wget or curl because then you actually see the http header values.  Here's an example showing the http to https auto redirect with wget:

    wget http://foo.bar.com/
    --2017-02-03 15:06:25--  http://foo.bar.com/
    Resolving foo.bar.com... 23.23.164.47, 54.225.218.129, 2406:da00:ff00::36e1:da81, ...
    Connecting to foo.bar.com|23.23.164.47|:80... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    **Location: https://foo.bar.com/ [following]**
    --2017-02-03 15:06:25--  https://foo.bar.com/
    Connecting to foo.bar.com|23.23.164.47|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 2083 (2.0K) [text/html]
    Saving to: 'index.html.12'
</description>
        <pubDate>Fri, 03 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/apache/2017/02/03/apache2-and-mod-rewrite.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/apache/2017/02/03/apache2-and-mod-rewrite.html</guid>
        
        <category>apache</category>
        
        <category>mod_rewrite</category>
        
        
        <category>apache</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day rvm --default</title>
        <description>I normally don't intersect Ruby and Linux Tip of the Day but I just used this and it is really, really useful. The rvm or Ruby Version Manager tool manages your installed rubies makes ruby development, well, just plain easier.  

To install rvm on Linux, use:

    sudo gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3
    sudo \curl -sSL https://get.rvm.io | bash -s stable --ruby

To install on OSX, you can use the commands above but generally omit the sudo (depending on your system).

A real problem with RVM is you often don't know what Ruby you're going to get when you start a new terminal window.  You can solve this with:

    rvm --default use 2.3.1
    
You can find your available rubies with: 

    rvm list rubies
    
And then use one of the installed rubies as the argument to rvm --default use.


</description>
        <pubDate>Thu, 02 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/02/linux-tip-of-the-day-rvm-default.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/02/linux-tip-of-the-day-rvm-default.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>linux</category>
        
        <category>rvm</category>
        
        <category>linux_tip_of_the_day</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Gitlab and The Danger of Short Names</title>
        <description>So the other day we had the gitlab disaster.  As my friend Nick described it to me via chat: 

&gt; did you hear about gitlab?

&gt; an ops guy did rm -rf on a production db with 300GB of data by accident, and they lost 295GB of data -- luckily it was only on a backup, but they are expecting 6h of data loss for production data [Nick](http://nickjanetakis.com)

Or as Robin might say in a grim and gritty version: **Holy F'ing Hell Batman!**  

Apparently they were using short names like db1.foo.com and db2.foo.com.  Nick's comment on Hacker News was to the effect of their [naming structure was wrong](https://news.ycombinator.com/item?id=13539876) and that a name like dbproduction might have saved the day.  

Perhaps but I've seen a lot of this kind of stuff over the years.  Happily I've only ever had one such incident - I actually dropped a table once during a group pairing session with [Dv](http://dasari.com) and [Jared](http://www.alloycode.com/) due to getting confused between production and development.

The thing that I've wanted most of all to address this is **color**.  If you look at the structure of the brain we have an inordinate amount of our cortex devoted to visual processing -- why not approach that to preventing stupid terminal mistakes?  What I've wanted for years upon years is really simple - I want my terminal window to change color when I log into a remote server.  If its a staging server or a back up server give me a light gray background.  If its a production web or api server then give me a green background and if its a production database server then give me a BRIGHT RED BACKGROUND.

The iTerm2 badging feature can do a bit of this but its not in your face enough to really protect you.  I suspect that with sufficient console trickery this is possible but neither I or Dv ever managed to accomplish it.  Personally I think its important enough that I'd name it the GitLab feature and built it into iTerm but that's not my core skill set.  Still I'm putting it out there.  Thoughts?

Cross Links:

* [Official Incident Report](https://docs.google.com/document/d/1GCK53YDcBWQveod9kfzW-VCxIABGiryG7_z_6jHdVik/pub)
* [Hacker News Thread](https://news.ycombinator.com/item?id=13537052)

</description>
        <pubDate>Thu, 02 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/sysadmin/2017/02/02/gitlab-and-the-danger-of-short-names.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/sysadmin/2017/02/02/gitlab-and-the-danger-of-short-names.html</guid>
        
        <category>sysadmin</category>
        
        <category>devops</category>
        
        <category>gitlab</category>
        
        <category>iterm</category>
        
        
        <category>sysadmin</category>
        
      </item>
    
      <item>
        <title>Startup Learnings The Smartest Thing I've Ever Seen PayPal Do</title>
        <description>PayPal is one of those companies that we all love to hate.  They work with money so we all feel about PayPal the way we sort of naturally feel about banks -- mildly distrustful.  Also the tales of PayPal woes are legendary.  And then there are the login / password issues that seem to surround PayPal like a cloud of flies.  All of that has always made me think that PayPal got to where it is now by luck but I'm really starting to reconsider that.  In fact my current thinking is that we should all realize that PayPal is **smart**. And when I say smart, I am starting to think that PayPal is *Amazon Smart*.  

In order to explain this I have to disclose a personal preference of mine -- if I have a choice as to a software engineer's nationality, I always tend to lean British.  I've had remarkable success over the years in hiring British software engineers and, more than any other nationalities, the results have skewed dramatically positive.  I'm not sure whether it is the educational system, the way they refer to computers as 'kit', extensive reading of Dick Pountain when I was young causing me bias, the fact that all the engineers my age learned on either a Sinclair or a BBC Micro (systems so limited that they forced you to be good to survive) or something else but something about the U.K. seems to produce great software engineers.

And because I tend to lean British in my software engineer hiring, I tend to favor British Linux magazines.  Even though I live in the U.S. and the subscriptions are frightfully expensive, I still get print subscriptions to [Linux Format](https://www.linuxformat.com/) and [Linux User](http://www.filesilo.co.uk/LinuxUser/).  These magazines are **fantastic** -- they remind me of Byte Magazine back in its early 1980s hey day when every issue taught you something new.

Earlier this morning I had to set up a new subscription to Linux User and when my credit card failed due to some U.S. / U.K. thing, I opted to use PayPal instead and I got this:

![paypal_is_smart.png](/blog/assets/paypal_is_smart.png)

Note: This was new to me but perhaps PayPal has been doing it for years and I just never knew.  For some reason, however, this felt to me to be a new feature.

What PayPal is doing here is recognizing that non local currency transactions are a combination of **hard** and **scary** for people.  Most of us, at least Americans for certain, never really have a clue about a foreign currency transaction except we sometimes know &quot;Ok a Euro or a pound is roughly about a dollar&quot;.  And when prices are in something truly foreign like Yen, well, don't even ask us.  What PayPal is brilliantly doing here is recognizing this and saying to the user: 

&gt; I know this is hard for you so here's the actual transaction value and I'm going to actually charge you for the privilege of my doing a teeny, tiny bit of math (and taking some risk).

The fact that I can look at 80 pounds and be told that it is $103.58 gives me much more confidence in the transaction.  Even though I can read the screen and **know** that I'm being charged (quite a bit actually) for just knowing this, well, it certainly made me feel good.  And, yes, I took them up on it.  PayPal has seriously removed an impediment to commerce here.  

You should note that if you're not American, I suspect you're thinking something like &quot;Dude -- it is just an exchange rate, deal with it!&quot;.  And that is certainly true but Americans don't have to deal with exchange rates anywhere near as often as non-Americans.  America's geography is so vast that you can spend decades here without leaving America whereas most Europeans that I know think nothing of crossing national borders even over a weekend.

The lesson here for startups is that you can always make money by making hard or scary things easier for people.  The simple fact that right after buying this I have to file an expense for it means that I don't want to wait for a credit card statement.  By paying PayPal that additional 2.5%, I know exactly what I spent and I can file my expense report immediately.

Once again I'm going to repeat it: **PayPal is smart**.</description>
        <pubDate>Wed, 01 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2017/02/01/startup-learnings-the-smartest-thing-i-ve-ever-seen-paypal-do.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2017/02/01/startup-learnings-the-smartest-thing-i-ve-ever-seen-paypal-do.html</guid>
        
        <category>startup</category>
        
        <category>learnings</category>
        
        <category>paypal</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Linux Tip Of The Day - lsblk</title>
        <description>I suspect a lot of these tips are going to end up being about storage -- even today storage is such a pain point.  The lsblk command shows you what block storage devices are hooked up to your Linux system.  This is really useful for when you attach a new device and need to write a mount statement for /etc/fstab.  Personally I use this a lot when I'm using different AWS volumes for organizing my storage.

There's nothing to install -- lsblk appears to be a standard command for most *nix systems.

Here's an example of lsblk without any options:

![http://fuzzyblog.io/blog/assets/lsblk.png](http://fuzzyblog.io/blog/assets/lsblk.png)

Personally I find the -l option most useful:

![http://fuzzyblog.io/blog/assets/lsblk-l.png](http://fuzzyblog.io/blog/assets/lsblk-l.png)

Also useful is the -m option which shows permissions:

![http://fuzzyblog.io/blog/assets/lsblk-m.png](http://fuzzyblog.io/blog/assets/lsblk-m.png)

</description>
        <pubDate>Wed, 01 Feb 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/02/01/linux-tip-of-the-day-lsblk.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/02/01/linux-tip-of-the-day-lsblk.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>sys_admin</category>
        
        <category>aws</category>
        
        <category>linux_tip_of_the_day</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Working with Windows 10 Pro HyperVisor When You Are a Mac Guy</title>
        <description>So that [data center migration](http://fuzzyblog.io/blog/aws/2017/01/26/s3-ruby-api-programming-tip.html) that I mentioned recently has turned into not, well a disaster, but a bit of challenge.  The problem at hand was how to get data off three large MySQL databases, about 4 terabytes in size and then capture whatever else was desired and do it all in about 10 days to avoid getting hit with another $5 K data center monthly fee.

**Note**: If you want a hilarious tale of how this data actually got to the data center in the first place then I highly recommend [Anatomy of a Business Trip Gone Horribly Wrong](http://fuzzyblog.io/blog/humor/2014/08/21/anatomy-of-a-business-trip-gone-horribly-wrong.html).  It was awful at the time but it is funny as hell now.  It is also the last time I was in New York, pretty close to where the person who helped me on this actually lives.  Small, small world.

All of this data was in Ubuntu virtual machines apparently hosted under Windows Server Hyper-V.  Why any hosting company worth their salt would ever opt to run a *nix based operating system under the Windows Hyper-V, well, I don't understand.  Sigh.  I'll come back to this if I have the mental energy to engage in a rant.

My initial approach was to roll a custom backup solution which took a prioritized list of databases and tables and them dumped the data using mysqldump, split the data with the unix split tool and then gzipped each split file.  Finally it uploaded it to S3, deleting its in process work files as it goes.  Yes this sounds awfully convoluted but rolling your own means that you actually understand what it is in and isn't doing -- something critical when you are know the original hardware is going away forever.  And since I didn't have much in the way of free disc space, this could not be done all at once.  

Using this approach we transferred over 400 gigs of data to S3.  Given that this was gzip compressed data without the MySQL index overhead, which takes up a huge amount of the raw disc space, we got a sizable percentage of the data.  And, yes, that's why I blogged about [bmon](http://fuzzyblog.io/blog/linux/2017/01/29/linux-tip-of-the-day-use-bmon-for-bandwidth-monitoring.html) recently -- I was monitoring these transfers.

Naturally, by the power of Murphy's Law, I wasn't able to get it all.  So this called for mechanism #2 -- hard drives.  I delivered to the data center several three terabyte drives with instructions to the effect of &quot;Get these mounted and I'll do the rest&quot;.  My plan was to just use ansible to recursively copy the directory structures onto the hard drives, get the drives back and then go on my merry way.  Remember -- all I wanted was the data.

Sadly the hosting company was never able to get these hard drives to show up in the Ubuntu virtual machine that is running under the Windows Hyper-V.  I don't understand why but since I wasn't physically in the data center, I couldn't do much about it.  With time running out the hosting company suggested &quot;Why don't we just copy the virtual machines onto the hard drives and then you can have your exact environment and do what you want&quot;.  

That sounds great, doesn't it?  I mean who wouldn't want their old servers back, right?  Well, **me** actually.  Once again, what I wanted was **my data**.  I didn't want the virtual machines but since I have them, here's how I went about using them.

My thanks go out to [Nick](http://www.nickjanetakis.com/blog/) who walked me thru large parts of this and edited this documentation.  I'm a *nix guy who last used Windows XP in 2004.  The experience was, well, illuminating to say the least.  I'm sure that I will return to this in a future blog post.

Here were the initial steps that I had to follow to get ready to use these virtual machines.

1.  **Buy a Windows laptop**.  This was the first step.  I took a trip to the local [Frys](http://www.frys.com/) and managed to find a refurbished HP laptop for only $500.  The key thing here is that you need Windows 10 Pro.  If you don't have Windows 10 Pro you don't have the Hyper-V tools.  And since Windows 10 Pro is about $200, well, it really is cheaper to buy it installed.
2.  **Set the BIOS Options to Enable Virtualization**.  Every BIOS is different so I can't tell you where / how to do this.  I had to press F10 during the boot process and navigate thru all the options until I found the virtualization options and turned them on.  If you don't do this then the HyperVisor itself won't start and you won't be able to run virtual machines.
3.  **Install the HyperV Manager in Windows**.   We enabled hyper-v by searching for &quot;windows features&quot; and then turned it on.
4.  **Connect the USB Drive / Drives**.  This was just a plug and play operation so piece of cake.
5.  **Label the Physical Drives**.  This is optional but since I have 26 virtual machines spread across three physical hard drives so I used my label maker to indicate which virtual machines are on which hard drive.

Once I had the hard drives attached, the next step was to actually deal with the virtual machines.  Even though I had actual .vhdx virtual machine files, for some bizarre reason, I was unable to use them directly.  Instead I had to recreate the virtual machine from scratch, yes, 26 times, and then attach the .vhdx file as a drive.  This process was something like this:

Note: I say &quot;something like this&quot; because I'm writing this from memory after a long, long day.

1.  In the HyperVisor Manager, start by creating a virtual network interface.  This needs to be your very **first** step because without it you can't attach the .vhdx files -- you wil die on errors like &quot;Hyper-V encountered an error while configuring the network on New Virtual Machine&quot;.  This should be called NIC#1 or at least this is what I needed.
2.  Create a new virtual machine and walk thru the steps and in the last step browse to the .vhdx file and select it.  
3.  But if your vhdx file is actually in two parts then you don't want to browse to the .vhdx file at all.  Instead create the virtual machine and then edit its settings after the fact and attach the multiple .vhdx files at that stage.
4.  Once your .vhdx files are attached as per #2 or #3 then you can actually make use of them.  Remember that these really are virtual machines and, well, you still have some hoops to jump through.  The first step is to turn on the virtual machine.  
5.  After the virtual machine is running you then need to connect to it.  And this, dear reader, is where you will be confronted by a login prompt -- and you will realize that you've been using SSH keys to login for over two years now and you have **no idea** what the password is. 
6.  A quick email back to the data center staff will elicit information like &quot;well it might be this username and this password or it might be this other username and this other password&quot;.  
7.  Here's where I'm going to argue against rational security practices and say use your label maker again and attach the different username / password combos physically to each backup hard drive.  Trust me -- when I need this data for real sometime in the next six months I'm going to be wondering where the heck that text message with the details disappeared to.
8.  Once you've resolved the username and password info you can actually get into a database server virtual machine and verify that, well, it appears to be actual data.  I mean the directories appear to be there and the data appears to be real.  Huzzah!
9.  You **sudo su** to root and the do a **passwd actual_username** and reset that password to something insecure but easy (again these are virtual machines on hard drives on a development laptop; its ok).  
9.  And then the grim spectre of reality strikes -- you go and start MySQL and you get this wonderful kick to the nuts: &quot;The partition with /var/lib/mysql is too full&quot;.

And that's where I stopped.  As best I can tell I actually have the data and the virtual machines but I don't know if the data is actually valid.  My guess is that verifying this is likely about a full week of repetitive, manual, unscriptable work.  

Here are some screenshots I captured along with way.  And yes, because I don't have the equivalent of command+shift+4 on OSX, I used my phone.  Alas.

## Windows Hardware On My Desk

![hyperv_hardware.jpg](/blog/assets/hyperv_hardware.jpg)

## The Error when You Don't have a Virtual Network Adapter

![hyperv_error.jpg](/blog/assets/hyperv_error.jpg)

## The HyperV Manager

![hyperv_manager.jpg](/blog/assets/hyperv_manager.jpg)

## An Ubuntu Virtual Machine Running Under HyperV

![hyperv_mounted_filesystem.jpg](/blog/assets/hyperv_mounted_filesystem.jpg)

</description>
        <pubDate>Tue, 31 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/devops/2017/01/31/working-with-windows-10-pro-hypervisor-when-you-are-a-mac-guy.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/devops/2017/01/31/working-with-windows-10-pro-hypervisor-when-you-are-a-mac-guy.html</guid>
        
        <category>devops</category>
        
        <category>sysadmin</category>
        
        <category>windows</category>
        
        
        <category>devops</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day inotifywait</title>
        <description>My podcast listening, which has always been on the nerdy side, has recently taken a trip down the far, far nerdier rabbit hole.  Whereas I used to listen to things with at least some connection to pop culture such as [Major Spoilers](http://majorspoilers.com/category/podcast/), I now find myself listening to the [Ubuntu Podcast](http://ubuntupodcast.org/), a British podcast covering the Ubuntu community.  It was there that I learned about inotifywait and I had a feeling it was brilliant.  I tried it out and it is brilliant!

To install on Linux, use:

&gt; sudo apt-get install inotify-tools

Since inotify is a Linux api this doesn't exist for OSX but you can use an open source tool called notifywait available [here](https://github.com/ggreer/fsevents-tools).  Alternatively there is fswatch which can be easily installed with **brew fswatch**.  Please note that I didn't try either.

The inotifywait utility takes a few command line parameters and then tells you all file accesses that go on in that directory.  This sounds a bit obscure but here's how I tested it last night: 

&gt; inotifywait ~ -mr

This tells inotifywait to run on the ~ or home directory and monitor it forever (the m flag) and r tells it to monitor recursively on all directories below the current.

Here's an example of its output once I set it monitoring in one shell window on a server after I logged into the same box in a different shell window and then touched the file foo.txt:

![inotifywait.png](/blog/assets/inotifywait.png)

The really interesting here is that while I sort of know that the bash prompt initialization pattern is .profile and then  .bashrc but I've never **seen** it before.  I have a feeling that I'll understand this better from now on.

Not only is this useful for sysadmin / devops work but I can see it being highly useful for application developers. Sometimes the files you are touching or your io access patterns are not always apparent.  Using inotifywait makes them obvious.

Different events can even be monitored with the --e flag.  Here's what you can monitor for:

    Events:
    	access		file or directory contents were read
    	modify		file or directory contents were written
    	attrib		file or directory attributes changed
    	close_write	file or directory closed, after being opened in
    	           	writable mode
    	close_nowrite	file or directory closed, after being opened in
    	           	read-only mode
    	close		file or directory closed, regardless of read/write mode
    	open		file or directory opened
    	moved_to	file or directory moved to watched directory
    	moved_from	file or directory moved from watched directory
    	move		file or directory moved to or from watched directory
    	create		file or directory created within watched directory
    	delete		file or directory deleted within watched directory
    	delete_self	file or directory was deleted
    	unmount		file system containing file or directory unmounted


</description>
        <pubDate>Tue, 31 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/01/31/linux-tip-of-the-day-inotifywait.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/01/31/linux-tip-of-the-day-inotifywait.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>git</category>
        
        <category>sys_admin</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day - git recall</title>
        <description>Every so often your boss asks you what you've been working on and if you have been all over a code base it can actually be hard to break it down.  The new git recall command actually makes this pretty easy.  This is a new Linux tool I just learned about - [Docs](https://github.com/Fakerr/git-recall) / [Hacker News Discussion](https://news.ycombinator.com/item?id=13517486).

Install it on Linux as follows:

&gt; npm install --global git-recall

Install it on OSX as follows:

&gt; brew install less

-or- you might need to do:

&gt; brew install homebrew/dupes/less

**Note**: I know you already have less installed on your system.  You need lesskey which git recall relies on but Apple, unhelpfully, [did not include](http://apple.stackexchange.com/questions/27269/is-less1-missing-lesskey-functionality).

Once you have lesskey then you need to install git recall itself:

&gt; npm install --global git-recall

After you change into a directory where you're working on something git, do this:

&gt; git recall

The initial view shows you your commits in a recent time frame:

![git_recall_.png](/blog/assets/git_recall1.png)

You can go up and down the list and select a commit that you're interested in with the arrow keys and enter or j/k and enter.  I had issues with the arrow keys so keep j/k in mind.  Then you'll see the commit details:

![git_recall_.png](/blog/assets/git_recall2.png)
</description>
        <pubDate>Mon, 30 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/01/30/linux-tip-of-the-day-git-recall.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/01/30/linux-tip-of-the-day-git-recall.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>git</category>
        
        <category>linux_tip_of_the_day</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>20 Days of Zero</title>
        <description>Note: This really is about weight loss and I'll even show much much I lost but I'm looking at this from a startup context.

I generally find people's second act in the startup world far, far more interesting than people's first act.  By second act, of course, I am referring to when an entrepreneur tries to replicate the lightning in a bottle experience that they initially had when they built a startup.  Second acts seem to almost always be less successful than people's first act.  Here are some examples:

* [Jeff Hawkins](https://en.wikipedia.org/wiki/Jeff_Hawkins) who built Palm and then Handspring into a multi billion success is now working on [Numenta](https://en.wikipedia.org/wiki/Numenta) software that mimic human memory -- and he's been at it for more than a decade
* Charles Ferguson who built Vermeer, the original creators of FrontPage, left high tech entirely and is now a [film maker](https://en.wikipedia.org/wiki/Charles_Ferguson_(filmmaker))
* [Marc Andreessen](https://en.wikipedia.org/wiki/Marc_Andreessen) who created Netscape then created Opsware and is now a Venture capitalist
* [Kevin Rose](https://en.wikipedia.org/wiki/Kevin_Rose) founded Digg which although it is much forgotten these days was once upon a time beating Reddit in the social space.  He then did Pownce and is now doing an app called Zero for weight loss

And that, dear reader, brings us to Kevin Rose and [Zero](https://medium.com/@kevinrose/introducing-zero-a-new-app-to-help-you-fast-209935e8245d).  Zero is a new app for what is termed Intermittent Fasting (IF) which is a technique where you fast for a long stretch of the day on an ongoing basis.  Here's what the app looks like:

![Zero App](https://cdn-images-1.medium.com/max/873/1*C9hnNf_51CYUoTl_MFBeYA.png)

I've now been using Zero since 1/7 so I have 20 days on it which I think is enough to make some comments.  On 1/7 I weighed in at 194 which isn't an all time high for me but does reflect, well, post Christmas and Holiday weight gain.  I certainly wasn't fat by any means but I felt larger than I wanted to be.  After showing my wife Zero we made the joint decision to go on Zero.  My wife suffers from [Celiac disease](https://en.wikipedia.org/wiki/Coeliac_disease) in a fairly bad way and by the after Christmas period she was in pretty much constant stomach pain.  Zero, at the time, seemed like a fairly rational decision.

Here's my weight metrics, unsophisticated as they are:

![weight_metrics.jpg](/blog/assets/weight_metrics.jpg)

Note: This was originally written late last week which is why the day count is slightly different.  I've lost about another pound since this was written but it wasn't worth updating the picture.

Since I started Zero I have lost almost 10 pounds which for a 49 year old man is, I think, fairly good.  The only real goal I had at the start of this was a Gandalf like approach to buying new jeans.  When I found out, post Christmas, that my jeans didn't fit all that well, I took the position:

&gt; Thou Shalt Not Buy Bigger Pants! (Gandalf to a group of fat hobbits)

I am happy to report that no new jeans have been purchased and their fit is much better.

# Adding Whole 30 to Zero

I cannot say that all of this is tied to Zero.  On 1/15 one of my wife's good friends, H, convinced her to go on [Whole 30](http://whole30.com/) and I came along for the chaos.  Whole30 is one of those diets where you: 

* drink no alcohol
* no sugar
* no preservatives
* no dairy
* no corn
* no carbs
* eat only things you cook yourself
* eat only meats and vegetables

Given that I cook everything from scratch already this basically amounted to slight modifications to what I already cook.  I've never drank so the alcohol restrictions amounted to &quot;don't put wine in the beef stew made yesterday&quot; and didn't affect me at all. 

So while I can't say that all of this is due to Zero, I think the bulk of the weight loss is attributable to the Zero's Intermittent Fasting approach.  By reducing the window of time during which you can eat down to say 7 to 8 hours, my wife and I have gone from two large meals (lunch and dinner) generally down to 1 (lunch) and a snack (dinner).  So from a weight loss perspective, I suspect the reduction in overall food volume is a bigger contributor than the foods I've taken out of my diet.

Overall I've been fairly pleased with Whole 30 as a approach to food.  My wife's constant stomach pains are gone so that's a huge win from my perspective.  That was why I agreed to go on it and it has been a complete success in my opinion.

Unfortunately the Whole 30  [cookbook](https://www.amazon.com/gp/product/0544854411?ie=UTF8&amp;tag=whole3009c-20&amp;camp=1789&amp;linkCode=xm2&amp;creativeASIN=0544854411) isn't all that good.  The recipes fall into that difficult, pretentious category of recipe where my comment would generally be &quot;trying too hard&quot;.  Additionally some of the recipes are flat out either bad or wrong (don't try to make mayonaise their way; go get [Kenji's book](https://www.amazon.com/Food-Lab-Cooking-Through-Science/dp/0393081087/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1485516745&amp;sr=1-1&amp;keywords=kenji+lopez+alt)) or follow [the Pioneer Woman](http://thepioneerwoman.com/food-and-friends/how-to-make-mayonnaise/).

# Comments on the Zero App

The Zero app is very much no nonsense -- you get a calendar and a few different options for scheduling your fasts and that's it.  And that's my biggest problem with it -- there's no way to add your own data to the fast.  I would have very much liked it if say you could attach a picture of each meal you ate to each day.  Or if you could put in a weight metric.  Or if you could put in how you felt.  I'd love to know how many days of Zero it took for my wife to feel better but I have no idea.

Still Zero led to me losing 10 pounds and that's nothing to disregard.

You do get some summary metrics -- I can see my average fast time was 16h 49m and that on average I ate for only 6 minutes during the night (I was using the Circadian approach to fasting).

# Getting Zero

Zero can be gotten from the [App Store](https://itunes.apple.com/us/app/zero-fasting-tracker/id1168348542?mt=8).

# Getting Whole30

My recommendation would be to just get the baseline [book](https://www.amazon.com/Whole30-30-Day-Guide-Health-Freedom/dp/0544609719) and then ignore their whole philosophy and just focus on the food aspect of it.  Diets, these days, generally seem to be as much guru advice as they are diet and, honestly, I doubt you need a guru.  I know I don't.

# Overall

Zero is ridiculously simple to use does make it easy to stick with a fast.  Pairing Zero with Whole30 did seem to produce, at least for me, overall weight loss and, I think, some health gains.  Recommended.</description>
        <pubDate>Mon, 30 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/weight_loss/2017/01/30/20-days-of-zero.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/weight_loss/2017/01/30/20-days-of-zero.html</guid>
        
        <category>diet</category>
        
        <category>weight_loss</category>
        
        <category>startup</category>
        
        <category>kevin_rose</category>
        
        
        <category>weight_loss</category>
        
      </item>
    
      <item>
        <title>Linux Tip of the Day - Use bmon for Bandwidth Monitoring</title>
        <description>I recently was trying to upload several terabytes of data over to AWS' S3 file storage.  Given that inbound transfers to S3 are actually free, this wasn't as pricey as it might seem.  The transfer seemed slow so [Nick](http://nickjanetakis.com/blog/) suggested using Bandwidth Monitoring software, an obvious thing, but something I didn't even know existed.  The tool Nick recommended was bmon.

Install it as follows:

&gt; sudo apt-get install bmon

Start it as follows:

&gt; bmon

Here's what you can see:

![bmon.png](/blog/assets/bmon.png)</description>
        <pubDate>Sun, 29 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/linux/2017/01/29/linux-tip-of-the-day-use-bmon-for-bandwidth-monitoring.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/linux/2017/01/29/linux-tip-of-the-day-use-bmon-for-bandwidth-monitoring.html</guid>
        
        <category>linux_tip_of_the_day</category>
        
        <category>linux</category>
        
        <category>sysadmin</category>
        
        <category>aws</category>
        
        
        <category>linux</category>
        
      </item>
    
      <item>
        <title>Choosing Account Software For Your Freelance Life</title>
        <description>I'm in the process of gearing up for a return to the freelance lifestyle.  One thing I've never done, since I started as a real freelancer, is actually use accounting software.  I know, I know, I know -- bad decision.  Well accounting and taxes are among my least favorite things in life so that's why I've justified it but that's kind of being a child about it.  Ignoring something does not make it better so one of my 2017 resolutions was &quot;Deal with Accounting / Taxes&quot;. 

# Requirements

Here are my requirements for an accounting tool for freelancers in 2017:

* cloud based / hosted
* ability to bill clients
* ability to record an invoice but NOT actually send it (I don't necessarily want to lost X% to credit card processing)
* mobile app (ios)
* ability to make an invoice
* ability to record a client
* time tracking is possibly desirable but unclear

# Options

I looked at three options: 

* Quicken
* Hurdlr
* FreshBooks

![accounting_software.png](http://fuzzyblog.io/blog/assets/accounting_software.png)

## QuickBooks

Like, I suspect every other entrepreneur, I've used Quick Books in the past so it was my first stop along this path.  Sadly, for what should be such a strong product, I rejected it almost immediately.  The version I was examining was the very low end, $10 / month version, [Quick Books Self Employed](https://quickbooks.intuit.com/var/pricing/) - left most option.  Here's what I found wrong with it:

* As a web product it is incredibly slow (to amuse myself, I tried holding my breath during page refreshes; sometimes I wasn't able to)
* It is very, very limited. For example there wasn't a way to cleanly track clients.
* It felt like it had been designed poorly at best.  My guess is that the product manager has never been a freelancer him or herself and it really, really shows.
* Just the fact that googling for QuickBooks self employed takes me to a [page where it isn't even listed](https://search2.quickbooks.com/get-quickbooks?gclid=Cj0KEQiAwrbEBRDqxqzMsrTGmogBEiQAeSE6ZQUoreXe38AFn6v2IcG49NYFmZBf527SNNUwB5lqGRsaAo5t8P8HAQ&amp;s_kwcid=AL!2419!3!147086503826!p!!g!!quicken&amp;ef_id=WIkw5QAABM0ZEIG4:20170129112821:s) tells me that my business isn't a priority for Intuit
* It is strong on the traditional tax side of things and that's a plus but account software should help me run my business not just my taxes.

## Hurdlr

I found [Hurdlr](https://hurdlr.com/) somewhere and I'm intrigued by the promise and absolutely terrified by the reality.  This is a brand new app / website seemingly and they're not even charging yet.  Sure I can use it for free -- and what happens when they fail?  Can I get my data back?  Can I trust them with passwords to my bank account?  Will they stay in business?  They do a good job of talking the talk but I'm not going thru this selection process again.  I hate to not risk my efforts on a startup given my predilections for startups but this is just too risky.

## FreshBooks

Surprisingly to me I ended up picking [FreshBooks](https://www.freshbooks.com/) as my choice.  Here's how it matched up against my criteria:

* cloud based / hosted - **YES**
* ability to bill clients - **YES**
* ability to record an invoice but NOT actually send it - **YES**
* mobile app (ios) - **YES**.  Please note that you want the Cloud Accounting app.
* ability to make an invoice - **YES**
* ability to record a client - **YES**
* time tracking is possibly desirable but unclear - **YES**

I've had one problem so far - expense import brought in far more than I wanted but Support helped me out and did a great job.

I have the app on my phone; my wife has it on her phone and for the first time since I've been a freelancer, I have a heads up view as to the money that's coming in.  Honestly it feels fantastic.

![freshbooks_dashboard.png](/blog/assets/freshbooks_dashboard.png)

Note: Actual number above removed.

# Further Reading

While FreshBooks was right for me, you may need more information.  I found very good info on the [CarefulCents.com](http://www.carefulcents.com/) site.

</description>
        <pubDate>Sun, 29 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/freelance/2017/01/29/choosing-account-software-for-your-freelance-life.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/freelance/2017/01/29/choosing-account-software-for-your-freelance-life.html</guid>
        
        <category>startup</category>
        
        <category>freelance</category>
        
        <category>accounting</category>
        
        <category>quicken</category>
        
        <category>hurdlr</category>
        
        <category>freshbooks</category>
        
        
        <category>freelance</category>
        
      </item>
    
      <item>
        <title>A Guideline for Working with SSL Certs You Get From Someone Else</title>
        <description>I think I have a new rule for ssl and it is really, really simple:

&gt; If you have to do anything with SSL and the data involved came from someone else then they have to give me an exact, documented process, command lines and all or I'm out!

SSL is one of those things that is supposed to be, well, not easy but it shouldn't be mind numbing either.

I cannot count the number of times that people have said to me what amounts to: &quot;Here's a cert; just make it work&quot;.  It never works cleanly and you know what -- **No**.  Just as an example, I was recently given a &quot;cert&quot; and a &quot;key&quot; and asked to plug them into an AWS Elastic Load Balancer (ELB).  When I dug more deeply into it, I found: 

* The cert file actually contained 2 certs.  Sigh.
* Neither cert worked with the ELB
* Dramatically different errors came up with each cert in the AWS Certificate Manager

**Note:** I'm not even saying that this was someone else's fault.  The sys admin I got these from is both **smart** and **great at his job**.  What I am saying is that without comprehensive knowledge of SSL, I shouldn't put time into SSL tasks without full documentation.

I think at this point that I'm **done** taking action on SSL without specific, written steps.  I'm happy to follow someone else's directions but the 58 million results for &quot;ssl configuration errors&quot; or &quot;ssl aws configuration errors&quot; is, honestly, just too much to deal with.

![google_ssl_errors](/blog/assets/ssl_configuration_errors.png)

![google_ssl_errors](/blog/assets/ssl_aws_configuration_errors.png)</description>
        <pubDate>Sat, 28 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ssl/2017/01/28/a-guideline-for-doing-with-ssl-certs-you-get-from-someone-else.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ssl/2017/01/28/a-guideline-for-doing-with-ssl-certs-you-get-from-someone-else.html</guid>
        
        <category>ssl</category>
        
        <category>sysadmin</category>
        
        <category>devops</category>
        
        <category>aws</category>
        
        <category>life_is_too_short</category>
        
        
        <category>ssl</category>
        
      </item>
    
      <item>
        <title>S3 Ruby Api Programming Tip</title>
        <description>![ruby_aws_s3.png](/blog/assets/ruby_aws_s3.png)

So I find myself dealing with a backup to a data center that I am decommissioning and while the ultimate backup solution is going to involve hard drives onto which about 6 terabytes of MySQL tables are stored, I want to get at least a sizable portion of this into the cloud as soon as possible.

Given my [public advocacy position](http://fuzzyblog.io/blog/aws.html) in favor of AWS, S3 is clearly the cloud storage that I'm going to use.  I've spent the better part of the past two weeks writing a backup tool, with Nick's able assistance, that can:

* take a large database
* decompose it into tables
* dump each table
* split the tables into manageable size using split
* gzip those split files
* move those split files up to s3
* delete its working data as it goes since we only have a fraction of the disc space needed to do this in large (about 10% of the 3 tb for our largest database)

Its currently standing at 570 lines of moderately elegant ruby code and its been an interesting exercise in really getting to know how to use the aws-sdk gem and its S3 functionality.

Here's a quick overview of the steps in using the S3 gem with Ruby under Linux. You should note that all of this has been done in the context of a command line tool under straight ruby.  I suspect its directly portable to Rails but I haven't tested that.

1.  Start by installing the AWS cli with: **sudo apt-get install awscli**
2.  Configure your credentials with: **aws configure**.  This writes a ~/.aws/credentials file that the ruby gem knows how to read.  And that's brilliant because the credentials then are NOT stored in your code and can't be leaked through a repo becoming public.
3.  Since you're missing Rails and the dynamic include structure, you have to do load the gem with these two statements: **gem 'aws-sdk'; require 'aws-sdk'; **
4.  You have to create an s3 instance with a command like this: **@s3 = Aws::S3::Resource.new(region:'us-west-2')**

Here is the official documentation on the [s3 instance](http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html).

After that you can call methods like this:

&gt; obj = @s3.bucket(bucket).object(name_in_s3)

As long as the variables bucket and name_in_s3 are set this will return back to you an instance that you can work with.  The mistake that I made was trying to get the size of an object with .content_length when the underlying data had not yet been transferred up to S3.  I was testing the existence of the object with:

&gt; if obj

after I had created the object and I assumed that it was ensuring that the object already existed.  In fact what I needed to do was this:

&gt; remote_size = obj.content_length if obj.exists?

The reason for this is apparent if you *think* about it.  You have to build the object in order to transfer data to it so you're going to have an actual object before you even transfer the data.  This reminds me so, very very vividly of [Spolsky on Leaky Abstractions](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/) and his 2002 rant on why you don't abstract long running network operations and try and make them be exactly the same as fast running local operations.  </description>
        <pubDate>Thu, 26 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2017/01/26/s3-ruby-api-programming-tip.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2017/01/26/s3-ruby-api-programming-tip.html</guid>
        
        <category>aws</category>
        
        <category>ruby</category>
        
        <category>s3</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Rails Tutorial - Making Font Awesome Work with Rails 5</title>
        <description>Like a lot of back end folks I suspect I'm not alone when I look at a project like Font Awesome and go &quot;Great Work; damn if I know how to actually use that.&quot;  I'd normally toss it over the wall to a front end guy like my buddy [Dv](http://dv2.dasari.me) but this is for my side project so I'm all alone on this one.  Since I really, really don't feel that I understand the world of front end work and CSS / SCSS very well I thought I'd write it all down for you (and myself).

# Here's what not to do

I took a number of wrong steps:

* I watched this [video](https://www.youtube.com/watch?v=RC_jIGABW-E) and it is really good -- except for the specific steps he gave me didn't work.  I really don't understand why and it might have been me.
* I used the [font-awesome-sass gem](https://github.com/FortAwesome/font-awesome-sass) and it too failed me hard.
* Don't follow the Stack Overflow post [here](http://stackoverflow.com/questions/37581599/rails-assets-is-having-issues-with-my-fonts).

In all of the above cases I got the error **No route matches [GET] &quot;/assets/fontawesome-webfont.ttf&quot;** and I just couldn't get past it.  The Stack Overflow post might have ultimately gotten me there but it did take me to a different gem -- font-awesome-rails discussed in the next section.

You should note that I don't meant to malign any of the urls in the above material -- I'm not a css / front end guy by any means.  All I did was some simple googles and the processes I was taken through ultimately didn't work and I blew over an hour of development time to find that out.

# Here's What To Do

What you actually need is to do the following:

1.  Install the [font-awesome-rails](https://github.com/bokmann/font-awesome-rails) gem.  Just add **gem &quot;font-awesome-rails&quot;** to your Gemfile and then do the bundle install dance.
2.  In your application.css.scss file add the line: **@import &quot;font-awesome&quot;;**
3.  Stop and start your rails server.
4.  Add to a test .html.erb view file this line**: fa_icon &quot;camera-retro&quot; ** (you'll probably need to enclose that in output tags using %= and angle braces)
5.  Render that test .html.erb view file in your browser and you should see a camera icon.  What this is doing behind the scenes is outputting this html: &lt;i class=&quot;fa fa-camera-retro&quot;&gt;&lt;/i&gt;

# Useful Resources

Here are some important urls:

* [The Icon Search Page for Font Awesome](http://fontawesome.io/icons/) - Use this to search for the icon you need.  This is the most useful page on their site.
* [Font Awesome Home Page](http://fontawesome.io/)
* [Font Awesome Rails Gem](https://github.com/bokmann/font-awesome-rails) - 
* [The Font Awesome Github Project](https://github.com/FortAwesome/Font-Awesome)
* [The Cheat Sheet](http://fontawesome.io/cheatsheet/)
* [Useful Example of Font Awesome at the HTML Level](http://stackoverflow.com/questions/12468359/using-font-awesome-icon-for-bullet-points-with-a-single-list-item-element)
* [Example of Using the Search Icon](http://fontawesome.io/icon/search/)
* [The General Examples Page](http://fontawesome.io/examples/)
* [The Getting Started Page](http://fontawesome.io/get-started/)
* [Another Way to Search for an Icon](http://glyphsearch.com/?query=x)
</description>
        <pubDate>Tue, 24 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/24/rails-tutorial-making-font-awesome-work-with-rails-5.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/24/rails-tutorial-making-font-awesome-work-with-rails-5.html</guid>
        
        <category>tutorial</category>
        
        <category>font_awesome</category>
        
        <category>hyde</category>
        
        <category>rails</category>
        
        <category>css</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>How to Handle Your Personal Taxes when You Are Startup Folk</title>
        <description>Please note the following: 

* This is a U.S. centric post; I don't know how taxes work anywhere but here
* This post assumes that you have an accountant to help you 
* This post documents what I personally had to go through on 1/23/17 to get my personal deductions together and I wrote it, honestly, because next year I'll have to go thru the same process and this is the first time since 1987 that I feel good about the process I used.
* I am not an account and you should really talk to your own accountant.  This is the process that I have followed for years and since this year I actually did it well, I thought it was worth sharing.  Taking tax advice from a stranger on the Internet, well, your mileage may vary (YMMV).

When you fall into that vague category of *startup folk* -- entrepreneurs, founding engineers, etc -- organizing your personal taxes is a bloody disaster.  I spend money all year long that should generally fall into the category of business deductions.  Normally I don't pay all that much attention and just treat it as a loss.  This year I've got some additional tax concerns so I actually got serious and really **itemized hard**.  Here's how I did it.

# What You Need

Here is what you need:

1.  Access to every single email account that you used where a receipt might accumulate
2.  Access to all the online services that don't always send receipts
3.  File folders
4.  Paper
5.  Pen
6.  Time
7.  Willpower; this is an incredibly draining process
8.  A printer

# Reminder - Use the gmail and Amazon Print View

When you are printing stuff from gmail you should use the printer icon to get a print view.  This renders much better when it is available and uses less paper.  There is also a specific print view for Amazon orders that is better than the default.

# A Brief Tax Rant

Taxes are my least favorite thing in the whole wide world.  I find the U.S. tax code byzantine in its complexity and entirely unfair.  As I see it, the core principles of taxation are: 

* Everyone should contribute
* Taxes should be understandable by everyone since they are imposed on everyone
* Every dollar should flow through the tax mill once and only once

The U.S. tax code violates every one of these core taxation concepts.  Now, that said, it is important to note that while I don't like it, I also believe in being 100% honest and I never want to be audited so the trick here is meticulous documentation.  There is nothing wrong with a $2.31 AWS bill being deductible -- but you have to document it.  That's why #8 above is a printer -- I made physical paper copies of every single expense.  

# Step 1: Create a Physical Artifact For Each Expense Category

Taxes are important because if you screw them up, the tax man can make your life a living hell.  My father has been audited, every year, for over a decade.  And nothing ever happens to him but, to my mind, that's flying far too close to the sun.  And when something is as important as taxes then you handle it with a physical artifact because physical artifacts last.

My first step was to create a representation of each category.  In my case that was a single sheet of white printer paper with the category name on it.  So I had a stack of pages like this: 

* Hardware
* Software
* Office Supplies
* Online Services
* Conferences 

# Step 2: Write Down What You Remember

The next step is that you want to write down on each representation what you remember as big ticket items.  So, for example, I wrote down these:

* Hardware
  
  * Intel NUC
  * MacBook Pro
  * Dell Widescreen Monitor
  
* Software

  * Ruby Motion
  
* Conferences

  * Elixir Con
  
I suspect that these few expenses covered maybe 70% of my year.  And by writing them down I'm at least guaranteeing that I'll look for them.  When you're a member of the startup tribe, well, life is fast paced and it is easy to forget about things.

# Step 3: Start with Email

Take your first email account and run a search for the keyword **receipt**.  You might also, if you use gmail, want to look at your purchases label that Google sets up for you.  You will find that this process is exhausting because you are likely going to look at hundreds of different emails.  As you find each receipt then:

* Print it out
* Put it in the right category
* Add it to a tally sheet or a spreadsheet

Here is the key idea -- **nothing is too small**.  If you bought a $10 USB stick and it was actually for business yes, well, then you should deduct it.

You need to do this for every single email account.

# Step 4: Access Every Ongoing Online Service

In the era of cloud services, we all have a frightening amount of monthly fees.  What you want to do here is log into each one and get a print out of either each receipt or the main receipt index.  Here's an example from the outstandingly good GoRails site:

![go_rails_invoicing.png](/blog/assets/go_rails_invoicing.png)

On your Online Services page you want to add a entry for &quot;Go Rails&quot; (or whatever) and then a total ($51) and a date (2016).  You need to do this for every single online service.

# Step 5: Review Every Single Thing from Amazon

This is the big, hard one.  Amazon does a brilliant job of making your order information available to you and that is excellent but you still have to go thru it yourself and remember &quot;Ok that hard drive -- was that for my kid's Xbox or is it on my desk?&quot;.  But, if you are anything like me, you'll find that there are a lot of small business purchases lumped in with your personal purchases.  

If you use Amazon a lot then you'll find that you have a large number of orders to sort thru.  I had **41** pages of orders to sort through for the year of 2016.

# Step 6: AWS

I didn't find many, if any, receipts for AWS in my email so I logged into my AWS console separately and went thru that manually.

# Step 7: Think Hard and Iterate Again

The final step here is actually the hardest -- you need to think and think hard about what you did over an entire year and what might actually be tax deductible.  It is almost certain that you will have forgotten something so think long and hard.  If you come up with something then follow the process above.

# Step 8: The Result!

At the end of the process here's what the floor of my office looked like:

![tax_hell.jpg](/blog/assets/tax_hell.jpg)

All I have to do now is summarize the tally sheet on top and give it to my accountant.</description>
        <pubDate>Tue, 24 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2017/01/24/how-to-handle-your-personal-taxes-when-you-are-startup-folk.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2017/01/24/how-to-handle-your-personal-taxes-when-you-are-startup-folk.html</guid>
        
        <category>startup</category>
        
        <category>taxes</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>If I Was a Traveling Musician</title>
        <description>My friend David Rovics, on his blog and email newsletter, [laments the loss of CDs as a revenue stream for the traveling musician](http://songwritersnotebook.blogspot.com/2017/01/why-do-they-keep-begging.html).  I'm not a traveling musician but I suspect that he is absolutely right.  As I gaze across my desk of computing hardware:

![my_desk.jpg](/blog/assets/my_desk.jpg)

there is only 1 device capable of playing CDs and it is a 2010 era MacBook Pro (it is actually in a stand behind everything so even tho it can play CDs, I can't reach the slot easily).  

If you're at all interested in the indie music scene / folk music / songs of protest then [David](http://songwritersnotebook.blogspot.com/) is a fascinating read.  

**Disclaimer**: David is a friend and he and I grew up together.  We don't share a wit of political ideology - he is as far left as it gets and I'm a staunch startup / technology believer but America was built on people being allowed to disagree and it all making us stronger.  If I want to be able to believe what I hold dear then he **has** to be able to believe as he chooses.  In these divisive times it feels like this has been forgotten.

Anyway, David is making the point that CDs don't sell like they used to and that makes his touring much, much harder since a key revenue source has diminished.  He terms it a post-CD reality and, yep -- that makes sense.  People aren't buying the discs, I suspect, because outside of the car they don't have a way to play them.  Why should you buy them if you can't play them?  And I know the feeling because I belong to [David's Community Supported Art (CSA)](http://www.davidrovics.com/subscribe/) program and I just got a batch of CDs recently.  And I know that I put them right in the car.  But even there, they **may not** get played - most of my music in the car comes from either a USB stick or XM Radio.

**Note**: I think that a touring musician offering a physical artifact like a CD for sale is a great thing.  People do spend money for physical artifacts to support an artist but I suspect that they won't spend money when they flat out can't use it at all.  Just for example, there's only 1 other CD player in our whole house.

# Sidebar: The Miracle of Things Being 10x Easier

One of the tenets of the technology world that I've always believed is that when you make something literally *10x easier to use* then it is a f*undamentally different* thing.  As an example, I've recently been experimenting quite a bit with the [Amazon Echo](https://www.amazon.com/Amazon-Echo-Bluetooth-Speaker-with-WiFi-Alexa/dp/B00X4WHP5E) / Alexa service.  My test case for it is actually playing music and even though I'm not really a music person, with Alexa, the simple fact that: 

* I don't have to mess with my phone
* I don't have to fight with BlueTooth pairing
* I don't have to screw with iTunes
* I don't have to plug in a cable
* I don't have to pick from a library of thousands of tracks
* I can say &quot;Alexa play the doors&quot;

makes a music playing experience that is literally **10x easier than a CD** (CDs get lost, damaged, don't always have metadata, need to be imported into iTunes, etc).

# So What is Easier than a CD?

The answer here to my mind is simple -- **a USB stick** dramatically improves on the listening experience:

* It works in almost any computer out of the box (with new Macintoshes it needs a dongle)
* Even a cheap 4 gig USB stick can hold tons of music
* There is an advertising / promotion opportunity to lower your costs by allowing other artists music to be on there
* Songs can be easily dragged into iTunes
* Metadata and album art could actually be correct

The problem with a USB stick is that it is **static** and indie musicians are constantly making new music -- even I know that's part of the attraction.  A low volume artist like David doesn't want to invest in say 100 usb sticks and then find that they are out of date.

And that brings us to the next section, My Proposal. 

# My Proposal for Touring Musicians

I think what touring musicians should sell is a USB stick which has the following features:

* A Small Music Archive like a Best of David Rovics album
* A program which updates the USB stick from the Internet
* A readme file that basically amounts to &quot;Run the update program&quot;

The update program would be a small executable that connects to a source on the Internet and downloads a full archive complete with album art and metadata.  People could the plug that into their computer, take that to their car, etc.  

You could call this *USB Stick as Platform* perhaps.  The software could easily be written in JavaScript using the [Electron](http://electron.atom.io/) toolkit.  If [Slack](https://slack.com/downloads/) can ship versions of their app for OSX, Windows and Linux off the same code base and [Atom](http://www.atom.io) can do the same then there should be no problem developing a pretty simple downloader.  All it has to do is:

* Read a config file which determines the source url to read from
* Start downloading songs
* Store the songs in a rational file hierarchy 
* Display a progress bar

If you wanted to you could get fancy like embed an encrypted token in the config file that limits how many downloads but I wouldn't recommend it.  I think Simple is better here.  I would allow for it pulling down some advertising style content since that would give a musician the ability to promote an upcoming concert, cross promote another artist, etc.

The end result of this is that the musician could give to a manufacturer like [FlashBay](http://www.flashbay.com/usb-music-distribution) a master USB stick and get back say 100.  I don't know the prices on them but if you promoted it properly like &quot;David Rovics USB Library&quot; and it could perpetually update itself then you'd have something that actually might have  dramatically higher price than a single CD.  You'd need to test pricing but my gut says that a USB stick of this nature could easily fetch $20 or more.  

# A Final Note

In my last [post](http://fuzzyblog.io/blog/rant/2016/12/20/david-rovics-community-supported-art-or-a-tale-of-paywoe.html) that referenced David, I failed to define some terms: 

* UI - user interface -- the look, feel and interactions a user has with a product
* UX - user experience -- the overall experience that a user has with a given product
* UI/UX - the fusion of the two

My apologies for leaving these out.  As a card carrying member of the technology tribe, I do exist in a world of acronyms and I sometimes don't realize that they are not clear to others.</description>
        <pubDate>Mon, 23 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/music/2017/01/23/if-i-was-a-traveling-musician.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/music/2017/01/23/if-i-was-a-traveling-musician.html</guid>
        
        <category>music</category>
        
        <category>random</category>
        
        <category>echo</category>
        
        <category>alexa</category>
        
        <category>10x</category>
        
        <category>david_rovics</category>
        
        
        <category>music</category>
        
      </item>
    
      <item>
        <title>Tmux Mouse Mode Problems Under Linux</title>
        <description>I am in the process of decommissioning a data center I set up about 2 years ago.  This was done in my pre-ansible days when I used Chef to bootstrap all the servers and it only functioned about 90% of the way leaving the final configuration of each server entirely manual.  This has meant that rather than each server being identical to every other server, each one is a unique &quot;snowflake&quot;.  And snowflake servers can really be the bane of your existence at times.  

One box, for example, when you run a [Tmux](https://tmux.github.io/) session it mysteriously trashes the mouse and you can't actually copy some bit of text for pasting it later.  This doesn't sound all that significant until you are trying to remember a mysqldump statement that is, with all options, about 220 characters long, it is 3:07 am and you find yourself saying &quot;Man this is just the #U$I#U$#I balls&quot;.  

And that's when you realize that even if you are turning these boxes off in the next 15 minutes it would still be worth fixing immediately.

The problem was in the file ~/.tmux.conf and it was the presence of these lines:

    setw -g mode-mouse on
    set -g mouse-select-pane off
    set -g mouse-resize-pane off
    set -g mouse-select-window off

To be honest this is a problem that I likely introduced myself - I vaguely remember noodling on this configuration file about 18 months ago and then something blew up.  A quick :wq in VIM and I was off to the next disaster.  And since I didn't restart tmux entirely (configuration is loaded only on startup), it wasn't until the box was physically rebooted, long after I had forgotten this particular hell, that the mouse died and I was left scratching my head.  Sigh.

Had I read it at the time, this [blog post](http://tangledhelix.com/blog/2012/07/16/tmux-and-mouse-mode/) might have been useful.</description>
        <pubDate>Fri, 20 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/tmux/2017/01/20/tmux-mouse-mode-problems-under-linux.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/tmux/2017/01/20/tmux-mouse-mode-problems-under-linux.html</guid>
        
        <category>tmux</category>
        
        <category>linux</category>
        
        <category>sys_admin</category>
        
        
        <category>tmux</category>
        
      </item>
    
      <item>
        <title>Building a Real Parser in Ruby Using Parslet</title>
        <description>I deal with &quot;parsing&quot; all the time and I generally do it with one or more regular expressions.  And while I know that JWZ is famous for this statement:

&gt; Some people, when confronted with a problem, think &quot;I know, I'll use regular expressions.&quot; Now they have two problems.  alt.religion.emacs (lost; recovered) [JWZ](https://en.wikiquote.org/wiki/Jamie_Zawinski)

Regular expressions provide a uniquely powerful way to deal with day.  What the don't do well though is handle problems when you arbitrary complexity / recursion in your structure.  Let's say you are trying to parse: 

* &quot;name like '%Warning%'&quot;
* &quot;name like '%Warning%' AND name not like '%noob%'&quot;
* &quot;name like '%Warning%' AND name not like '%blah%' AND name like '%bar%'&quot; 

You see where this is going -- I have an arbitrary SQL like syntax and while I could craft a specialized regex which tried to deal with the max cases and then back off, at best that would be a hack.  At worst it would be code that I'd be ashamed to show Mother Johnson and that would be sad.  This calls for -- wait for it -- a real **parser**.

And, yes, I am talking about the stuff we all learned in compilers class aka [The Dragon Book](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811).  If, like me, you didn't actually take a compilers class then let me explain.  The Dragon Book, for as long as [I can remember](https://en.wikipedia.org/wiki/Dragon_Book), has been the source of how you write compilers, parsers, etc.  Wikipedia cites it back to 1977 and that's what I mean by &quot;as long as I can remember&quot;.  I know my first startup partner, Brian, used it in college circa 1988 and it has been in constant use, well, forever.

The stuff taught in the Dragon Book are techniques that either you use constantly because they neatly correspond to how you think or they are things you trot out once a decade or so when the problem is that severe.  In my case I last did this stuff in 1996 when I used it to write an HTML parser in Visual Basic.  That time I did it with a Lex / Yacc / Bison grammar using this [O'Reilly book](https://www.amazon.com/lex-yacc-Doug-Brown-ebook/dp/B009THDEBC) to assist me.

![http://kschiess.github.io/parslet/images/parsley_logo.png](http://kschiess.github.io/parslet/images/parsley_logo.png)

One of the beautiful things about computers as a profession is that if you've ever done something then there's a near certainty that it will loop back on you.  This time around I am tackling it with Ruby and the [Parslet](https://github.com/kschiess/parslet) gem.  I've been following Parslet on the mailing list since at least 2014 because I know just how important parsing is and [Kaspar Schiess](https://github.com/kschiess) has done fantastic work on it over the years.

Developing a parser with a tool like Parslet is not something you can just sit down and hack out.  This isn't that kind of gem -- I know, I tried it that way.  This is a:

* roll up your sleeves
* watch a video
* work thru an example
* scratch your head
* drink coffee
* re-watch the video
* finally get it
* extend the example

type of thing.  And that honestly is the exact process I used.  [Dv](http://www.dasari.me) and I worked thru it together and we couldn't make any progress until we sat down and did it this way.  Yes we were able to cobble together a simple grammar but we just couldn't put all the pieces together without this level of understanding.  But, I can promise you this, once you do actually understand how a tool like Parslet works then it is fundamentally transformative for you.  

In our case we started with Calle Erlandson's Recursive Descent Parsers talk available on [YouTube](https://youtu.be/_F-eh66zw90) and [github](https://github.com/calleerlandsson/recursive-descent-parsers-talk).  This is a fantastic video where he walks you through the development of an infix parser for a calculator i.e. &quot;3 + 4 * 2&quot; should be give a result if 11 (8 + 3) because you have to evaluate it mathematically not left to right.  Here is my repository where Dv and I extended Calle's example to include [division](https://github.com/fuzzygroup/recursive-descent-parsers-talk).  Yes I even sent a pull request over to him to perhaps get division into the master.

I know I wrote a lot of words and didn't even talk about parsing SQL like expressions.  That's because priorities got shifted and, just after getting ourselves ready to tackle the real project, it was a case of &quot;And now please do this instead&quot;.  Sigh.  I'm sure I'll come back to this topic but really do watch the video -- it is fantastic.

</description>
        <pubDate>Fri, 20 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/01/20/building-a-real-parser-in-ruby-using-parslet.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/01/20/building-a-real-parser-in-ruby-using-parslet.html</guid>
        
        <category>ruby</category>
        
        <category>parsing</category>
        
        <category>software_engineering</category>
        
        <category>parslet</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Why I Didn't Look at Phusion's Union Station But You Should</title>
        <description>Of all the companies that populate the Ruby and Rails world, I think my absolute favorite is [Phusion](http://www.phusion.nl).  I was early into the Rails space -- 2007 so I date back to the days of Mongrel based rails installations and when [Passenger](https://www.phusionpassenger.com/) from Phusion came out, it was a revelation.  Running a rails application went from a &quot;How the F do I do this&quot; to &quot;I'll use Passenger and it will just work&quot;.  And Hong Lai is to me a stellar example of how to support an Open Source product.  I cannot say how many times I've been researching something and, damn, there's Hong Lai answering stuff.  The man is a damn machine and I mean that in the very best of ways.  Once upon a time I was a Ruby Enterprise Edition customer for Passenger and other times I've been a paid support customer.  And it is all because of the quality of Passenger and the tireless work of Hong Lai.  Every single time I go to deploy a Rails app, I reach for Passenger -- it is just that good.

Phusion has a new product, [Union Station](https://www.unionstationapp.com/):

![https://www.unionstationapp.com/images/dashboard.png](https://www.unionstationapp.com/images/dashboard.png)

(Stock image; I never got it running for my apps; alas.)

And it is squarely in my sweet spot of tools: 

* For ruby / rails 
* Deals with performance / scalability
* Is NOT New Relic
* Is priced reasonably / rationally

Unfortunately I am unlikely to look at Union Station and there is a lesson here about **software marketing** that I think is illustrative.  The first thing to understand is that the person handling my account, **Tinco**, has been absolutely excellent - he has been helpful, timely and I can't say anything bad about him as a sales person -- which rare for me.  The problem here is that Phusion wants me to make a decision with only 1 to 2 weeks of time.  This is a product that I'm going to need to evaluate at a pretty deep level to make a buying decision and I refuse to rush that kind of thing.  Even just understanding performance tools is hard and takes quite a bit of time.  What happened to me is:

* I requested the evaluation when I had free time to deal with it
* There was some kind of glitch where my account wouldn't activate
* By the time my account was fixed, there was some other higher priority problem that I had to deal with
* Now that I have the time, my account is expired

My problem is with time limited evaluations in general -- people these days are **busy**.  And time limited evaluations, particularly for complex products, are hard to get around to.  This is particularly true for a product where real data needs to be gathered and you might not have an immediate use case for it.  In our case we do intensive processing under Passenger only about once per month.  So with a week long evaluation period, the timing just didn't work out.  

Yes I can ask for more time and I'm sure that Tinco would grant it; that's not the point.  This is a general problem with selling complex products to busy people with limited time; I don't think I'm alone in this.

Now, with all that said, companies do need to close sales leads -- I get that.  I don't know if there is a solution to this problem.  My personal preference would be usage based or capacity based i.e. I'd like to be able to install Union Station and use it for say 100 times in order to make a decision but that, while good for me, is directly against their interests.

Overall I'm impressed with Union Station and I suspect if you're in the Rails space you should look at it.  The pricing model looks particularly rational which, having been burned by New Relic in years past, appeals to me.  Personally I'm going to wait until I have a more pressing front end project in mind but that's coming up soon for me.


</description>
        <pubDate>Wed, 18 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/18/why-i-didn-t-look-at-phusion-s-union-station-but-you-should.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/18/why-i-didn-t-look-at-phusion-s-union-station-but-you-should.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>startup</category>
        
        <category>marketing</category>
        
        <category>oobe</category>
        
        <category>hyde</category>
        
        <category>phusion</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Nylas Mail Review</title>
        <description>Nylas N1 is a new email client that has just been released as free and open source.  And, as a piece of software, it is also actually **good**.  Among people that know me in the real world, I am notorious for handling personal email poorly if at all and I actually used it for quite a while last night.  Now that's a great sign for a new app.

![nylas.png](/blog/assets/nylas.png)

# Overall

I actually **really like** this application and I'm intending to use it to see if perhaps, in 2017, I can put on my big boy pants and deal with email as a professional (i.e. daily). The fact that it works for OSX, Windows and Linux is a huge, huge win.  The UI is impressively good and they've open sourced it which means that it has potential to get better -- fast.  

**Note**: I just tried to install it under Linux and its apparently not fully read yet.  Alas, sigh.

There is some impressively good thinking here 

Now, that said, I'm still impressed.  I'll be even more impressed when I can run it on my wide screen Linux desktop with 32 gigs of memory.

Kudos to the good folks at Nylas and ***Thank You*** -- you just made not only the personal computing world a little bit richer but also the open source world.  

# The Bad - Memory Usage

Nylas is an Electron app and that means that memory consumption is going to be an issue.  Now I'm old school -- [my first computer](https://en.wikipedia.org/wiki/TRS-80) had 4K in it and I've known people who owned things like the [KIM-1](https://en.wikipedia.org/wiki/KIM-1).  Sure I moved it to 48K as soon as I could but I started small.  That's why when I see an application like Nylas and how it uses memory it makes me want to [weep and then die](https://www.youtube.com/watch?v=SZGEiz2cpEw).  

Here's the memory usage after I had used it for about a half hour, 979 megs:

![nylas_memory_12_hours_earlier.png](/blog/assets/nylas_memory_12_hours_earlier.png)

As an additional test I left Nylas running all night long and the next morning it had eaten another 100 megs of RAM:

![nylas_memory_12_hours_earlier.png](/blog/assets/nylas_memory_12_hours_earlier.png)

It is important to understand that this isn't all that different from Slack which clocked in at 1.68 gigs.  Here's my Slack memory usage after multiple days up and running:

![slack_after_multiple_days.png](/blog/assets/slack_after_multiple_days.png)

# Performance

As I write this, I'm seeing very bad performance which is surprising because last night it was actually pretty peppy.  My guess is that with a release yesterday the backend servers which drive Nylas are likely under heavy load right now so even thought my second time experience isn't all that great, I'm going to give Nylas a pass on performance right now.

# The Bad - UI Glitches

Note: I mostly reviewed Nylas using the two panel theme.  Some of the problems I experienced you might not see at all if you use the one panel theme.

I did notice a few screen related UI rendering glitches around selection of non-contiguous mails in a list view.  Here's an example (5 selected but only 2 shown as selected).

![nylas_selection_problems.png](/blog/assets/nylas_selection_problems.png)

One of the key Nylas features is the ability to display contextual information related to the emailer and that's excellent but the app really calls for a very portait style, wide screen monitor.  I've seen it get to point where the contextual information was clipping off the right hand side of the message.  Here's a screenshot that shows this experience but not the clipping:

![nylas_clipping.png](/blog/assets/nylas_clipping.png)

One very real concern that I have is that the Send button is way at the bottom of the screen so if you have a long email there's a lot of scrolling just to say &quot;Ok&quot; or &quot;Can we reschedule?&quot;.

Some smaller things:

* I have Choosy installed on OSX which is supposed to intercept calls to Safari and let you pick which browser to open links in.  Nylas somehow gets around this and launches Safari even though I have 3 other browsers open.  Sigh.
* I have scroll bars set to always be displayed and Nylas doesn't respect that.  So I have to click into a content region just to get a scroll bar to scroll.

</description>
        <pubDate>Wed, 18 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/email/2017/01/18/nylas-mail-review.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/email/2017/01/18/nylas-mail-review.html</guid>
        
        <category>email</category>
        
        <category>nylas</category>
        
        <category>slack</category>
        
        
        <category>email</category>
        
      </item>
    
      <item>
        <title>Rails Tutorial - Making Awesome Print Work Everywhere</title>
        <description>I know that Pry is the new hotness for command line Ruby work in a repl -- but I've never felt comfortable with it.  I stick with the tried and true AwesomePrint.  I recently bootstrapped up two new boxes and I found that I've been using AwesomePrint so long that I don't actually remember how to make it work.  And that, dear reader, is how I write a post -- if I can't rememeber it then I need to write it down.  Here are the steps:

1.  You need awesome_print in your Gemfile so add **gem 'awesome_print' **.  I put it into development and test groups only generally but it is awful convenient in production (but you need to follow step 3 server side on ALL servers which sucks).
2.  Do the bundle install dance
3.  Create a file ~/.irbrc and add the following two lines:

    require &quot;awesome_print&quot;
    AwesomePrint.irb!
    
For more information of AwesomePrint, I've been going to [this web page](http://www.rubyinside.com/awesome_print-a-new-pretty-printer-for-your-ruby-objects-3208.html) for years now.
</description>
        <pubDate>Tue, 17 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/17/rails-tutorial-making-awesome-print-work-everywhere.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/17/rails-tutorial-making-awesome-print-work-everywhere.html</guid>
        
        <category>rails</category>
        
        <category>ruby</category>
        
        <category>awesome_print</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Ruby - How to Sort an Open Struct</title>
        <description>I am a huge, huge fan of the OpenStruct class in Ruby.  OpenStruct lets you build data structures that act like classes complete with dot notation.  Underlying OpenStruct is really a hash like structure with some semantic niceties that enable the dot methods.

I recently started building a fairly complex structure that represents a blog's metadata and I wanted my keys to be sorted not structured willy nilly.  A little bit of thought and looking at the methods that an OpenStruct instance responds to gave me this:

    b = OpenStruct.new
    # complicated routine here that builds the metadata
    b.to_h.sort.to_h
    
All this is doing is taking the OpenStruct, converting it to a hash and sorting it and converting it back to a hash -- easy peasy!
</description>
        <pubDate>Mon, 16 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/01/16/ruby-how-to-sort-an-open-struct.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/01/16/ruby-how-to-sort-an-open-struct.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>hyde</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>When rails g won't generate a Model</title>
        <description>I just saw about an hour of precious, early morning development time disappear because this failed:

    bundle exec rails g model TextCommon    

All that would happen was I'd get a cryptic:

&gt; Running via Spring preloader in process 19539

And then nothing.  No model file was generated, no spec file was generated.  

This was a newly created Rails 5 api application and I had already used rails g to create my first controller.   So I knew it was working -- I knew it.  Here was my trouble shooting process:

1.  I started my troubleshooting by stopping my Tmux session and trying without it -- **NOPE**.
2.  Being a cranky programmer who is often distrustful of newer things, I figured that spring might be the problem so I did a ps auwwx | grep hyde_api_ruby and found a truly disturbing number of processes.  I went through a number of p*kill -f hyde_api_ruby * gyrations (more on [pkill](https://fuzzygroup.github.io/blog/unix/2016/11/23/pkill-rocks.html)) and still no luck.  
3.  Next up was to try this on an El Capitan machine because I've had enough issues with Sierra to, well, think it sucks monkey chunks.  Nope.  Same issues.
4.  My next thought was to revert to my spring theory and disable spring and listen at the Gemfile level.  Apparently you can't do that because if you don't have listen then the rails command dies with an exception.
5.  So I added back listen but not spring and got another version of the same error.

At this point I thought to myself -- **wait** -- you generated this app with **--skip-active-record.**  Is that why model generation is failing?  So I tried rails g controller Foo as a test and that worked just fine.

The bottom line is that if you don't have persistent storage configured at the Rails level, rails g model just won't work.  That's a pity because:

* PORO (plain old ruby objects) are really, really nice 
* Storing them in app/models gives you the benefits of reload!
* The rails g process ensures that a spec file is generated correctly

I added this to the Spring issues tracker [here](https://github.com/rails/spring/issues/486).</description>
        <pubDate>Thu, 12 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/12/when-rails-g-won-t-generate-a-model.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/12/when-rails-g-won-t-generate-a-model.html</guid>
        
        <category>rails</category>
        
        <category>model</category>
        
        <category>spring</category>
        
        <category>listen</category>
        
        <category>hyde</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Electron Development is Like Mos Eisley</title>
        <description>So for my side project there is, for the first time in years, a client side component.  I already have 3 target users in mind and one of them uses Windows exclusively so that rules out any type of Mac development tool.  And because I really do believe that we are now in a [postmac](http://fuzzygroup.github.io/blog/category.html#postmac) world, I also need to support Linux.  All of that rolls up into the nigh inescapable conclusion that I'm going to have to use [Electron](http://electron.atom.io/) for development.

Given that the Atom editor and Slack are both based on Electron and run across OSX, Windows and Linux you really can't argue against Electron at this point.  Atom and Slack are both best of breed applications and my only objection to them is resource usage.  And arguing against resource usage is a sucker's bet -- you always, always, always build for tomorrow's platform because machines get faster (or at least stay the same) and memory gets larger.

The crying shame of electron development is the abysmal state of documentation and examples.  Honestly it is like [Mos Eisley](https://en.wikipedia.org/wiki/Mos_Eisley) -- a wretched hive of scum and villainy or really a **wretched hive of things that don't actually work *anymore***.  I've been working for the past two hours on nothing more than trying to follow top ranked tutorials and the only thing I've been able to make is a [Hello World](https://github.com/fuzzygroup/HelloWorldElectron01) app of my own. 

The core problem here is that the JavaScript / Node world which vomited all this technology onto us is changing more rapidly than anything I've seen before.  And when that happens people don't tend to go back and fix their samples.  And I get it, I really do.  But the taste that it leaves in your developer mind is, well, Mother Johnson told me never to use words like those.  ;-) 

I think my only reliable advice if you're trying to learn electron is the following:

1.  If you're trying to learn from a code sample on the net, git clone it to your machine IMMEDIATELY and try and run it with the normal electron . process from the checkout directory
2.  If it dies as seems to be the norm then abort and move on.
3.  Check the dates on sample apps carefully; older then a year and I wouldn't even try anymore.
4.  Personally I learn better from examples and I tend to avoid API documentation.  I think in this case I need to make a beeline for the [official electron docs](http://electron.atom.io/docs/).
5.  The docs are actually very good.  Because I'm trying to make a standard desktop app, I looked at the [Electron Menu](http://electron.atom.io/docs/api/menu/) docs and it was very helpful.  Integrating this into an overall app skeleton -- not helpful; sigh.  Getting the [menu to show up at all](http://stackoverflow.com/questions/34840026) in development -- entirely unhelpful.

</description>
        <pubDate>Wed, 11 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/electron/2017/01/11/electron-development-is-like-mos-eisley.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/electron/2017/01/11/electron-development-is-like-mos-eisley.html</guid>
        
        <category>javascript</category>
        
        <category>electron</category>
        
        <category>hyde</category>
        
        
        <category>electron</category>
        
      </item>
    
      <item>
        <title>How to Install Ruby 2.4 on OSX Using RVM</title>
        <description>Well Ruby 2.4 shipped back in December on the 12th and with almost a month of settle time that means that I now feel its go time for Ruby 2.4 and, specifically, using it on side projects.  I've already heard talk about Ruby 2.4.1 and the first mistake that I made installing it was to try and use that -- which isn't actually available yet.  

# What Ruby 2.4 Versions are Available?

The easiest way to see this is to travel over to [cache.ruby-lang.org](https://cache.ruby-lang.org/pub/ruby/2.4/) and just view them.

# Installing with RVM:

Here's the easy one liner to install with RVM:

    rvm install ruby-2.4.0
    
You may need to increase your timeout to get it to install:

    echo &quot;export rvm_max_time_flag=20&quot; &gt;&gt; ~/.rvmrc
    
# Other Useful RVM Commands

Like most open source projects, RVM is rich and deep.  Here are a few useful commands I used along with way of installing this (the command is in bold; the -- SOME TEXT gives an explanation):

* **rvm list rubies** -- shows you what rubies are on your system
* **rvm list known** -- shows you the rubies that rvm knows about (caveat - rvm can install more than it knows about; no I don't understand this either)
* **rvm --default use 2.4.0** -- set 2.4.0 as the default ruby when you open a terminal
* **rvm use 2.4.0** -- change the ruby in the current terminal to 2.4.0</description>
        <pubDate>Sun, 08 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2017/01/08/how-to-install-ruby-2-4-on-osx-using-rvm.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2017/01/08/how-to-install-ruby-2-4-on-osx-using-rvm.html</guid>
        
        <category>ruby</category>
        
        <category>rvm</category>
        
        <category>hyde</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>2016 Pop Culture Review</title>
        <description>If you're looking for a review of what happened in 2016's pop culture then I strongly recommend the Geek History Lesson's [look back at 2016](https://audioboom.com/posts/5448708-ghl-ep-143-best-of-2016).  They are doing an excellent review of 2016 with a number of surprising things that at least I missed.  **Recommended**

Here is their RSS feed if you want to subscribe: [Geek History Lesson RSS Feed](https://audioboom.com/channels/4864843.rss)</description>
        <pubDate>Sun, 08 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/pop_culture/2017/01/08/2016-pop-culture-review.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/pop_culture/2017/01/08/2016-pop-culture-review.html</guid>
        
        <category>pop_culture</category>
        
        
        <category>pop_culture</category>
        
      </item>
    
      <item>
        <title>Protecting Your Home from Water Damage</title>
        <description>I hate, hate, hate to be superstitious but at times it really does seem like bad things do come in groups -- I'd have said threes but there have been four occurrences.  The house I'm living in now, the one where I talked recently about [Debugging Christmas Light Wiring](https://fuzzygroup.github.io/blog/debugging/2016/12/01/debugging-christmas-lights.html), is one that I arrived in after my wife, kids and I were **flooded** out of our previous residence back in 2015.  Long story and the only relevant part about it is **water**.  So we  recover from the flood, buy this house and move in.  And then probably less than a month later, we find water pouring out of our basement furnace area due to a sump pump failure (see Tip 1).  About six months after that one of first floor toilets was clogged and we found water pouring thru the basement ceiling due to a broken wax gasket at the toilet base and an improperly plumbed run of line to the main waste feed.  And last Sunday we heard a noise behind our fridge when we got ice that turned out to be a pin hole leak in the water line feeding the fridge.  And, of course, it had been going on for some time so now we have a ruined piece of cabinetry.

Enough I say!  Enough!  If water is going to apparently be the *death of my home* then it is time to at least **try** and prevent it.

&gt; **Sidebar:** I've been a home owner most of my adult life and I grew up working in an old school hardware store so there isn't much in terms of home repair that I haven't done, sold or worked with.  I've seen first hand the damage that water does to a home.  I'm taking a break from my normal tech blogging because preventing this type of damage from happening is trivially easy with this approach.

# My Solution The Glentronics Basement Watchdog Water Sensor and Alarm

I started thinking about this as something I could build but it occurred to me that I cannot be the only angry home owner with this kind of problem so I hit Amazon and found this solution:

![Basement Watchdog](https://images-na.ssl-images-amazon.com/images/I/716GmOoFj7L._SL1280_.jpg)

This is a tiny plastic box with a base with two metal plates exposed.  Here's an [Amazon link](https://www.amazon.com/gp/product/B000JOK11K/ref=oh_aui_detailpage_o01_s00?ie=UTF8&amp;psc=1 &amp;tracking_id=fuzzyblog00-20) if you want to purchase.  If water bridges the two contacts then a connection is established and it goes off at 110 db -- loud enough that you should be able to hear it and take action.  Will it text me?  Nope.  Will it email me?  Nope.  But it costs about $12 on Amazon and has a battery rated for 5 years.  Additionally the base plate pulls out if you need to place it somewhere but you can't fit it.  And, because of those wires, if your fridge is very tall, like mine, you can extend the wires and locate it in a cabinet over the fridge like I did here:

![My Fridge with Alarm and Custom Wiring](https://fuzzygroup.github.io/blog/assets/my_fridge.jpg)

The arrow is pointing to the sensor box.  The long wiring harness is behind the fridge thru a 1/4&quot; hole I drilled in the cabinet.  If I had tried to drop this device behind the fridge itself it would have push the fridge out by about 1&quot; to 1 1/2&quot;.

I've now ordered enough of these to place under every sink and near every single toilet in my house.  Because we have a fully functional basement with a sink that gets little use it would be ridiculously easy for a water leak to get noticed after say 24 hours -- at which point massive damage has been done.

# Tip 1: Where to Place Your Water Detectors

I've chosen to place one of these under every sink, behind every toilet, behind the washing machine and in the furnace room where the water heater is located.  If you have kids then it isn't uncommon for a toilet to overflow and kids don't always want to fess up so better to be safe than sorry.

# Tip 2: Set A Calendar Reminder to Change All Your 9 Volt Batteries in the Water Detectors

These water detectors all have a 9 volt battery in them -- note that it isn't included so order them from Amazon if you don't have any.  The batteries are supposed to last for 5 years but after what I've been through, I'm choosing to set an annual calendar reminder so that every January 10th or so I'll be reminded.

# Tip 3: Add a Calendar Reminder to Your Spouse's Calendar Also

If you have a spouse then you might want to go full belt and suspenders on this and add a calendar reminder to their calendar as well just to be sure.  Again make this an annual reminder.

# Tip 4: Have You Checked Your Sump Pump Lately?

Apparently sump pumps are only rated for 10 years according the a plumber that I actually do trust.  I sold sump pumps for years in the hardware store and I don't think I ever actually knew that.  Given that I have seen them fail myself, I can attest that it is actually true.  If you have a sump pump then you might want to check the installation date.  Often times the person who installs it will note it on the pump itself.  Or there may be a manufacturing date on the pump which can be used as a proxy for installation time.

</description>
        <pubDate>Sat, 07 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/house/2017/01/07/protecting-your-home-from-water-damage.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/house/2017/01/07/protecting-your-home-from-water-damage.html</guid>
        
        <category>house</category>
        
        <category>hacking</category>
        
        <category>home_ownership</category>
        
        <category>water_damage</category>
        
        
        <category>house</category>
        
      </item>
    
      <item>
        <title>How Tech Companies Can Easily Create Jobs</title>
        <description>A few days ago Ross Mayfield wrote an absolutely compelling piece about jobs, tech companies and the original [Luddite movement](https://en.wikipedia.org/wiki/Luddite):

&gt; 50% of the jobs will be gone in ~20 years. Not from the great sucking sound of jobs to Mexico that can be stopped with a wall. Not from moving offshore to China. From automation that is moving quickly from blue collar manufacturing to white collar information work. Second only to climate change, this is the greatest disruption of our time, and I don’t mean that word in a good way. [Link](https://shift.newco.co/the-coming-tech-backlash-82b22e0c1198)

Years and years ago I actually knew Ross on a fairly good level and Ross is **smart**.  He's also not wrong about what tech companies are doing to the economy.  While I don't think the trends are as dire as he says, there is a real wild card and that's self driving technology that actually works.

If self driving technology works then we can easily see things like:

* Massive disruption of the trucking industry; [America's Number One Job](http://www.marketwatch.com/story/keep-on-truckin-in-a-majority-of-states-its-the-most-popular-job-2015-02-09)
* A move away from personal vehicle ownership towards transportation as a service and that in turn will nuke the car dealership and car repair industries, car detailing, etc.  
* This will absolutely devalue the car that you own today so it affects you in **your** pocketbook.

But self driving technology is a wild card and I'm less certain about it getting to the fully automated level than a lot of analysts.  We've all heard the statement that the last 5% of an engineering project can be as hard as the rest and we're not even at the 95% level yet.  So while self driving technology will get here, it is very unclear to me if it is 2020 or 2040.

&gt; **Sidebar**: Ross cites a number of frightening statistics from different research reports such as  *[50% of occupations today](http://www.cbre.com/research-and-reports/future-of-work) will be gone by 2020*.  The one thing to always understand about statistics from research reports is that large numbers **sell** research reports.  No one buys a research report that forecasts something low; low is boring.  I learned this the hard way back in 1988 when one of the leading firms, either Gartner or DataQuest, forecast that hypertext tools would be a $50 million business in 1990.  I was in the industry at the time and I knew everyone.  My own guess was that hypertext tools in 1990 was probably less than a $5 million business and that's probably generous.  So research firms can be wildly off and still make a ton of money on a research report

Now, that said, I don't think that most people would dispute that we have a jobs issue at present.  There are definitely ways that tech companies can actually create jobs and do it fairly easily:

1.  Actually hire support workers
2.  Avoid big acquisitions
3.  Reduce hours

Each of these is covered below.  None of these will be popular with tech companies because they do affect the bottom line.  But if we don't affect our own bottom line then the government or the people will do it for us -- and that's always worse.

# Actually Hire Support Workers

I do not think that I am the only person who feels that it is utterly unacceptable that you can never reach technical support at most of the leading tech companies -- Google, Facebook, PayPal, etc.  There is always some kind of problem that only a human can resolve.  The high tech industry used to understand this and understand it well.  I came of age in this field when [WordPerfect](https://en.wikipedia.org/wiki/WordPerfect) still ruled the desktop for word processing and WordPerfect had a near legendary approach to support that actually helped build its business.  Great support can truly build your brand in a way that a web page never will.

It certainly feels to me like the big tech companies could actually afford to hire support workers; none of these companies lack for profits.  And the world isn't the 1980s anymore.  You don't even have to staff a giant call center with full time workers -- this could all be done with a distributed work force that works from home on a part time basis to avoid the health care costs.  And perhaps these aren't great jobs but they are still jobs and that's what we are currently lacking.

# Avoid Big Acquisitions

Big acquisitions are often predicated on the concept that if you put two companies together you can increase profits by reducing common overhead.  For example you don't actually need two accounting departments.  And while not everyone in one of the accounting departments is let go, a substantial portion of them generally are.  Multiply that across the span of acquisition activity that seems to happen across the high tech industry in general and you have a lot of lost jobs.

Please note that I do actually recognize that this is a simplistic argument and there are lots and lots of reasons for acquisitions but large acquisitions generally seem to lead to net job losses.

# Reduce Hours

The concept of the 8 hour work day, 40 hour work week is fairly engrained in our culture but things weren't always this way.  The 8 hour work day originated as a political movement globally against employer abuses of workers and then became, in the U.S., part of the [Fair Labor Standards Act under Roosevelt's New Deal](https://en.wikipedia.org/wiki/Eight-hour_day#North_America). But is an 8 hour work day the only way that work can be done?  Could, for example, a company move to a 6 hour work day?  Or perhaps just move away from people working the 10 to 14 hour days that are oh so common to the high tech world?

I'd be really, really curious to know if a company formally moving to a six hour day resulted in hiring more staff or people simply being more efficient so they could enjoy those two extra hours they get back to their own life.

There's substantial economic research that people lose effectiveness beyond the 8 hour day anyway.  Some of those studies can be found [here](https://hn.algolia.com/?query=8%20hour%20day&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=all&amp;type=story).

**Side Thought**: Given all the problems that we see in tech recruiting, I wonder what the impact would be on a company if they announced &quot;We actually work a six hour day&quot;.  Might that be a compelling pitch to job candidates?
</description>
        <pubDate>Fri, 06 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2017/01/06/how-tech-companies-can-easily-create-jobs.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2017/01/06/how-tech-companies-can-easily-create-jobs.html</guid>
        
        <category>jobs</category>
        
        <category>economy</category>
        
        <category>ross_mayfield</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Scott's Rule of API Development</title>
        <description>So on Tuesday I wrote a new api that allows an [Ansible](https://www.ansible.com/) playbook to register an AWS instance with an internal service I'm writing.  This is for the purpose of monitoring large jobs (think tens if not hundreds of lightweight EC2 instances) so that they can be shut down programmatically when they are done.  

In some kind of alternate universe I'd just do all this with [AWS Lambda](https://aws.amazon.com/lambda/) but I'm working in a monolithic rails code base so I used my [Large Datasets approach](https://fuzzygroup.github.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html) I documented earlier this week.  

My ansible playbook creates the EC2 instances, binds the whole process together and initiates everything -- I can literally run one playbook and have 40 workers (or really N workers) created and an entire data set start processing along with getting an internal record of all the instances and the job so that I can do proper cost accounting, dynamic instance shut down and the whole nine yards.  This is a big step for me.  I am getting close to a technical goal that I've been working towards either for six months or about 7 years depending on how you measure it.

So [Nick](http://nickjanetaks.com/) and I did all this fancy, fancy EC2 automation -- AMI creation, instance creation, job launching -- and the thing that ***failed*** was a simple *http post API* call using the [Ansible URI module](http://docs.ansible.com/ansible/uri_module.html).  Like everyone else in the industry I've written a post API, I don't know -- 50 times? 100 times? --- and they always fail the first time.  And this brings me to what I'm going to immodestly call Scott's Rule of API Development:

&gt; **Scott's Rule of API Development**.  Always, always, always test your APIs with [curl](https://curl.haxx.se/) from an external box before you actually try and use them.

Seriously -- I hit this problem six months ago at [TheraChat](http://www.therachat.io) when I wrote their MVP.  I hit it yesterday.  I seemingly hit it every single damn time I make an API.  Hence Scott's Rule of API Development.  Now, that said, what does using curl to test an api actually mean / how do you do it:

1.  Create your API.  
2.  Write your curl statement. Here's a sample which I used once upon a time for an authorization API (the -d tells curl to post it): *curl -i -d &quot;api_key=forceawakens13928534aY&amp;&amp;mobile_number=317-531-4853&amp;password=BLAH&quot; &quot;http://app.foo.com/api/authorize&quot;*
3.  Test it locally against your development system.  Fix any bugs.  Lather / Rinse / Repeat.
4.  Deploy your code.
5.  Run it on another box that isn't your actual API to make sure that there are no security glitches / remote code issues / etc.
6.  If this API is part of a full stack Rails app then don't forget to disable forgery protection in Application Controller with *protect_from_forgery with: :null_session*.
7.  For bonus points store the curl commands that exercise your API as part of your code base and always try them before a release.  APIs can be tricky little buggers and they are both difficult to get and seem to break easily.  Regularly testing with curl prevents that and makes users of your API much, much happier.

Yes test coverage should prevent what's described in #7 but I'd strongly recommend belt and suspenders on this one.  When the people who rely on your API have issues you can always give them a curl statement and say &quot;Well this works for me -- what about you&quot;.  You can't do that we test coverage as APIs are often used from a variety of languages and environments but curl is absolutely **universal**.
</description>
        <pubDate>Thu, 05 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2017/01/05/scott-s-rule-of-api-development.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2017/01/05/scott-s-rule-of-api-development.html</guid>
        
        <category>software_engineering</category>
        
        <category>api</category>
        
        <category>curl</category>
        
        <category>ansible</category>
        
        <category>aws</category>
        
        <category>rails</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Returning to RSS Programming After 14 Years</title>
        <description>There is a very odd, very good feeling when you as a programmer return to some technical task that you used to do -- and you find that you do it **better**. For myself I am once again dabbling in RSS programming and it feels **great**.  It was almost 14 years ago to the day that I started Feedster based on an idea that [Dave](http://scripting.com/2003/03/09.html) put out there by linking to an idea that Dave Aiello  had.  

That one tiny mention on [Scripting.com](http://www.scripting.com/) inspired me to hack together a truly awful RSS search engine that in turn survived a slashdotting and then picked up a co-founder, a CEO, angel investment, a move to San Francisco, real venture funding and actual staff.  

It was, I believe, March of 2003 on a snowy night and now in January 2017 on another snowy night I am again writing feed processing code.  

The technically interesting part here is that the operations I did poorly back in 2003 are now flowing and gracefully from my fingers as I work.  Specifically I used to monitor feeds by doing a full fetch and then hashing the body with MD5 and comparing it to a stored value from the last time.  This time I *knew* &quot;Ok.  Do an HTTP head and look at just the header values&quot;.  Additionally I am using a far better toolset in 2017 (ruby / rails / elixir / phoenix) than I ever did back in 2003 (php).

If as an engineer you've never returned to a problem space after a long interval, I'd recommend it.  Just seeing how you tackle the same problem after a long break certainly makes you think.
</description>
        <pubDate>Thu, 05 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2017/01/05/returning-to-rss-programming-after-14-years.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2017/01/05/returning-to-rss-programming-after-14-years.html</guid>
        
        <category>hyde</category>
        
        <category>rss</category>
        
        <category>rails</category>
        
        <category>startup</category>
        
        <category>software_engineering</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>On the Changes at Medium</title>
        <description>So Ev has just announced major layoffs at Medium:

&gt; I’ll start with the hard part: As of today, we are reducing our team by about one third — eliminating 50 jobs, mostly in sales, support, and other business functions. We are also changing our business model to more directly drive the mission we set out on originally. [Renewing Medium](https://blog.medium.com/renewing-mediums-focus-98f374a960be#.kh7coodv8)

As someone who, on a vastly smaller scale, has been through this type of thing, I have nothing but sympathy for Ev.  Even when you are as monstrously successful as Ev (Blogger, Odeo, Twitter), layoffs always hurt.  You never hire people thinking that you are going to have to fire them.  You always hope that the people you hire will hire more people and so on.  

The [Hacker News discussion](https://news.ycombinator.com/item?id=13321322) on it has the normal nay sayers and people saying &quot;But it is just a blogging platform&quot;.  And, yes, Medium is a blogging platform but it is operating on a scale which is very, very hard to achieve.  The really interesting there is from the post by MG Siegler:

&gt; The numbers speak for themselves. 2 billion words written on Medium in the last year. 7.5 million posts during that time. 60 million monthly readers now.  [Long Medium](https://500ish.com/long-medium-b9ddfe2c3a0a#.wh8tzerfo)

Wow.  I had no idea that Medium was on that type of growth curve.  That is flat out astonishing.  When you consider the scaling issues at that type of size with the rich user interface that Medium offers and the integrated analytics, aggregation and notification, yes, I can see why they raised the amount of funding that they have.  

Now, that said, it does seem that they overstaffed at the business level.  And it seems that those are the positions being scrapped.  Interestingly at least some of the staff seem to be dealing with this well:

&gt; Newly ex-Median here. This was not a huge surprise. On the surface, this is a change in product strategy. The underlying story is the company positioning itself so it can survive an adverse environment if it needs to. It's hard to fault managers for dealing with that potential (and its hard to deny that the next 2-4 years could be really bad). Hopefully not, but it would be malpractice not to prepare. So better to focus resources now than be walking dead in a year or so, jettison unnecessary products/projects, and hope for the best. It's a great product, and with time and luck, they'll sort out a good business model, but like the rest of the publishing world, they're still sorting things out. Despite being one of those made redundant, I enjoyed being there, and wish them the best. On that note, you should ask yourself if you are prepared for winter, because winter is coming. [Permalink](https://news.ycombinator.com/item?id=13322380)
</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/medium/2017/01/04/on-the-changes-at-medium.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/medium/2017/01/04/on-the-changes-at-medium.html</guid>
        
        <category>medium</category>
        
        <category>blogging</category>
        
        <category>startup</category>
        
        
        <category>medium</category>
        
      </item>
    
      <item>
        <title>Capistrano Failure - Asset Manifest Not Created</title>
        <description>I setup a new Rails application using Capistrano earlier today and hit a fair number of odder than normal [Capistrano](http://capistranorb.com/) issues.  The biggest issue was around the asset pipeline and the asset manifest not being created.  The error message revolved around &quot;cannot stat (pathname) manifest file&quot;.  

Digging into the issue with Google revealed that it actually is a problem with Capistrano itself.  Here's the [thread](https://github.com/capistrano/rails/issues/111).  Upgrading to at least Capistrano 1.1.8 fixes this problem.
</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/04/capistrano-failure-asset-manifest-not-created.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/04/capistrano-failure-asset-manifest-not-created.html</guid>
        
        <category>rails</category>
        
        <category>capistrano</category>
        
        <category>asset_pipeline</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Ansible Unable to Find Boto Errors</title>
        <description>Over the past several days I have been doing quite a bit of work with the ansible [EC2](http://docs.ansible.com/ansible/ec2_module.html) and [AMI](http://docs.ansible.com/ansible/ec2_ami_module.html) modules for dynamically creating instances and AMIs on AWS.  Ansible, however, doesn't actually talk directly to AWS; it talks to AWS thru a python module named [**boto**](https://github.com/boto/boto).

There are a number of common problems that you might find when you the error &quot;boto required for this module&quot; or &quot;unable to find boto&quot;:

* Make sure boto is installed: *sudo pip install boto*
* Uninstall ansible entirely and then install it via Python: *sudo pip install ansible*
* Run your playbook using sudo to ensure that the version of python is the one that comes from sudo
* Eliminate *connection: local* at the playbook level and move it to the task level

**Note**: If you uninstall Ansible using HomeBrew or apt-get, you may find that your Galaxy roles have been uninstalled.  This can very badly impact your playbook execution so be careful.  If you want to avoid this then use the -p option when you install a role to specify that the role goes into a local directory of your choosing; [Stack Overflow Explanation](http://stackoverflow.com/questions/22201306/ansible-galaxy-roles-install-in-to-a-specific-directory).  

The last one of these, eliminating connection: local, requires a bit of explaining.  Details can be found [here](https://github.com/ansible/ansible/issues/15019).  When I first encountered this problem, my playbook looked like this:

    - hosts: &quot;fi_app_metadata_monthly&quot;
      become: yes
      remote_user: ubuntu
      connection: local
      vars:
        - redis_ami_id: &quot;ami-XXXX&quot;
        - redis_security_group_id: &quot;sg-YYY&quot;
        - redis_instance_type: &quot;t2.micro&quot;
        - redis_tag_name: &quot;dynamic_redis_fi_app_metadata_update&quot;
        - redis_instance_count: 1
        - template_instance_id: &quot;i-UUUUUUUU&quot; # in future this comes in from command line
        - number_of_instances: 3                      
        - instance_type: &quot;m3.large&quot;                   
        - region: &quot;us-west-2&quot;
        - vpc_subnet_id: &quot;subnet-IIIIII&quot;
        - vpc_id: &quot;vpc-RRRR&quot;
        - group_id: &quot;sg-YYYY&quot;
        - aws_access_key: &quot;ERRERERE&quot;
        - key_name: &quot;appdata_aws&quot;
        - aws_secret_key: &quot;ERERERE&quot;
        - farm_job_name: &quot;monthly_fi_app_metadata_update_2017-01&quot;
        - farmer_address: &quot;ec2-9-9-9-9.us-west-2.compute.amazonaws.com&quot;
        - tag_name: &quot;fi_app_metadata&quot;
        - rake_task: &quot;bundle exec rake fi_farm_work:fi_app_metadata_update --trace&quot;
      roles:
        - { role: ec2_make_redis_instance_from_ami, tags: ec2_make_redis_instance_from_ami}
    
When the above playbook was run, it resulted in boto not being found errors.  The solution was to remove the line *connection: local* from the playbook and move it to the task level:

    - name: make the redis instance from the ami
      # note - you used to be able to do this at the role level; now it has to be at the task level
      # note - think of this as you are setting the connection context for the task being executed
      connection: local
      ec2:
        aws_access_key: &quot;{{ aws_access_key }}&quot;
        aws_secret_key: &quot;{{ aws_secret_key }}&quot;
        region: &quot;{{ region }}&quot;
        image: &quot;{{ redis_ami_id }}&quot;
        ...

This is one of those code errors that, honestly, I just don't understand.  From talking with another Ansible buddy, [Nick](https://nickjanetakis.com/), he confirmed that it used to work at the playbook level.  Perhaps this is just one of those perplexing changes that results from other other architectural work going on at the project level.</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2017/01/04/ansible-unable-to-find-boto-errors.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2017/01/04/ansible-unable-to-find-boto-errors.html</guid>
        
        <category>ansible</category>
        
        <category>boto</category>
        
        <category>aws</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Tutorial How To Upgrade Your PS4 to 2 Terabytes of Storage</title>
        <description>There are times when it seems like we live in a weird world of storage capacity imbalance.  We see this in two places: 

* phones
* gaming consoles

For phones we see this when a 12 megapixel camera shoots 4K video but the manufacturer sells it with only 32 gigs of ram.  And for gaming consoles we see this when a manufacturer ships a device with only a 500 gb drive but every single game you play might be 10+ gigs or more in size -- and every game requires a full install + a multiple gig update.

My kids have a PS4 -- ok its actually mine but I've barely played it -- and having gotten them the new PSVR for Christmas, we are officially out of space.  I had to delete some installed games just to make enough space to get things installed.  Clearly this is madness.

My oldest son is an Xbox One gamer mostly and he has an external drive where he can just install all his extra games and it it works *beautifully*.  Unfortunately the PS4 **cannot** play games from an external drive and while this sucks green monkey chunks, it isn't as bad as you might think because Sony made upgrading the hard drive in a PS4 brilliantly simple.  

1.  You need a new 9 millimeter hard drive of at least bigger than your current.  I went with 2 tb.  Here's the [amazon link](https://www.amazon.com/gp/product/B00FRHTSK4/ref=oh_aui_detailpage_o01_s00?ie=UTF8&amp;psc=1) for about $80.  This is in a case so you can reuse the old drive in case you want to.  You also need a backup drive so you don't lose anything.
2.  Here are two youtube videos to watch: [Better](https://www.youtube.com/watch?v=fkIyCXbiGZs) and [ok](https://www.youtube.com/watch?v=YDQL0qvt3Qk). 
3.  Take a blank, USB FAT, FAT32 or EXFAT drive of at least the same size as the drive in your PS4 (this should be either 500 gigs or 1 tb).  Connect the drive to the PS4.
4.  Go into the PS4 settings and turn off all power down options so the PS4 stays on no matter what.
5.  Go into each PS4 account and sync all trophies with the PlayStation Network.
6.  Go into the PS4 settings and do a full backup.  This will take hours and hours.  For me it was like 6 hours.
7.  Open the case which is done by powering down the system and disconnecting all cables.  Slide the case open per the video above.  Unscrew the hard drive and add a new one.  Any 9 millimeter USB 3 hard drive should do.  
9.  After you try and power the device on you'll get a message on screen about &quot;ps4 update file for reinstallation&quot; which basically means *you have no operating system so go download one* from Sony.
10.  Take a USB stick with at least 1.5 gigs of free space on it and create a directory PS4 and then a directory PS4/UPDATE and place the PS4UPDATE.PUP file on it which you get from this [Sony PS4 download page]https://www.playstation.com/en-us/support/system-updates/ps4/.  Insert this into the PS4 and then power it on per their instructions holding down the power button for 7 seconds and it will prompt you to re-install the OS.
11.  Once the PS4 is actually running again then go back into settings and select backup / restore and now choose restore and the backup drive will be loaded back onto your PS4 and all games, achievements, trophies, etc will be available to you.
</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/gaming/2017/01/03/tutorial-how-to-upgrade-your-ps4-to-2-terabytes-of-storage.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/gaming/2017/01/03/tutorial-how-to-upgrade-your-ps4-to-2-terabytes-of-storage.html</guid>
        
        <category>tutorial</category>
        
        <category>ps4</category>
        
        <category>gaming</category>
        
        
        <category>gaming</category>
        
      </item>
    
      <item>
        <title>Recent PostMac Round Up</title>
        <description>Well a new year and people are still unhappy about the state of the Mac.  Here are some interesting links worth following:

* [Apple 2016 in Review](https://chuqui.com/2017/01/apples-2016-in-review/) [HN Commentary](https://news.ycombinator.com/item?id=13307040)  His point on Apple relying on data too much is likely correct.
* [Regular Restarts on OSX](https://news.ycombinator.com/item?id=13307648)  Yep.  Right there with you.  Even my brand new box had to be restarted today after only 10 days of uptime.
* [Warm Takes on Microsoft's Surface Pro 4](https://medium.com/@searls/warm-takes-on-microsofts-surface-pro-4-580f77634d2c#.7ix3536dx)  All I can say here is that I have an equivalent setup to my OSX box on Linux for my daily work and I've had a far easier time than he has.  Overall a fantastic read.
* [Using a ThinkPad for Development under Linux](https://news.ycombinator.com/item?id=13299585) (good discussion of gear and options here and also on the [X1 Carbon article](https://news.ycombinator.com/item?id=13286150))

</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/postmac/2017/01/03/recent-postmac-round-up.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/postmac/2017/01/03/recent-postmac-round-up.html</guid>
        
        <category>apple</category>
        
        <category>osx</category>
        
        <category>postmac</category>
        
        <category>windows</category>
        
        <category>mac</category>
        
        
        <category>postmac</category>
        
      </item>
    
      <item>
        <title>Processing Large Datasets On AWS Using Ruby, Rails and SideKiq</title>
        <description>Two days ago I did a data processing task which previously took me a week -- overnight.  I did this using the following technology stack:

* Ruby
* Rails
* AWS
* Sidekiq

My platform was a &quot;cluster&quot; of 40 m3.large AWS ec2 instances.  We all see a lot about cloud computing and using AWS / Azure / Google Cloud to do these types of large jobs but you rarely see what I think of as the hard details:

* How do you get code deployed?
* How do you fix bugs?
* How do you deal with Capistrano failures when a box isn't available and Capistrano doesn't give you good feedback?
* How do you get things coordinated?
* How do you kick off the overall job?
* How do you know when it is done?

In this blog post I'm going to illustrate how I managed these things.  I'm not saying that what I did was the only way to do this.  I'm not even saying that what I did was the best way to do this.  What I am saying is that this is a practical approach to ad hoc large data processing jobs using a ruby / rails / sidekiq approach.  And I'm going to describe how I did this without using cloud formation or another large, complicated AWS or third party API.  The only external tool I used was Ansible and even that was optional.

For obvious reasons of company confidentiality I can't go into the details of what the job was.  Suffice it to say:

* a &quot;lot&quot; of data had to be &quot;processed&quot;
* the actions were time consuming including deliberate sleep calls to avoid being blocked on the remote end
* about 35,000 discrete data items needed to be processed.  With sleep calls at a randomized 10 to 15 seconds between each call that's 350,000 to 525,000 seconds in aggregate compute time (less if threaded but too many threads and we get blocked)

So, with that said, here's how I went about this:

* build an ec2 instance as a template
* deploy the current code onto it
* test
* make an image
* launch the job on the template box
* launch more copies of the image

Each of these is described below.

My thanks go out to [Nick](http://www.nickjanetakis.com/) who was a consultant on this and paired on it throughout the process.  I also have to say thank you to [Mike Perham](http://www.mikeperham.com) who built [Sidekiq](http://www.sidekiq.org) which is at the heart of this.  

# Step 1 - Build an EC2 &quot;Template&quot; Instance

The first step is that like with everything AWS you need an instance.  Picking the right instance type isn't a topic that I'm going to cover here.  I did know that I needed a reduced thread count so I wasn't terribly worried about memory.  We had already arranged with Amazon for up to 200 m3.large instances so that's what I went with.  I didn't worry terribly about whether not not I had the perfect instance type -- I just used what was available.  

A m3.large is 7.5 gb of RAM and 8 gigs of storage so that's perfectly fine for a Rails app of even large size.

After I created the box I provisioned to run my Rails app as [per all the things I've written about using Ansible](https://fuzzygroup.github.io/blog/category.html#ansible).

Once we get this machine built out we're going to be using it as a template for making more machines later hence my referring to this as a &quot;template&quot; instance.

# Step 2 - Deploy the Current Code Base with Capistrano

The next step was to get my code base onto the box using Capistrano.  I just added this box to my ~/.ssh/config file and then dropped the hostname into my config/deploy/production.rb file and did a normal deploy.

# Step 3 - Test, Test, Test

At this point we have a single instance running our rails application.  We need to make very, very sure that this is working correctly because our next step is to make an **image** of this instance and then use AWS to launch N copies of the image.  Here's what you want to test:

* connectivity to your database
* connectivity to your redis
* that the job process code works
* that sidekiq works
* that your thread count is tuned properly
* that sidekiq starts on boot

This last point, that sidekiq starts on boot, is the key thing that you need to ascertain.  Since sidekiq is what's going to run our jobs and we don't want to manually ssh into each machine, we need a way for the job to start.  If sidekiq starts on boot then job processing begins automatically when the machine starts up.  

The only real way to verify this is:

    /sbin/reboot
    log back into machine
    ps auwwx | grep side

If you see sidekiq running then you have things configured correctly and sidekiq is starting on boot.

# Step 4 - Make an Image

At this point you know that things work and you might be thinking - &quot;Ok I now create a bunch more boxes; provision them and deploy with capistrano.&quot;  That's absolutely correct from a classical hosting perspective and absolutely wrong in a cloud environment.  The far easier, far faster approach is to make an *image*. An image is simply a full disk copy of the instance that you can use to replicate the machine.  If you're an old school PC guy then think of this as ghosting the machine.  Where installing things from scratch or even provisioning from ansible takes hours or minutes, cloning takes only a few minutes and then AWS can launch your instance in parallel so 40 machines might come up in just a minute or two.  

On your EC2 instance list select the instance and then on the Actions menu select Image, Create Image.  You'll need to give it a name and the more descriptive, the better  It will take a minute or two but Amazon will make it just fine.

Note: Making an instance shuts down the machine fully to make sure that any open files are backed up.  Keep this in mind since you'll need to re-login to the machine for Step 5.

# Step 5 - Launch the Job Using Sidekiq and Re-test to be Sure

At this point you're ready to actually launch the job using sidekiq and start processing on one instance.  You can do this with the Rails console or a Rake task.  I prefer a rake task. Here's what my rake task looked like:

    task :some_large_job =&gt; :environment do
      search_urls = MiscClass.large_urls_collection
      search_urls.each do |search_url|
        MiscCkassWorker.perform_async(search_url)
      end
    end

That built a redis queue and gave each method to sidekiq as an asynchronous call to be processed.  Check your sidekiq log file to make sure that things are going ok.  

**NOTE:** If You find that there are changes you need to make then you'll need to re-create the image as per Step 4.

As long as things are looking fine then it is time for Step 6 -- launching more copies of the image.

# Step 6 - Launch N More Copies of the Image

The final step is to launch more copies of the image.  Because the job is already queued into redis and running, as soon as you launch any more instances the copy of sidekiq which runs on boot will start pulling jobs and processing them.  

Launch an instance the way you create any instance, only this time you'll select that you want to make the instance from &quot;My AMIs&quot;] and then pick the image that you created in Step 4.  You can then tell AWS how many copies of the image you want made.  I specified 40 and then it is the normal AWS instance creation options like security groups and such.  Sadly all of these options aren't defined solely in the instance itself.

Note: The AWS command line tools or ansible code can be used to automate this further.

# Step 7 - Make Your Wife a Margarita

Well you can celebrate how you want but that's what I did.  I checked the sidekiq queue the next morning and it was at 0.  I checked the database and we had generated 2,500 new records which was about what I expected.

# Epiphany - Realize You're Making an Appliance!

I'm writing this blog post now having done this a dozen times or more. What finally made all this click in my head is the realization that what I'm doing here is making an *appliance* or actually a *farm* of appliances. An appliance is a tool which does one thing and does it well.  If you think about what we've done here is that we've made a ruby appliance in the form of an AWS image which eats data and (presumably) excretes some type of database record.

# Circling Back to The Hard Questions Mentioned Earlier

At the start of this piece I mentioned a number of hard questions like deployment, bug fixing, etc.  Each of these is addressed below.

* How do you get code deployed?  Capistrano is currently our tool for code deployment.  If we need to get a code fix onto the boxes we built off the template we add the ec2 host name into our SSH config and then just do a deploy.  We are currently writing a simple deployer in Ansible to make deploy easier and more integral with the entire process.  Hopefully I'll be able to open source that at some point.  Yes we looked at [Anistrano](https://github.com/ansistrano/deploy) but Anistrano lacks critical rails features like bundle install which I find to be an absolute show stopper on using it.
* How do you fix bugs?  We try very hard to test up front to avoid having to fix bugs on a long running job.  We streamlined our testing and focused hard on it before the jobs began deliberately to minimize bugs.
* How do you deal with Capistrano failures when a box isn't available and Capistrano doesn't give you good feedback?  This remains an issue.  When Capistrano fails on a multiple box deploy it often isn't clear why and Capistrano is specifically designed to stop when a single box in a deploy fails.  This contrasts nicely with Ansible which is specifically designed to continue despite failing.
* How do you get things coordinated?  Coordination is always, always hard.  I have some interesting ideas on management tools for pulling this together but it isn't time yet to implement them.
* How do you kick off the overall job?  We use a Rake task which is my default for automation and is documented above.
* How do you know when it is done? We don't have a great answer yet on this.   Again I have some interesting ideas but we're not yet at the implementation stage yet. 


</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>sidekiq</category>
        
        <category>aws</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Invalid route name, already in use 'page'</title>
        <description>It seems that whenever you start a new Rails project you hit some kind of wackiness with respect to an error message that you've never seen before.  Yesterday I started a new project and I wanted to use Bootstrap for my layout.  Well I couldn't make it work and then [Dv](http://dv.dasari.me) couldn't make it work so I finally turned to the [RailsApps](https://github.com/RailsApps/) project and used their [bootstrap template app](https://github.com/RailsApps/rails-bootstrap/blob/master/config/routes.rb).  And that worked so Huzzah! both for them and me.  Thanks Guys!

Note: Dv and I have both used bootstrap on I can't tell you how many different sites and its always a pain in the neck to initially get going.  We were using the bootstrap gem and we had the scss stuff configured correctly at least by comparing to a reference site.

Today I integrated [authlogic](https://github.com/binarylogic/authlogic) for authentication based on a [SitePoint AuthLogic tutorial](https://www.sitepoint.com/rails-authentication-with-authlogic/).  When I generated a Pages Controller and dropped a resources :pages into my routes file I got this:

    Invalid route name, already in use: 'page' 
    
My routes file right now is like 5 lines so I was actually certain that I didn't have pages in there already (embarrassingly though I did do a command+F anyway).  A bunch of googling turned up this [answer](https://github.com/thoughtbot/high_voltage/issues/109).  Apparently the [High Voltage gem](https://github.com/thoughtbot/high_voltage) from [Thoughtbot](https://github.com/thoughtbot) automagically inserts its pages route into the routes file.  And I get the desire for simplicity but when it doesn't even require a declaration in the Gemfile it makes tracking this kind of stuff down annoying.  Sigh.

Once that was removed then I was able to get my static pages working again so that's nice.  And I like what the High Voltage gem is actually doing.
</description>
        <pubDate>Thu, 29 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2016/12/29/invalid-route-name-already-in-use-page.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2016/12/29/invalid-route-name-already-in-use-page.html</guid>
        
        <category>rails</category>
        
        <category>pages</category>
        
        <category>authlogic</category>
        
        <category>bootstrap</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Getting Past SSH Errors in OSX Sierra</title>
        <description>If you are having problems with OSX Sierra and authenticating with github, the issue is that OSX Sierra doesn't automatically add ssh keys by default.  



# References:

* [Reddit](https://www.reddit.com/r/osx/comments/52zn5r/difficulties_with_sshagent_in_macos_sierra/)
* [SSH Keys in MacOS Sierra](https://github.com/jirsbek/SSH-keys-in-macOS-Sierra-keychain)
* [Adding SSH Identities via Terminal](http://askubuntu.com/questions/363404/ssh-add-command-does-not-add-my-identity-to-ssh-agent)
* [Github SSH Add](https://help.github.com/articles/error-permission-denied-publickey/)

One solution is to add the below lines to your .ssh/config file:

    Host *
      IdentityFile ~/.ssh/id_rsa
      AddKeysToAgent yes</description>
        <pubDate>Thu, 29 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2016/12/29/getting-past-ssh-errors-in-osx-sierra.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2016/12/29/getting-past-ssh-errors-in-osx-sierra.html</guid>
        
        <category>osx</category>
        
        <category>ssh</category>
        
        <category>sierra</category>
        
        <category>sshagent</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Fixing Sudo on OSX Sierra</title>
        <description>OSX Sierra makes a number of low level changes in how things operate.  Earlier I wrote about OSX and SSH errors.  If you're having issues with sudo then you may want to add this to the sudoers file:

    sudo visudo (this command gets)
    
    Defaults !tty_tickets
    
[Stack Overflow Reference](http://stackoverflow.com/questions/39474047/sudo-command-on-macos-sierra-does-not-respect-timestamp-timeout)</description>
        <pubDate>Thu, 29 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2016/12/29/fixing-sudo-on-osx-sierra.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2016/12/29/fixing-sudo-on-osx-sierra.html</guid>
        
        <category>osx</category>
        
        <category>sierra</category>
        
        <category>sudo</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Ansible Error Fixing ControlPath Too Long Error</title>
        <description>If you get the ansible error *ControlPath Too Long Error* then all you need to do is create an ansible.cfg file in the directory where you run your playbook.  Please note that this is generally an OSX only error related to the length of the .  Then you need to add this line to it:

    [ssh_connection]
    control_path = %(directory)s/%%h-%%r
    
There are other options that can go here as well.</description>
        <pubDate>Thu, 29 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2016/12/29/ansible-error-fixing-controlpath-too-long-error.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2016/12/29/ansible-error-fixing-controlpath-too-long-error.html</guid>
        
        <category>ansible</category>
        
        <category>osx</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>How to Build Rails views with Markdown</title>
        <description>So I'm starting prototyping on an all new thing and I it want to be commercially successful out of the gate.  That's a bit different for me where I usually focus initially on the code.  This means thinking, from not just *day 1* but **hour 1**, about how the customer sees things and for me that starts with two pages on the site:

* about
* faq

I would argue that for any web thing, an about page and a faq are at the core of your marketing.  And since both of these are content, I can't see the point of using straight erb or haml views when MarkDown is so damn easy.  I wouldn't use markdown for a page where there were forms or UI elements but for content?  Hell yes!

# How to Use Markdown for Rails Views

* In your Gemfile you need to call the [kramdown-rails gem](https://github.com/chrisroberts/kramdown-rails): gem 'kramdown-rails'
* Do the bundle install happy dance
* In your app/views/controller_name directory create your views with a .md extension

The kramdown-rails gem is a light wrapper around the core [kramdown gem](https://github.com/gettalong/kramdown) which does the markdown to html translation and supports using it in an Rails view context.

Just to be a good internet citizen I added the content of this answer to [StackOverflow on Rails Markdown](http://stackoverflow.com/questions/36957097/rails-4-how-i-use-the-contents-of-a-markdown-file-in-a-view/41362259#41362259).  If you have the time and like this then I'd appreciate an upvote.  Thanks!
</description>
        <pubDate>Wed, 28 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2016/12/28/how-to-build-rails-views-with-markdown.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2016/12/28/how-to-build-rails-views-with-markdown.html</guid>
        
        <category>rails</category>
        
        <category>markdown</category>
        
        <category>kramdown</category>
        
        <category>hyde</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Product Review Amazon Echo is Excellent</title>
        <description>So my wife got me an Amazon Echo for Christmas and I powered it up today.  Overall I was absolutely delighted with it -- Amazon has a hell of a product here.  The ease of use that a conversational user interface brings is staggering:

* Alexa play happy holiday music
* Alexa may the force be with you
* Alexa play pink
* Alexa what is the weather
* Alexa what is the temperature
* Alexa what is the news
* Alexa 5 minute timer

Those were a few of the commands that we came up with right away.  My wife gets the serious nerd fu credit for the force one; bless her.  

# Amazon Echo Out of the Box Experience

A few thoughts on the out of the box experience: 

* The out of the box experience is far, far rougher than I had expected.  You certainly get thru it but I didn't find it clear.
* You do need to install the Amazon Echo app on your phone
* There's a weird config step where the Echo presents a wifi network that you connect to so it can find the actual network that in your house; at the end of it your phone is left connected to the echo and its unclear if you change that or not (I would think so but who knows)
* The trend towards &quot;no one reads the manual so let's not even have one&quot; is a suck ass trend.  There are those of us who do read the manual and we get very annoyed by the fact that there isn't even comprehensive online documentation.  I can rationalize the lack of a printed manual due to rapid updates but sheesh.

# When It Doesn't Play Music

My biggest problem was that it wouldn't play music initially.  Googling was relatively fruitless but it did reveal that lots of other people have this problem:

* [Google Search - 3.42 million results](https://www.google.com/search?q=amazon+echo+won%27t+play+music&amp;oq=amazon+echo+won%27t+&amp;aqs=chrome.3.0j69i57j0l4.5987j0j7&amp;sourceid=chrome&amp;ie=UTF-8)
* [NTP Related](http://www.echotalk.org/index.php?topic=322.0); best theory I saw

The answer, at least for me, turned out to be turn it off and then on and then wait.  And that did it. Sigh.  Given how important digital technology is to all of our lives the answer of power on / power off or as I refer to it &quot;get out of the car and get back into the car&quot; is a crappy answer at best.

# Conclusion

Well I've already tried to order the [hockey puck extension](https://www.amazon.com/dp/B01DFKC2SO/ref=ods_gw_b_h1_ha_justask_black?pf_rd_r=QMHK561Z7EWPEZQJ1BFX&amp;pf_rd_p=68d26f18-1cf3-45a4-870c-87bf6bd9cee6) for the echo so I guess that says all you need to know about how much I like it. This is a fantastic product.  Here's a [good article that clarifies the difference between the Echo and Echo Dot](http://www.pocket-lint.com/news/136952-amazon-echo-vs-amazon-tap-vs-echo-dot-what-s-the-difference); much to my surprise the hockey puck isn't an extension - its a standalone version of the echo with a smaller speaker so it can be cheaper but it doesn't require the echo at all.  You can plug it into your own speakers for better output.</description>
        <pubDate>Mon, 26 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/amazon_echo/2016/12/26/product-review-amazon-echo-is-excellent.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/amazon_echo/2016/12/26/product-review-amazon-echo-is-excellent.html</guid>
        
        <category>alexa</category>
        
        <category>echo</category>
        
        <category>music</category>
        
        <category>echo_dot</category>
        
        <category>review</category>
        
        
        <category>amazon_echo</category>
        
      </item>
    
      <item>
        <title>My Worst Git Commit Message Ever</title>
        <description>I was in the car with my wife talking to her as I worked and this was the message:

     git commit -m &quot;misc:
    → '''''''''''''''''''vvvvvvvvvvvvvvvvvvvvvvV 
    VVVVVVVVVVVVVV                                                               
                              ///////// vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
                              vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
                              vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
                              vvvvvvvvvvvvvvvvvvvvvvvvvvv

I feel asleep while talking to her and coding.  I suspect drooling was also involved.  Too many late nights...</description>
        <pubDate>Fri, 23 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/git/2016/12/23/my-worst-git-commit-message-ever.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/git/2016/12/23/my-worst-git-commit-message-ever.html</guid>
        
        <category>git</category>
        
        <category>humor</category>
        
        
        <category>git</category>
        
      </item>
    
      <item>
        <title>PostMac Roundup</title>
        <description>Here are a few more things in the ongoing PostMac roundup:

* [Apple’s Tim Cook assures employees that it is committed to the Mac and that ‘great desktops’ are coming](https://techcrunch.com/2016/12/19/apples-tim-cook-assures-employees-that-it-is-committed-to-the-mac-and-that-great-desktops-are-coming/) | [HN Discussion](https://news.ycombinator.com/item?id=13217008)  Given that Tim Cook is the one who said publicly that people should use iPads and not laptops I'd have to wonder about this.  [Gruber on This Comment](http://daringfireball.net/linked/2016/10/31/cook-why-would-you-buy-a-pc-anymore).  Lots of good comments on people using Linux in place of OSX here.  [A brilliant comment](https://news.ycombinator.com/item?id=13217412).
* [How Apple Alienated Mac Loyalists](https://www.bloomberg.com/news/articles/2016-12-20/how-apple-alienated-mac-loyalists) | [HN Discussion](https://news.ycombinator.com/item?id=13220623)

</description>
        <pubDate>Wed, 21 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/postmac/2016/12/21/postmac-roundup.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/postmac/2016/12/21/postmac-roundup.html</guid>
        
        <category>postmac</category>
        
        <category>mac</category>
        
        <category>osx</category>
        
        
        <category>postmac</category>
        
      </item>
    
      <item>
        <title>David Rovics Community Supported Art or a Tale of PayWoe and UI / UX Failures for Startups to Learn From</title>
        <description>Tales on the Internet of problems with PayPal are damn near epic in their size.  Everyone, always, seems to have issues with PayPal and I am no exception.  Part of this is the nature of PayPal:

* a long running Internet service - I remember the first time I used PayPal and that was three houses ago (not three rented houses; three owned houses); if I measure PayPal in terms of places I have lived then the number is 8.
* focus on a single persistent personal address; I have been using the same email address since the early days of gMail. It is how PayPal knows me but that means there is &quot;contact cruft&quot; attached to it in the form of dead address; dead cell phone numbers
* it is constantly under attack.  I read [The PayPal Wars](https://www.amazon.com/PayPal-Wars-Battles-Media-Planet/dp/1936488590) back when I worked with [Kareem](https://medium.com/@kareem.m?source=false---------0) and I've fought spam wars in both email and blog so I understand how being under constant attack changes how you build and design products.

One of the people I grew up with is [David Rovics](http://www.davidrovics.com) and consider a friend albeit one I haven't seen since 1985.  I remember trading comic books with him at his parent's house back in the late 1970s.  Our political leanings couldn't be more different - he's a card carrying anarchist / socialist / musican who writes beautiful songs that honestly bring tears to my eyes.  Songs of social significance is how he describes them and he isn't wrong.  Me?  I may live in Indiana but I'm deeply tied to tech entrepreneurialism and the open source world.  We could not be more different along ideological lines but I have damn near infinite respect for him.

David is also a writer / blogger and his piece about the [differences between America and Europe](http://songwritersnotebook.blogspot.com/2016/03/rejected-by-america.html) is illuminating.  I don't agree entirely with his conclusions but I also don't argue with the facts.  If you like folk music / songs with meaning then his stuff, all freely available on [BandCamp](https://davidrovics.bandcamp.com/) is fascinating.  Personally I'd highly recommend the [Battle of Blair Mountain](https://www.youtube.com/watch?v=Z_rCdNdkb_g) which tells a dark tale of American labor relations that you don't learn in school -- certainly not Wilton High School where David and I attended.

So what does that very long introduction have to do with PayPal?  Well David is an artist which means that he sells CDs, tours, etc and he recently introduced his CSA or Community Supported Art program where you can sign up as a member for $50 / year (other options available as well).  And I tried, back in September, to signup only to be thwarted by PayPal issues.  And then, sadly, I got busy and lost it amongst the weeks of AWS, Ansible, server migrations, code migrations, learning elixir, etc.  But you should never forget your friends.  Yesterday I noticed that David's main site was down so I dropped him an email and that inspired me to finally fix my PayPal account.  In the process I noticed a few UI issues that I found surprising in a site of PayPal's scale.

You would think that a company like PayPal would have a finely tuned UI / UX team around the issues of common problems. Somehow, given what I just experienced, I don't think so.  And therein lies the tale.

# Problem the First - Login and Verification

This happened yesterday morning and I did not capture any screenshots because, well, my bad.  Given that PayPal had old contact info for all my cell phones user verification was tricky but I got past it.  To be entirely honest tho I was actually kind of happy that verification was tricky - that means accounts cannot be compromised.

# Problem 2 - Expired Card

When I went to purchase from a link from David's site, I got this screen:

![paypal_link_to_add_new_card_is_where.png](/blog/assets/paypal_link_to_add_new_card_is_where.png)

If you notice there is no way to add a new card here and the card that they had on file for me was easily 5 years old.  Now I do get that the designers might have been worried about breaking the payment flow by allowing a link to add a new card.  But given that I'm not a UI guy and I can easily think of at least one way around that, should't they been able to?  I have to think that adding a new credit card is a fairly common thing if you are PayPal.  And if the card is as old as I had then shouldn't a link be presented at least in that case if not every case?  

# Problem 3 - The No No No Screen

So I navigated thru the PayPal ui and finally found where to add a new card -- the correctly named &quot;Wallet&quot;.  So I copied and pasted my details from [Enpass](https://fuzzygroup.github.io/blog/software_worth_purchasing/2016/09/15/software-worth-purchasing-02-enpass.html) (highly recommended btw) and I got what I think of as the No No No screen:

![paypal_no_no_no.png](/blog/assets/paypal_no_no_no.png)

Yeah I get it.  My wife has our main credit card attached to her PayPal account but she's sleeping.  Time to dig out the wallet and find some other card.  But, again, shouldn't there have been a link here to use a different card instead?  The web is, inherently, a hyper linked medium -- didn't the PayPal designers ever get the memo?

So I navigated backwards thru the UI and found the place to add a different credit card and then went back to my friends site and completed my purchase.  Huzzah!

# Good Things

There are a few good things that PayPal has done that in all fairness I should point out:

* The list of security questions has been substantially revised.  My older security questions where last 4 of social and mother's maiden name.  Now it is things like &quot;name of first pet&quot; / &quot;name of favorite childhood stuffed toy&quot;.  That isn't as good as write your own question but it is much, much harder to find that type of personal trivia online.
* I was able to add a credit card, pay for something and then delete the credit card even tho the transaction might not have been processed yet.  That is a wonderful design decision and very much pro user.  Kudos!

# How to Fix Core Usability if You Are a Startup

The take away here and the reason that I tagged this as startup is that even well established, mature companies get common UI / UX issues wrong all the time.  I don't think I'm wrong to think that adding a credit card is a common use case for PayPal and that it should have been a 1 step action from payment.  Instead I spent about 15 minutes to do nothing more than complete one purchase.  If it wasn't for my friend then there's a very good chance that I would have dropped out -- as I did once before.  

The single best way to avoid this type of UI disaster is to simply watch your users use your product, take notes and then iterate on your product until that issue is addressed.  Then you go back to a different user and watch them try and do the same thing and see if you made it better or worse.  And repeat until this whatever that issue is is fixed.  I learned this lesson a number of times over the years but most recently from [Kouris](https://twitter.com/kouriskalligas) and [Dave](https://en.wikipedia.org/wiki/Dave_Sifry).

People, particularly &quot;UX specialists&quot;, want to make a big deal out of usability but it really isn't all that hard or even all that expensive.  All you need to do, if you want to solve these type of core usability issues, is have **the will to do it**.  You have to take responsibility for the problem, focus on it and then set an agenda like &quot;this week we're going to improve login&quot;.  It can be absolutely exhausting since you're iterating on the same, often tiny details, over and over but that's part of the game.  A software engineer, particularly someone like myself, generally won't take ownership of that kind of problem since our focus is internals but someone on the management or product side absolutely can do this.  Sometimes the improvements are absolutely tiny -- perhaps better messaging is needed or even just a link.  As an example, messaging in one case and a link in both cases would have addressed all of my issues above.

In closing, given the size of PayPal I can actually sort of understand why getting this fixed is hard -- I suspect that there are a lot of organizational boundaries to cross and people to deal with.  A startup shouldn't have that problem.

**Note**: Kudos to [Acorn](https://fuzzygroup.github.io/blog/software_worth_purchasing/2016/09/11/software-worth-purchasing-01-acorn.html) for being easy enough to use that even I, a thumb fingered, graphically challenged engineer, could get it right.  Recommended.</description>
        <pubDate>Tue, 20 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rant/2016/12/20/david-rovics-community-supported-art-or-a-tale-of-paywoe.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rant/2016/12/20/david-rovics-community-supported-art-or-a-tale-of-paywoe.html</guid>
        
        <category>startup</category>
        
        <category>paypal</category>
        
        <category>paywoe</category>
        
        <category>rant</category>
        
        <category>art</category>
        
        <category>rovics</category>
        
        <category>ui</category>
        
        <category>ux</category>
        
        <category>usability</category>
        
        <category>david_rovics</category>
        
        
        <category>rant</category>
        
      </item>
    
      <item>
        <title>The Worst Business Decision I Ever Saw Disney Make</title>
        <description>My wife, kids and I are at DisneyWorld right now for a pre-Christmas vacation.  There are 4 of us stuffed into a *single* room since when we booked it the adjoining room was not available.  This happened yesterday so the date was December 18th and we noticed early in the day that the room adjoining ours was actually empty.  And, at 5 pm, it was still empty so my wife and I went to the front desk and asked: 

* Is that room really empty?
* Can we rent that room as well as ours? 

This seems like a simple, straightforward request.  Unfortunately it took ten minutes of waiting and two authorization phone calls for the person helping us to write a price down on a scrap of paper and show it to us.  The price for that unoccupied room was:

&gt; **$493 / night**

She mouthed some sort of corporate line about how &quot;when she rents it directly to the customer she **has** to charge *rack rate*.&quot;  So for the four remaining nights of our trip adding this additional room would cost almost $2,000 or more than we are currently paying.  Curiously she seemed to know that this price was too high since she seemed mildly embarrassed about giving us the price.

My only response here is: *balderdash*, *poppycock* and what a *load of hooey*!  Yes I pulled that from an old episode of [Scorpion](http://www.imdb.com/title/tt3514324/).  But let's look at this from a business perspective.

# Trick Question #1- What is the Value of an Unsold Hotel Room?

This isn't actually much of a trick question, it is a simple one - the value of an unsold hotel room is **$0**.  Each hotel room is only available for rent for 365 nights a year.  A hotel room that isn't rented for a night is simply **lost revenue**.  

# Trick Question #2 - What Are the Additional Costs of Renting It to Us?

Now you could argue that by renting it to us, Disney incurs additional house keeping expenses but, at best, that's an incorrect argument.  House keeping is already paid for since there is a person assigned to the floor anyway.  And, under normal circumstances, she would be planning on cleaning that room anyway.

# A Business Decision So Bad It Makes Me Sad

The saddest thing of all in this case is that Disney actually knows who we are as customers.  My wife is a certified Disney nut and we have been to Disney an embarrassingly large number of times.  With the MagicBand system any Disney customer service rep seems to always know everything we spend / have done / etc.  As best I can tell our entire purchase history is always available to them.  This would have been a fantastic opportunity to reward a loyal customer with something like &quot;Normally I have to rent it out at rack rate which is $493 / night but since you are a regular customer I can rent it out for $300 / night&quot;.  We're here for 4 more nights and that would have guaranteed Disney an additional $1,200 in revenue.  Sure it wouldn't have been the $2,000 they might want but the room isn't sold and I'd bet that if a room isn't pre-booked right before Christmas then it is very unlikely to be booked.  Now my business background isn't in hotels and I could be wrong here -- perhaps there is a large quantity of last minute, pre-Christmas Disney bookings but I think not.  

Sad.  

**Note 1**: In discussing this with my wife she commented that perhaps Disney did look at our history and saw that we actually spend money with them and wanted to get the potential maximum for the room.  That's an intriguing perspective but I would think that looking at the ratio here would be instructive i.e. if the requested upgrade doubles the overall vacation cost then no one would ever do it. 

**Note 2**: The room is still empty the next day.</description>
        <pubDate>Mon, 19 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2016/12/19/the-worst-business-decision-i-ever-saw-disney-make.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2016/12/19/the-worst-business-decision-i-ever-saw-disney-make.html</guid>
        
        <category>startup</category>
        
        <category>business</category>
        
        <category>disney</category>
        
        <category>pricing</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Rails Refactoring Tip - When You Remove a Database Column and You Are Still Trying to Use It</title>
        <description>I have to admit, up front, that this is a hacky, schlocky recommendation but it is also **useful**.  As much as all of us would like to imagine that we refactor with grace, skill and perfection the reality is often far darker.  

I recently had a refactor involving the removal of database columns **fail** on me.  I attempted something quite ambitious and it just didn't work out.  And, worse, it involved removal of database columns from a production database so reverting to the older code is now quite an issue.  When I run it my master exception handler gives me output look this: 

    EXCEPTION===========================================================================
    undefined method `crawl_id=' for #&lt;Form2016Q4:0x007fa52c346128&gt;
    Did you mean?  crawl=
                   crawl
                   crawl_url=
                   crawl_url (PAGE) /Users/sjohnson/Dropbox/appdatallc/banks_before_refactor/banks/lib/common_page.rb 157 new_and_create_for_external_links
    ====================================================================================

What's happening here is that I've removed the crawl_id attribute from the form2016_q4s table. And the older code doesn't understand this.  At this point I'm still trying to get evaluate if the original problem I was trying to fix is fixed in the older code check out.  I don't want to make major changes in this check out since I'm still at that &quot;should I stay or should I go now&quot; part of the evaluation (i.e. toss out my refactor or try and pull in from the older check out).

Here's how I got around this, *attr_accessor*. [Good Blog Post](http://notes.jerzygangi.com/using-attr-accessor-in-rails-model-classes/) versus [Actual Docs](http://ruby-doc.org/core-2.0.0/Module.html#method-i-attr_accessor)

attr_accessor is one of those bits of Ruby magic that you don't use often when when you need it you really, really need it.  It generates reader and writer methods for an instance variable and it does so automagically behind the scenes.

Here's the only change that I needed to make at the top of my class:

    attr_accessor :crawl_id
    
Note 1: that I put it at the top of my class by style preference; it could have gone anywhere after class and before protected or private.  I also used attr_accessor instead of attr_writer I wanted support to enable reads in case I had any of those.

Note 2: If you're using a deleted column in an ActiveRecord where statement you're still going to have issues; those I had to fix by hand.</description>
        <pubDate>Mon, 19 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2016/12/19/rails-refactoring-tip-when-you-remove-a-database-column-and-you-are-still-trying-to-use-it.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2016/12/19/rails-refactoring-tip-when-you-remove-a-database-column-and-you-are-still-trying-to-use-it.html</guid>
        
        <category>rails</category>
        
        <category>refactor</category>
        
        <category>activerecord</category>
        
        <category>database</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Tech Interviewing is Broken - A Suggestion</title>
        <description>So [Medium](http://www.medium.com/) this morning tells me that person X is following me, I forget who -- that was on my phone and I'm writing on my laptop.  I try and figure out why and I notice that person X commented on an article by [Eric Elliot](https://medium.com/@_ericelliott).  I bounce over there and I find his [Tech Hiring is Broken](https://medium.com/javascript-scene/tech-hiring-has-always-been-broken-heres-how-i-survived-it-for-decades-b7ac33088de6) rant and I am inspired.  And that is how this post vomited its way out of my cortex and onto the Internet.

Sidebar: Medium has a brilliant authoring interface but the integration between phone and browser on your laptop is deeply, deeply flawed.  Know why I referred to someone as person x?  I can't find the same stuff on my laptop as on on my phone.  Honestly I was barely able to find Eric Elliot at all -- know how I did it?  [Google](https://www.google.com/search?q=medium+eric+elliot&amp;ie=utf-8&amp;oe=utf-8).  Know how I found the hiring post?  [Google](https://www.google.com/search?q=medium+eric+elliot+tech+hiring&amp;ie=utf-8&amp;oe=utf-8).  Don't get me wrong, I love what Ev has done for the industry (Blogger, Odeo, Twitter and now Medium as well as more) but there are deep usability problems here.  This might well be fine if you could use your phone to author on Medium but all authoring in Medium still has to be done on a PC which means responding to Medium's notifications is painfully hard.  Sigh.  I would strongly suggest that Medium do real usability work and give people problems like &quot;there's a post by this person; respond to it&quot;.  Their product would get better as a result.

Eric makes a number of good points about tech interviewing, whiteboard coding exercises and so on.  Having both been a interviewer and a interviewee over the years and having **failed** the interview process at every big tech company I've ever tried at, I think I have an interesting perspective on the process so I thought I'd put together some notes on how to tech interview.  I've used variants of this over the years and I think this is exactly how I'll interview someone the next time I'm hiring.

# What Does an Engineer Actually Do?

If you think about what an engineer does then you might think that it is something like this:

* code
* code code!
* code code code!

Sadly, even though writing code is the fun part of the job, that's **not** actually what we do at all.  Here's actually what we do in order of frequency:

* think
* research problems
* debug code
* read code
* write code 

So, yes, writing code is a part of it but writing new code is actually, with a few exceptions, probably the smaller part of your job, particularly at bigger companies.  The bigger the company, generally, the smaller the amount of new code you generally have to write and the more you have to debug and read code.

Eric, in his post, argues quite well that white boards in the interview process are broken:

&gt; People don’t think or code linearly, from top to bottom, in neat lines. They put together blocks, frequently cut and paste, etc… On a whiteboard, that requires erasing, and that looks like a mistake. Of course, that makes the interviewee nervous, and it sends false signals to interviewers [Eric Elliot](https://medium.com/javascript-scene/tech-hiring-has-always-been-broken-heres-how-i-survived-it-for-decades-b7ac33088de6#.mw5b2ha1n)

He's absolutely right on this and the simple truth of it, whether we want to admit it or not, is that an engineer's skills using Google / Stack Overflow / Github to find the answer to a problem is probably a bigger factor in overall job success than any ability to solve interviewing brain teasers.  Overall Eric makes a huge number of good points and you really should read his post.

# Adapting What an Engineer Actually Does to the Interviewing Process

The hands on technical interview portion of the overall interview process is likely between 1 and 3 hours with the initial hour determining whether you advance to the next stage or you fail out.  At least that's been my experience.  So given that I've argued that you spend more time thinking / researching / debugging code / reading code how do you evaluate those aspects?

Now I basically regard my last bit of technical hiring as a failure of mine at the interview level so I've spent a lot of time thinking about this and this is roughly my third time I've written all this down (the first 2 were abject failures; bytes fed into the bit bucket).  Here's my conclusion:

&gt; An engineer needs to be evaluated along the lines of the work he is going to be doing.  And the best proxy for that is likely the work that **you** are **currently** doing.

Unfortunately, giving the secrecy fetish in most large companies, this doesn't work when:

* a person is being interviewed by someone on the team but the current project can't be shared
* by someone who isn't on the same team as the person being hired and the same dev tools aren't being used
 
But if you can avoid the above two issues, let me illustrate this with an example.  And I will admit that for the next person I interview, this will be the interview process I use.

## Pre-Requisite

All of these approaches below require screensharing as a pre-requisite. Specifically control needs to shift fluidly between interviewer and interviewee so that the interviewer can give the interviewee a task and watch them execute on it.  I'd recommend ScreenHero for this but sadly Slack has brutalized ScreenHere through the normal acquihire debacle.  I'm not sure what else to recommend although there are a few web based tools around that are promising.

## Test What You Know 1 - Thinking

If you're an engineer writing code on a project then there is **always** some aspect of the project that you are currently thinking about.  Something, no matter what, is always unsolved.  Bill Joy, the principal author of BSD Unix and the co-founder of Sun, has something to say here:

&gt; Joy's law is the principle that &quot;no matter who you are, most of the smartest people work for someone else,” [Wikipedia](https://en.wikipedia.org/wiki/Joy's_law_(management))

If you accept that that's right then here's my suggestion - get the interviewee's input on what you are currently thinking about.  Find out what they have to say on the matter.  If nothing else I suspect the process of cogently explaining it to a fresh brain might help you; I always find that to be the case.  And, if the interviewee has a great insight, isn't that a **damn solid indicator** that they might be a good hire?

## Test What You Know 2 - Research

Once again if you're an engineer writing code on a project then there is something that you googled either today or yesterday where: 

* the answer was hard to find
* the answer was hard to interpret
* tenacity was needed i.e. you needed to dig into the 3rd page of google results or find the answer not in a github wiki post but actually in a closed issue or perhaps by back tracing the original author to their reddit presence and posts they wrote

The sad, sad truth of search today is that finding answers to hard questions often takes a considerable amount of sleuthing.  And if you accept that as true then watching how an engineer takes a problem you give them and find the answer tells you how efficient they might be in the work place.  And, best of all, all you have to do for this interview task is look in your own search history and say something like &quot;here's an error message -- how do you look this up to take the next step&quot;.

## Test What You Know 3 - Debugging

This is my personal favorite in terms of interviewing tests.  Debugging for engineers is the equivalent for wood workers of *measure twice, cut once*.  Debugging is the **essential** skill that tells you how an engineer tackles a problem and watching how someone debugs something is always interesting.  

My first business partner, Brian Giedt, was an absolute super hero at this.  You could hand Brian a code base with 3.5 million lines of C in it, describe the error and say &quot;go&quot; and he'd find it and fix it.  You might not always like the way he fixed it but he'd get it done and get it done in short order.   I'm not, yet, at that stage but I do quite well.  Once again my suggestion here is to give the interviewee a current problem you're debugging and then watch his progress.  Yes he's going to be missing the overall context you have but you're looking for approach / technique here more than actual results.  And, once again, you yourself might learn something.  

Let me illustrate this with an example from my own pair programming history.  [Dv](http://www.dasari.me) used to work for me and we'd pair but not always as often as we should have.  One day he was watching my technique, I was using raise statements as my primary tool and he finally said &quot;Let's try the debugger&quot;.  He dropped a few gems in my Gemfile and then added the debugger pragma at the offending spot in the code and we restarted.  And you know what?  My life has **never been the same** since.  That's not hyperbole by any means.  I write / debug ruby code 60+ hours per week or 3,120 hours per year and, based on my own premise, more of that is spend debugging than writing.  By teaching me how to use the debugger gem, my life was literally changed.  

So take a part of the code that you need to debug, perhaps even what you were working on before the interview, create a branch so the interviewee can't screw up your work and let them go! What you want to do here is watch their process.  Even if they don't know the code base the process of debugging is generally the same so they should be able to do something.  Whenever you interact with someone who isn't vested in your culture and tooling you have a tremendous opportunity to learn from them.  If you pick up one small technique from an interviewee then isn't that also a good indicator that they might be a good hire?

And if someone can't make any progress or even suggestions then that tells you something.  Perhaps you might get an interaction like this:

* **interviewee**: Perhaps we need a log file here to gather data
* **interviewer**: We have a 10 server environment; how would you go about that

Even if they don't solve the problem, and they likely won't, watching the process is likely hugely informative.  If you see that they are an inefficient debugger or, worse, an ineffective debugger then that's a strong argument against hiring them.

## Test What You Know 4 - Reading Code

I've never seen a code base, not **once**, without a particularly tricky section somewhere.  We may not admit it but every single code base has at least one module where you either tread gently or you modify only with a degree of regret and self loathing.  So here's my suggestion:

1. Hand them that code module to read and, perhaps, indicate to them what you want them to focus on
2. Give them say 5 to 15 minutes to go thru it (adjust the time to fit based on size)
3. Ask them questions; adjust your questions to match the the instructions in 1.

Once again how well they read the code and understand it gives you a huge indicator of whether or not they might be a good hire.

## Test What You Know 5 - Write Code

At this point, if a person has proven to you that: 

* they can think
* that they can research
* that they can debug
* that they can read code

isn't it a given that they can write code?  I don't think I've ever seen a candidate get thru thinking, researching, debugging and reading and then not be able to write code.  But, if you're still concerned then give them a coding problem.

# Conclusion

I think this is a very solid approach to gauging whether or not to hire someone but, honestly, I've only used portions of it in the past.  I've never done it as a step 1 to step 5 process.  However the next time I do I will definitely document it here.</description>
        <pubDate>Sun, 18 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/software_engineering/2016/12/18/tech-interviewing-is-broken-a-suggestion.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/software_engineering/2016/12/18/tech-interviewing-is-broken-a-suggestion.html</guid>
        
        <category>software_engineering</category>
        
        <category>hr</category>
        
        <category>hiring</category>
        
        <category>interview</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Vacation Insomnia and the Hotel Business Office</title>
        <description>As a remote worker I've found that you tend to work more, there are fewer distractions, there are no co-workers to go out with, etc, so you just work more.  And when you always have a laptop with you then even vacation tends to flow into work.  One of the oddest things about being a remote worker is that you end up with recollections of work that are entirely cognitively dissonant with work itself.  

Here are a few examples:

* When I first wrote the iOS version of AppData after fighting an internal battle **hard** that iOS mattered (this was back in 2010 or 2011) I remember staying at a cheap Dallas hotel with bad wifi on vacation with my family when I first wrote that crawler
* I can't tell even begin you how much code I wrote from Great Wolf Lodge on different family trips.  I suspect I've not only written code there but entire products there.
* The last time I was in Disney world I wrote an anti-spam algorithm to evaluate your Twitter followers for their *spamminess* related to your content.  I guess that wasn't for work tho -- more for late night boredom.

So I'm once again on a short vacation and I find myself in the hotel business office working in wonderful, blessed quiet and darkness.  This time I'm in the hotel's business office and it is *absolutely lovely*.  After a night's drive on the way to our destination and a late checkin I found myself up at the crack of 1 am.  And with three other people in the hotel room, what do you do?  Well, if you're me, you shower, shave, grab your gear and find a place with wifi.  

And this time around I'm rewriting the core crawl / recursive_crawl routines for our main HTML crawler.  This is a core loop which is the innermost guts of our crawler and it is ugly beyond belief.  I can say that because I'm one of its two core authors.  This is a routine which takes like 8 parameters and returns 7 (or the other way around; even I can't tell).  So I'm in the middle of a massive refactor where it takes in one struct and returns another.  And I know it will be better but right now I suspect it resembles a butcher shop when a flood of carcasses has just arrived -- there's blood everywhere and bits of bone and gristle from the big band saws that a real butcher shop would actually use.  Even if you're a self confessed carnivore like myself, you likely wouldn't want to see it and that's how I feel right now.  I want this refactor done desperately but, man, even I don't want to do this work.

I've now gotten it to this stage:

    crawl_struct = UrlTool.get_mechanize_links_on_a_page_from_struct(OpenStruct.new(:site =&gt; site))
    
which returns this:
    
    OpenStruct {
               :num_pages =&gt; 1,
         :already_crawled =&gt; [
            [0] &quot;14b461cc3eecf248213c23999ca33236363d083f&quot;
        ],
        :page_body_hashes =&gt; [],
                   :links =&gt; [],
          :mechanize_page =&gt; nil,
          :resolved_links =&gt; []
    }
    
which means that I've made a crawler which, now, does not crawl.  **Groan**.  I wish I'd stayed in bed.  Happily that likely means that I've missed something basic like a conditional since at least my basic return structure is better.  And, while writing this, I just found it -- I had omitted a conditional. 

Now I suspect that a number of my readers are saying something along the lines of &quot;Dude -- **it is vacation**; STOP CODING!&quot; and I will.  And you are right but there are some mitigating factors

* this is time sensitive work that needs to get done before year end 
* I suffer from an over developed sense of personal responsibility on all fronts
* everyone else is sleeping which means no reading, no media to consume
* workaholic

The strongest mitigating factor here tho is that I've found that when I'm forced to leave my home office it often **spurs creativity** fairly dramatically.  Just as an example we normally compare crawl results to past crawls only on 2 dimensions, pages and links because the attributes for those versus past crawls are easily accessible.  What I just realized tho is that I can implement a simple JSON api and compare my new crawl against any of the dimensions by which we crawl - forms, iframes, etc.  And, in the process of writing this post, I actually implemented that JSON api.</description>
        <pubDate>Sat, 17 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/remote_work/2016/12/17/vacation-insomnia-and-the-hotel-business-office.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/remote_work/2016/12/17/vacation-insomnia-and-the-hotel-business-office.html</guid>
        
        <category>vacation</category>
        
        <category>remote_work</category>
        
        
        <category>remote_work</category>
        
      </item>
    
      <item>
        <title>When Ruby bzip2 Won't Install</title>
        <description>I spent quite a bit of time today trying to get the bzip2 gem working.  Not even the fork worked.  Happily I found [rbzip2](https://github.com/koraktor/rbzip2) which worked brilliantly.  Not sure if it compresses or decompresses yet but I can deploy at least and that was today's goal.  

 </description>
        <pubDate>Wed, 14 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2016/12/14/when-ruby-bzip2-won-t-install.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2016/12/14/when-ruby-bzip2-won-t-install.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>bzip</category>
        
        <category>gem</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>What Just Happened To Your Database In the Past 10 Minutes</title>
        <description>So I'm rapidly coming to the conclusion that Christmas is the enemy of getting things done.  While I have come up with a lot of potential topics to write about recently I don't seem to be able to get anything actually finished due to: 

* Present Shopping
* Present Wrapping
* Christmas Events (parties, choir concerts, etc)
* Christmas Baking

Alas -- at least, as my old boss, [Scott Rafer](https://twitter.com/rafer), used to say -- *these are high quality problems to have*!

A quick post today -- looking at the state of your database.  Let's say you have a table called posts and it has a field called created_at (the Rails standard way of tracking table changes) and you want to see how many things came into that table in the past 10 minutes.  Here's the sql you need:

    SELECT COUNT(*) FROM posts WHERE created_at &gt; DATE_ADD(NOW(),INTERVAL - 10 MINUTE);

Previously I've generally looked at tables and then tried to relate what's going on to the current time and adjust in my head for the server time zone offset.  This is **much, much** better.

A really nice side effect of this is that you can simply change the interval quickly and see if your performance is linear or non-linear.  Let's say you change your interval to 5, 10 and 15 minutes and this was the result:

    MariaDB [foo_production]&gt; SELECT COUNT(*) FROM posts WHERE created_at &gt; DATE_ADD(NOW(),INTERVAL - 5 MINUTE);
    +----------+
    | COUNT(*) |
    +----------+
    |      468 |
    +----------+
    1 row in set (0.45 sec)

    MariaDB [foo_production]&gt; SELECT COUNT(*) FROM posts WHERE created_at &gt; DATE_ADD(NOW(),INTERVAL - 10 MINUTE);
    +----------+
    | COUNT(*) |
    +----------+
    |      512 |
    +----------+
    1 row in set (0.44 sec)

    MariaDB [foo_production]&gt; SELECT COUNT(*) FROM posts WHERE created_at &gt; DATE_ADD(NOW(),INTERVAL - 15 MINUTE);
    +----------+
    | COUNT(*) |
    +----------+
    |     1312 |
    +----------+
    1 row in set (0.45 sec)
    
What this illustrates is that while performance was non linear in the 5 to 10 minute range, it became linear again (roughly) at the 15 minute mark.  This is a nice simple way to get a *feel* if performance is overall on track.

</description>
        <pubDate>Wed, 14 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/mysql/2016/12/14/what-just-happened-to-your-database-in-the-past-10-minutes.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mysql/2016/12/14/what-just-happened-to-your-database-in-the-past-10-minutes.html</guid>
        
        <category>mysql</category>
        
        <category>rails</category>
        
        <category>sql</category>
        
        
        <category>mysql</category>
        
      </item>
    
      <item>
        <title>So You Want to Hire a Freelancer to Be Full Time</title>
        <description>For most of the past 8 years I've been a professional freelancer and it all started with *eduFire* where I was VP of Engineering.  When eduFire dissolved in a wash of founder level &quot;oh crap we can't raise our A round; what do we do; I know -- I'll just quit and start something else&quot;(Note1) I made the choice to become a professional freelancer and I have really enjoyed that decision.  I've never made as much money or learned as much as I have since I've been a freelancer.  My skills now are better than they ever have been and it is largely from freelancing -- more on that in a future post.

From time to time clients try, often hard, to hire you full time.  And if you want to hire a freelancer full time then I have one piece of advice for you:

&gt; **Pay the freelancer's bills promptly**

When someone is a freelancer and they are resistant to taking a full time job, I'd argue that there has to be a reason for it since it is **dramatically harder** to be a freelancer than a full time employee.  We may all talk a lot about the *gig economy* but, in 2016 America, things are still structured against freelancers in every way -- insurance, taxes, etc.

In my case I have **trust** issues.  eduFire shut down 2 weeks before Christmas in 2009 with *no notice* and all of us were left high and dry and scrambling for what was next.  And since no one hires in December or even January it meant that everyone was faced at least 2 months of no salary.  I think we all landed on our feet - I certainly did.  

As long as a client is moderately rational or at least not insane the best impression you can give a freelancer is, well, if you **pay your bills promptly**.  As long as the freelancer is paid on time, well, we're generally happy.  And if you want them to join you full time, well, pay your bills on time.  That goes a long, long way towards giving a freelancer the positive feelings you want them to have if you are going to name them a job offer.

Note1: No eduFire wasn't mine and yes that's more than a little bit of much deserved bitterness.
</description>
        <pubDate>Wed, 14 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/hiring/2016/12/14/so-you-want-to-hire-a-freelancer-to-be-full-time.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/hiring/2016/12/14/so-you-want-to-hire-a-freelancer-to-be-full-time.html</guid>
        
        <category>hr</category>
        
        <category>hiring</category>
        
        <category>software_engineering</category>
        
        <category>freelance</category>
        
        <category>gig</category>
        
        
        <category>hiring</category>
        
      </item>
    
      <item>
        <title>Recent PostMac Notes</title>
        <description>Some recent things in the Post Mac category:

* [Jean Louis Gassee returned his New MacBook Pro](https://mondaynote.com/macbook-pro-launch-perplexing-b47003037b2e#.88txiffc1) and called the launch **perplexing**; [Hacker News Discussion](https://news.ycombinator.com/item?id=13162310)
* [Apple is removing battery life estimates from the MacBook Pro](https://9to5mac.com/2016/12/13/why-apple-is-removing-time-remaining-battery-life-estimates-macbook-pro/)
* [The Guardian calls the next MacBook Pro the Best Computer You Shouldn't Buy](https://www.theguardian.com/technology/2016/dec/12/apple-macbook-pro-review-the-best-computer-you-shouldnt-buy)

[Hacker News search on recent MacBook articles](https://hn.algolia.com/?query=macbook%20pro&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=pastWeek&amp;type=story).

I should note that I am basically happy with my MacBook Pro 13&quot; but I bought the previous generation so it is much, much older than the current.  And my wife absolutely adores here brand new MacBook (but not the pro).  Her only real objection is having to use a dongle to print on our old Brother laser printer but she's back in graduate school now and very, very happy with the weight and size of it.  
</description>
        <pubDate>Wed, 14 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/postmac/2016/12/14/recent-postmac-notes.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/postmac/2016/12/14/recent-postmac-notes.html</guid>
        
        <category>postmac</category>
        
        <category>osx</category>
        
        
        <category>postmac</category>
        
      </item>
    
      <item>
        <title>Adhoc Ansible Example</title>
        <description>I've never been a fan of adhoc ansible -- I'm more of a playbook and role kind of guy but this was pretty useful earlier today:

    ansible -i inventories/machines workers -m shell -a 'python --version'
    
And that gave me the python version on roughly a dozen or so machines allowing me to focus in on the one box where it was was incorrect.</description>
        <pubDate>Wed, 14 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ansible/2016/12/14/adhoc-ansible-example.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ansible/2016/12/14/adhoc-ansible-example.html</guid>
        
        <category>ansible</category>
        
        <category>adhoc</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Apple MacBook Pro Alternatives</title>
        <description>Well I'm certainly not the only one who is curious about life beyond the Macbook:

* [Medium Article](https://medium.com/broken-window/my-search-for-a-macbook-pro-alternative-e549ea2b2dee#.j80auzeup)
* [Hacker News Discussion](https://news.ycombinator.com/item?id=13142754)

Surprisingly he didn't even mention [System 76](https://system76.com/) which makes native Ubuntu laptops.  </description>
        <pubDate>Sat, 10 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/postmac/2016/12/10/apple-macbook-pro-alternatives.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/postmac/2016/12/10/apple-macbook-pro-alternatives.html</guid>
        
        <category>postmac</category>
        
        <category>apple</category>
        
        <category>mac</category>
        
        
        <category>postmac</category>
        
      </item>
    
      <item>
        <title>The Incredible Smartness of Bezos or A New Revenue Stream for Amazon with Amazon Go</title>
        <description>Before I made a full commitment this past summer to AWS, I read [The Everything Store](https://www.amazon.com/Everything-Store-Jeff-Bezos-Amazon/dp/0316219282/ref=sr_1_1?ie=UTF8&amp;qid=1481104973&amp;sr=8-1&amp;keywords=The+Everything+Store) which is a detailed analysis of how Amazon operates and Jeff Bezos personally.  As someone who has been an entrepreneur on many different occasions, I find this type of book particularly compelling and this particular book was no exception.  What I walked away from the book were these observations: 

1.  Jeff Bezos is at the essential core of Amazon's success; yes there are many other talented executives but at its heart it is Jeff's place.
2.  Jeff Bezos is a once in a century type businessman and if I don't think of in the same light as Gates, Rockefeller and Carnegie then I'm making a massive mistake.
3.  It is far, far smarter for me to commit to AWS than it is for me to fight AWS.

And that, dear reader, is why I went full scale with AWS.  Yes there were [technical reasons](https://fuzzygroup.github.io/blog/aws.html) as well but at its heart it was a bet on Bezos.

# What is this Categorized as Startup

Given that Amazon is literally a globe spanning entity with aggregate revenues north of 100 billion, you may find it odd that my primary category for this piece is startup.  Well Amazon, at least to me, just plain acts like a startup.  The level of innovation that the company consistently delivers is breath taking.  I mean what other big company launches new things at the pace that Amazon does?  Honestly Amazon is a startup at least in my mind.

# Amazon Go

And now I come to yesterday's announcement of [Amazon Go](https://www.amazon.com/b?node=16008589011#) - [Hacker News Discussion](https://news.ycombinator.com/item?id=13105689), a complete re-thinking of grocery stores.  Amazon has announced that they will be building 2,000 local grocery stores which are basically a complete rethinking of the grocery store concept implemented along the lines of a convenience store.  Particularly interesting to me since I grew up working retail were these ideas:

* no cashier
* wave your phone when you enter to authenticate yourself
* shop by putting items in your basket; take whatever you want
* amazon figures out what you bought and automatically charges your credit card 

If you think about all the various services that Amazon offers through AWS this is a clear implementation of those services applied to the physical world.  In specific I'd place a good bet that this will be powered by [GPU instances](https://aws.amazon.com/about-aws/whats-new/2016/09/introducing-amazon-ec2-p2-instances-the-largest-gpu-powered-virtual-machine-in-the-cloud/) using some kind of machine vision API that you'll see announced next year at the next Amazon developer conference. And I'd be surprised if we don't see the store proactively reaching out to people's phones using the new [Amazon Lex](https://aws.amazon.com/lex/) ChatBot tool to say something like:

&gt; I see that you picked up and put down three different types of red meat.  In case it helps make your buying decision we have a 10% instant coupon on any type of red meat you want.  

And this would be dynamically coupled with an [Amazon Redshift](https://aws.amazon.com/redshift/) warehouse analysis of inventory trends and if meat is going to spoil or not.  Amazon could entirely automate what a talented meat department manager does for a grocery store only better because the special is dynamically delivered to customers who are much more likely to buy.

# A New Revenue Stream for Amazon

The new revenue stream that I see coming out of this is consumer marketing data.  My strong guess is that Amazon will be able to roll up data from actions like pick up and put down on an individual consumer level and then see that type of analytics back to consumer goods firms.  Now I'm sure that Amazon already licenses some consumer marketing data to companies but I suspect this is going to be fairly unprecedented.  Let's say that you are a consumer goods company that just rolled out a new label.  Here's what I can envision happening:

1.  New label rolls out.
2.  Amazon is able to give you time adjusted comparisons of shopping experiences before and after along with being able to watch the consumers interact with the product.
3.  Think of this as a/b analytics for the real world.

I doubt there's a single person reading this who, whether or not they believe in a/b testing, doesn't understand the power it offers.  Personally I'm not a fan but as a marketer I can see this being truly compelling.

# My Wife's Reaction

My wife isn't part of the high tech world anymore but she had an interesting reaction:

&gt; All restaurants are going to be Taco Bell!

Props to her for an outstanding nerd culture reference, the 1993 [Demolition Man](http://www.imdb.com/title/tt0106697/?ref_=nv_sr_1) movie starring Stallone, Bullock and Snipes of a dystopian future where literally all restaurants are Taco Bell.  Perhaps all commerce will be Amazon...</description>
        <pubDate>Wed, 07 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2016/12/07/the-incredible-smartness-of-bezos-or-a-new-revenue-stream-for-amazon-with-amazon-go.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2016/12/07/the-incredible-smartness-of-bezos-or-a-new-revenue-stream-for-amazon-with-amazon-go.html</guid>
        
        <category>amazon</category>
        
        <category>startup</category>
        
        <category>bezos</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>No Rdoc Once and For All</title>
        <description>After watching me swear like a sailor earlier today, [Dv](http://www.dasari.me/) provided this snippet:

    cat ~/.gemrc
    install: --no-rdoc --no-ri
    update: --no-rdoc --no-ri

Never again will I have to wait for rdoc to generate (which at times takes longer than the damn gem itself).  Huzzah!
</description>
        <pubDate>Tue, 06 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/ruby/2016/12/06/no-rdoc-once-and-for-all.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/ruby/2016/12/06/no-rdoc-once-and-for-all.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>gem</category>
        
        <category>rdoc</category>
        
        
        <category>ruby</category>
        
      </item>
    
      <item>
        <title>Code Rot is Very, Very Real</title>
        <description>Today has been an unmitigated disaster.  In the process of reviving a [MacBook Pro that was old](https://fuzzygroup.github.io/blog/osx/2016/12/06/bringing-an-old-macbook-pro-back-to-life.html) I had to delete and re-install rvm, gemsets, etc and, **damn**, has it ever sucked.  And I don't mean &quot;oh it sucked&quot; -- I mean this has sucked monkey balls.  I've been at it 12 hours and my conclusion is that I'm not sure that it will ever work.  And this brings us to **code rot**.  Rot is what happens to a physical artifact when you don't take care of it.  Now if you haven't been at this for too long you're thinking something like: 

&gt; These are perfect digital assets how can there possibly be rot; Scott is smoking the whacky weed.

Alas, no and I wish I was.  Even if your digital assets don't decay, the supporting ecosystem that they are in actually does change.  As an example part of the Gemfile had a nokogiri 1.4.4 hard coded version number in it.  Now I might refer to Nokogiri as NokoVietnameGiri since I'm generally about as successful in installing Nokogiri as the U.S. was in winning Vietnam.  But today was far, far worse.  Finally I discovered that Nokogiri 1.4.x is now considered too old and things have changed too much and their recommendation is simple -- **upgrade**.

Note: No disrespect meant to anyone or their family involved in Vietnam.  Vietnam was a political war and soldiers were not allowed to win it.

And that's what I mean by code rot -- all executable code these days depends on a precarious web of dependencies.  If you're not actively maintaining it then you are very, very much hosed should a sufficient amount of time pass and you need to re-install things.

In this case I have to go into a code base that has laid fallow for well over a year now.  And while it has run like a champ until recently, certain things have changed and I now need to actively make changes to the code, modify vendored gems, etc.  And that means I really, really need a workable code base.  And without the ability to run bundle install, well, I've got nothing.

Since I grew up in a family that made their living from, among other things, real estate, I'm going to make an analogy.  If you owned physical real estate, you wouldn't dream of not touching it for more than a year -- you'd expect to do small maintenance, plumbing,  paint, weatherstrip, whatever.  And if it was a revenue producing property, as this was a revenue producing bit of software, you would really never dream of letting it lie fallow for a year.  

So code rot is real but I don't think that it really is any different from physical artifacts.  The universe does, after all, trend from order to disorder, towards ever increasing entropy.  

The bottom line here is that the next time someone tells you that its ok to let code just sit around and decay, perhaps you might advocate more strongly for doing at least some maintenance on it.  I strongly wish that I had done a better job of that.  *regrets*</description>
        <pubDate>Tue, 06 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/rails/2016/12/06/code-rot-is-very-very-real.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/rails/2016/12/06/code-rot-is-very-very-real.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>software_engineering</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Bringing an Old MacBook Pro Back to Life</title>
        <description>This morning has been more than a tad frustrating -- and let's leave Tad out of this, shall we?  I've been engaged in bringing my 2011 MacBook Pro 15 back to life.  After a brief venture into Linux, I found that I really needed the data on this box back and despite my Time Machine woes I was able to restore most everything.  I'm now trying to make it actually a functional development tool since it is the only machine that generally seems to run this particular obsolete gem stack -- although [Dv](http://www.dasari.me/) would argue I should just kill everything and run Vagrant -- something I'm thinking about.

Here's the process I've been thru:

* kill brew and reinstall it: **ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall)&quot;** and then **ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;**
* kill rvm and re-install it: **rvm implode** and then **\curl -sSL https://get.rvm.io | bash** [Docs](https://richonrails.com/articles/uninstalling-rvm)
* install cask room: **brew cask install google-chrome** [Docs](https://caskroom.github.io/)
* kill bundler and re-install it:  **gem install bundler**
* kill gem and reinstall it: **gem update --system '2.3.0'** [SO](http://stackoverflow.com/questions/13626143/how-to-upgrade-rubygems)

I then updated my [Ansible MacBook Pro configuration routine](https://github.com/fuzzygroup/ansible-macbook-pro) and ran it against the box.  And then I hit a high degree of [code rot](https://fuzzygroup.github.io/blog/rails/2016/12/06/code-rot-is-very-very-real.html) that, well, totally ruined my day.  </description>
        <pubDate>Tue, 06 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2016/12/06/bringing-an-old-macbook-pro-back-to-life.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2016/12/06/bringing-an-old-macbook-pro-back-to-life.html</guid>
        
        <category>osx</category>
        
        <category>mac</category>
        
        <category>rvm</category>
        
        <category>ruby</category>
        
        <category>bundler</category>
        
        <category>gem</category>
        
        <category>brew</category>
        
        <category>cask</category>
        
        <category>mysql</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Accessing MariaDB without sudo on Ubuntu</title>
        <description>So I'm finally able to get back to trying Rails development on my Ubuntu box and the first real error I found was when I tried to create a database -- I couldn't get into MariaDB unless I used sudo -- very, very odd.  I found the solution on [Ubuntu Forums](https://ubuntuforums.org/showthread.php?t=2275033):

    sudo mysql -u root
    use mysql;
    update user set plugin='' where User='root';
    flush privileges;
    \q

Honestly I have no idea what this is.  Sigh.  Oh well it works and it is documented at least.</description>
        <pubDate>Tue, 06 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/mariadb/2016/12/06/accessing-mariadb-without-sudo-on-ubuntu.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mariadb/2016/12/06/accessing-mariadb-without-sudo-on-ubuntu.html</guid>
        
        <category>mariadb</category>
        
        <category>ubuntu</category>
        
        <category>sudo</category>
        
        
        <category>mariadb</category>
        
      </item>
    
      <item>
        <title>MySQL and Time Machine Woes Part 2</title>
        <description>So I'm not alone with MySQL Time Machine woes.  My buddy [Dv](http://dasari.me) skyped me last night along the lines of &quot;how do I get my MySQL stuff off Time Machine&quot;.  Sheesh -- it is an epidemic.  He is having the same type of virtual environment woes that I am where rvm / rbenv / chruby seem to cause nothing but OSX issues and he is looking to Vagrant for an answer.  This means that he's likely doing the re-install dance.  And since he has been a stellar trooper in assisting me for years with all kinds of random ass crap, it is my turn.

# Where Are My MySQL Databases?

The first step towards getting your databases back from Time Machine is to locate where they are supposed to be.  MySQL itself can tell you this:

    SHOW VARIABLES WHERE Variable_Name LIKE &quot;%dir&quot;;
    +---------------------------+------------------------------------------------------+
    | Variable_name             | Value                                                |
    +---------------------------+------------------------------------------------------+
    | basedir                   | /usr/local/Cellar/mysql/5.6.21                       |
    | character_sets_dir        | /usr/local/Cellar/mysql/5.6.21/share/mysql/charsets/ |
    | datadir                   | /usr/local/var/mysql/                                |
    | innodb_data_home_dir      |                                                      |
    | innodb_log_group_home_dir | ./                                                   |
    | lc_messages_dir           | /usr/local/Cellar/mysql/5.6.21/share/mysql/          |
    | plugin_dir                | /usr/local/Cellar/mysql/5.6.21/lib/plugin/           |
    | slave_load_tmpdir         | /var/folders/rf/3tfhwgrj1sl85y6rcs4x_s5c0000gn/T/    |
    | tmpdir                    | /var/folders/rf/3tfhwgrj1sl85y6rcs4x_s5c0000gn/T/    |
    +---------------------------+------------------------------------------------------+

So we now know that our databases are supposed to be stored in /usr/local/var/mysql/.  That's what we need to get them back from Time Machine.

# Important STOP MYSQL First

The very first thing you need to do is stop mysql first:

    mysql.server stop

# Getting Them Back from Time Machine

Open a Terminal window and cd /Volumes and then find your Time Machine latest backup and change into /usr/local/var/mysql underneath it.  You should then theoretically be able to copy the back with a cp -r * /usr/local/var/mysql as per this [Stack Overflow post](http://apple.stackexchange.com/questions/139175/transferring-a-single-folder-from-a-time-machine-to-a-different-mac).  Please note that I have not done this myself yet since for my issues I cannot find the damn backup ever being made.

# Recommendation - DO NOT Restore to Previous Location - Do This Instead

After my personal issues with Time Machine and MySQL I no longer have faith that any data outside of /User/my_username will be correctly restored.  Personally I would make a directory structure like this:

    /Users/sjohnson
    /Users/sjohnson/servers
    /Users/sjohnson/servers/mysql
    /Users/sjohnson/servers/mysql/data
    /Users/sjohnson/servers/postgres
    /Users/sjohnson/servers/postgres/data
    /Users/sjohnson/servers/redis
    /Users/sjohnson/servers/redis/data
    /Users/sjohnson/servers/mongo
    /Users/sjohnson/servers/mongo/data
    
As best I can tell, I have lost every mysql database I've built since 2011 on my main development system and the only reason I'm not raging further is that I just don't have the mental energy given everything else I'm juggling.  I do think that if I do this approach then when Apple gets stupid in the future with Time Machine that this will protect me.

If you're going to this then you need to follow the steps below.  Alas I am getting a bit theoretical here since I haven't had time to do this for myself yet.

## Finding my.cnf

After HomeBrew installs mysql for you it creates a shell routine mysql.server.  Find it with which:

    which mysql.server
    /usr/local/bin/mysql.server
    
This file tells mysql where to store its data and you're going to need to:

* stop mysql 
* edit this shell script
* edit /etc/my.cnf which also seems to store this location

I don't know which is the primary and which is the default so personally I'd edit both.  You would then need to:

* adjust permissions and groups accordingly 
* adjust postgres, redis and mongo / anything else as well </description>
        <pubDate>Sat, 03 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/mysql/2016/12/03/mysql-and-time-machine-woes-part-2.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mysql/2016/12/03/mysql-and-time-machine-woes-part-2.html</guid>
        
        <category>mysql</category>
        
        <category>time_machine</category>
        
        <category>backup</category>
        
        <category>mac</category>
        
        <category>osx</category>
        
        
        <category>mysql</category>
        
      </item>
    
      <item>
        <title>More Ansible on AWS for Sidekiq</title>
        <description>So, much like the Godfather, just when I think I'm out, Ansible pulls me back in...  Yesterday I tackled using Ansible to manage Sidekiq.  Today I'm going to bring you:

* Installing Sidekiq as a service
* Viewing the status of the service

Let's start with installing Sidekiq as a service.  Happily [Mike Perham](http://www.sidekiq.org) has built Sidekiq to run as a service (and thanks to Mike for the service configuration; noted and appreciated).  Here's what you need:

    playbook: playbook_service_install_and_start_sidekiq.yml
    
    role: service_install_and_start_sidekiq.yml
    
    ---
    - name: prevent sidekiq init from running if it has already been done
      stat: path=/etc/init/sidekiq.conf
      register: sidekiq_init_installed

    - name: Copy sidekiq init template to init.d dir
      template: src=sidekiq_perham_init.j2 dest=/etc/init/{{ app.process_name }}_worker.conf owner=root group=root force=yes
      template: src=sidekiq_perham_init.j2 dest=/etc/init/sidekiq.conf owner=root group=root force=yes
      sudo: yes
      when: sidekiq_init_installed.stat.exists == False

    - name: start_sidekiq
      service: name=sidekiq state=started enabled=yes
    
    template: sidekiq_perham_init.js
    
    # /etc/init/sidekiq.conf - Sidekiq config
    # source: https://github.com/mperham/sidekiq/blob/master/examples/upstart/sidekiq.conf

    # This example config should work with Ubuntu 12.04+.  It
    # allows you to manage multiple Sidekiq instances with
    # Upstart, Ubuntu's native service management tool.
    #
    # See workers.conf for how to manage all Sidekiq instances at once.
    #
    # Save this config as /etc/init/sidekiq.conf then manage sidekiq with:
    #   sudo start sidekiq index=0
    #   sudo stop sidekiq index=0
    #   sudo status sidekiq index=0
    #
    # Hack Upstart's reload command to 'quiet' Sidekiq:
    #
    #   sudo reload sidekiq index=0
    #
    # or use the service command:
    #   sudo service sidekiq {start,stop,restart,status}
    #

    description &quot;Sidekiq Background Worker&quot;

    # This script is not meant to start on bootup, workers.conf
    # will start all sidekiq instances explicitly when it starts.
    #start on runlevel [2345]
    #stop on runlevel [06]

    # change to match your deployment user
    setuid {{ user_name }}
    setgid {{ user_name }}
    env HOME={{ app_path }}

    respawn
    respawn limit 3 30

    # TERM is sent by sidekiqctl when stopping sidekiq. Without declaring these as
    # normal exit codes, it just respawns.
    normal exit 0 TERM

    # Older versions of Upstart might not support the reload command and need
    # this commented out.
    reload signal USR1

    # Upstart waits 5 seconds by default to kill the a process. Increase timeout to
    # give sidekiq process enough time to exit.
    kill timeout 15

    #instance $index
    instance 0

    script
    # this script runs in /bin/sh by default
    # respawn as bash so we can source in rbenv
    exec /bin/bash &lt;&lt;'EOT'
      # Pick your poison :) Or none if you're using a system wide installed Ruby.
      # rbenv
      # source /home/apps/.bash_profile
      # OR
      # source /home/apps/.profile
      # OR system:
      # source /etc/profile.d/rbenv.sh
      #
      # rvm
      # source /home/apps/.rvm/scripts/rvm
      source {{ rvm_path }}

      # Logs out to /var/log/upstart/sidekiq.log by default

      cd {{ app_path }}
      #exec bundle exec sidekiq -i ${index} -e production
      exec bundle exec sidekiq -i 0 -e production
    EOT
    end script
    
    output: 
    
    ansible-playbook -i inventories/production2 playbook_service_install_and_start_sidekiq.yml
     [WARNING]: While constructing a mapping from
    /Users/sjohnson/Dropbox/appdatallc/ansible/roles/service_install_and_start_sidekiq/tasks/main.yml, line 6, column 3, found a duplicate dict
    key (template). Using last defined value only.

    [DEPRECATION WARNING]: Instead of sudo/sudo_user, use become/become_user and make sure become_method is 'sudo' (default).
    This feature will
    be removed in a future release. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

    PLAY [worker] ******************************************************************

    TASK [setup] *******************************************************************
    ok: [fiworker3]

    TASK [service_install_and_start_sidekiq : prevent sidekiq init from running if it has already been done] ***
    ok: [fiworker3]

    TASK [service_install_and_start_sidekiq : Copy sidekiq init template to init.d dir] ***
    skipping: [fiworker3]

    TASK [service_install_and_start_sidekiq : start_sidekiq] ***********************
    changed: [fiworker3]

    PLAY RECAP *********************************************************************
    fiworker3                  : ok=3    changed=0    unreachable=0    failed=0
    

Once you install this as service you **do not** want to log into N machines and check if its manually running so now you need this:

    playbook: playbook_service_status_sidekiq.yml
    
    role: service_status_sidekiq.yml
    
    --- 
  
    - name: display sidekiq's status
      shell: &quot;service sidekiq status&quot;
      register: out

    - name: view the output
      debug: var=out.stdout_lines
    
    output:

    ansible-playbook -i inventories/production2 playbook_service_status_sidekiq.yml

    PLAY [worker] ******************************************************************

    TASK [setup] *******************************************************************
    ok: [fiworker3]

    TASK [service_status_sidekiq : display sidekiq's status] ***********************
    changed: [fiworker3]
     [WARNING]: Consider using service module rather than running service

    TASK [service_status_sidekiq : view the output] ********************************
    ok: [fiworker3] =&gt; {
        &quot;out.stdout_lines&quot;: [
            &quot;sidekiq (0) start/running, process 7759&quot;
        ]
    }

As long as you see a process id here then you know its running.  Would it be better to have this report out something like &quot;Out of N boxes, sidekiq is running on N-1&quot;?  Sure.  And I suspect that I will get there at some point but I have critical pressing needs **today** -- all these examples are being written as part of actual devops that I'm doing now.  These blog posts are the time stamped documentation that I can refer back to.</description>
        <pubDate>Sat, 03 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/12/03/more-ansible-on-aws-for-sidekiq.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/12/03/more-ansible-on-aws-for-sidekiq.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>sidekiq</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>When MySQL Loads Go Wrong - What Do You Do?</title>
        <description>This piece is the natural follow on to my [Adventures in Partition Resizing](https://fuzzygroup.github.io/blog/aws/2016/11/26/fear-and-loathing-in-awsville-or-adventures-in-partition-resizing.html) epic.  At this point our systems still aren't fully back up yet and, well, it tastes like ashes.  Like a lot of hard core techies I tend to internalize the state of my system and when its not right, well, things suck for me.  

The current state of things is that out of 187 tables that needed to be migrated over, we're down to the last one -- big number 187 and nothing appears to be happening.  All I see at the console is the same thing I've seen for over 24 hours:

    ubuntu@ip-172-31-43-176:~$ mysql -uappdata -pSOMEPASSWORD foo_production &lt; /mnt/old2/foo2016_m2s.sql

And it just sits there.  Sigh.  Figuring out the next logical step means that you need to wrap your head around the quirky aspects of the MySQL load architecture.  Happily this isn't the first time for me so here goes.

# Step 1 - SHOW FULL PROCESS LIST;

When you have a large table coming in the command line like this, well, MySQL gets seriously weird.  You can't select a row to see the status.  The best you can do is a SHOW FULL PROCESS LIST which makes the system go nuts.  Seriously - try it sometime.  What you get is an enormous SQL dump showing what's being loaded and since it scrolls by at like 90 miles per hour, well, that's not helpful.  Here's a little trick tho:

    pager less;
    SHOW FULL PROCESS LIST\G

And this will give you something like this:

    *************************** 1. row ***************************
          Id: 257
        User: appdata
        Host: localhost
          db: foo_production
     Command: Query
        Time: 0
       State: update
        Info: INSERT INTO `foo2016_q1s` VALUES (240213,'2016-03-16 21:14:57','2016-
        
Given that there 927,040 records in this table what this has shown us is that after 36 hours or so we're only 1/4 of the way thru.  Press ESC and run SHOW FULL PROCESS LIST a few times to make sure that id value keeps changing to get confirmation that the load is continuing.

The only problem is that what I just wrote above may **NOT** be reliable -- emphasis on the **NOT**.  Here's what I just got when I ran it again:

    Id: 257
        User: appdata
        Host: localhost
          db: banks_production
     Command: Query
        Time: 0
       State: update
        Info: INSERT INTO `foo2016_q1s` VALUES (406347,'

There's no way that roughly 200K records went in while I was writing two paragraphs so what we now know is that SHOW FULL PROCESS LIST tells you that the load is proceeding but it may be **utterly useless** as a monitoring tool.  Deep, Deep Sigh.

# Step 2 - DESC foo2016_m2s

The next logical step is to check the schema to make sure nothing weird is going on.  This can be done with:

    DESC foo2016_m2s;
    
In MySQL checking the schema tells you nothing about the index structures so you can check those with:

    SHOW CREATE TABLE foo2016_m2s;
    
My guess is that you won't learn anything from these operations but this is step by step, high value debugging so you check everything.

# Step 3 - Options

The logical next step would be to:

* stop the load
* delete the last record as it might not be complete
* find the last id (thank heavens that ids are sequential)
* re-dump from the original source table from the last record + 1 forward
* re-import

If, Great Ghu Help Us, we find that stopping the load means that there isn't any data in the table then the next step would be:

* Export from the original source in say 100K record chunks
* Import the first one and see what happens
* Import the next one and see what happens
* Think carefully if we want to proceed sequentially or move to loading files in parallel; if the issue is bad data somewhere in a load file then parallel is a bad, bad, bad idea.
    
# Step 4 - Patience

Happily, whilst writing this screed, I looked up and noticed &quot;oh hell -- it finished!&quot;.  And that's the happiest of all results.  Hopefully what this post illustrates is how to tackle long running load processes even if I didn't have to act on any of them.</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/mysql/2016/12/02/when-mysql-loads-go-wrong-what-do-you-do.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mysql/2016/12/02/when-mysql-loads-go-wrong-what-do-you-do.html</guid>
        
        <category>mysql</category>
        
        
        <category>mysql</category>
        
      </item>
    
      <item>
        <title>Killing Sidekiq on AWS with Ansible</title>
        <description>We do a lot of batch data processing here and much of that involves shoving crap into [Sidekiq](http://sidekiq.org/) to be run in a threaded fashion.  I'm not done with my own threading stuff yet by any means but, for now, Sidekiq is a superstar so we're going to use it and bow west towards the [Mike Perham](http://www.mikeperham.com/) alter of threaded awesomeness that is Sidekiq.

One issue we had recently was that we thought we had killed sidekiq dead but, since it was running as a background service instead of a foreground task, it popped back up and kept happily eating data.  This would have been fine except for the fact that we had a db dump and restore in process and this caused data to be in an inconsistent state.  And that led to a second round of table dump / restore tangoing.  

The first thing to understand here is that this is **my** mistake.  I'm the moron who set it as a background service after all and, in the utter panic that accompanies disaster recovery, well, I forgot.  I did my normal kill -9 dance and went on my merry way ignoring the fact that Ubuntu would happily cackle gleefully as it re-launched it.  *Sigh*.

So there's at least two playbooks here:

* playbook_status_sidekiq.yml
* playbook_stop_sidekiq_with_prejudice.yml

The &quot;_with_prejudice&quot; refers to stopping Sidekiq with a -9 argument to kill.  This tells Linux &quot;really, truly, right DAMN NOW kill this process&quot;.  Sometimes Sidekiq fails to stop; often because of the ruby code its executing and while that's fine there are lots of times that you just need it to go away.  This is one of them.

Two additional related playbooks I could write are:

* playbook_stop_sidekiq.yml
* playbook_service_stop_sidekiq.yml

And here we go!  The first thing we need is the ability to know what's going on in our cluster of boxes.  This means the ability to know if sidekiq is running:

    playbook:
    
    role:
    
    output:
    
    ansible-playbook -i inventories/production2 playbook_status_sidekiq.yml

    PLAY [worker] ******************************************************************

    TASK [setup] *******************************************************************
    ok: [fiworker5]
    ok: [fiworker6]
    ok: [fiworker3]
    ok: [fiworkerbig]
    ok: [fiworker4]
    ok: [fiworker8]
    ok: [fiworker7]
    ok: [fiworker9]
    ok: [fiworker10]
    ok: [fiworker11]

    TASK [status_sidekiq : display sidekiq's status] *******************************
    changed: [fiworker5]
    changed: [fiworker3]
    changed: [fiworkerbig]
    changed: [fiworker4]
    changed: [fiworker6]
    changed: [fiworker7]
    changed: [fiworker8]
    changed: [fiworker10]
    changed: [fiworker9]
    changed: [fiworker11]

    TASK [status_sidekiq : view the output] ****************************************
    ok: [fiworker5] =&gt; {
        &quot;out.stdout_lines&quot;: [
            &quot;root     11103  0.0  0.0   4440   636 pts/5    S+   12:43   0:00 /bin/sh -c ps auwwx | grep sidekiq&quot;,
            &quot;root     11105  0.0  0.0  10460   912 pts/5    S+   12:43   0:00 grep sidekiq&quot;,
            &quot;ubuntu   17371  0.0  0.0  27920  5328 ?        Ss   Oct27  13:57 tmux new -s sidekiq&quot;,
            &quot;ubuntu   25815  0.6  6.5 1933952 1022068 pts/1 Sl+  Dec01   7:13 sidekiq 4.2.3 banks [0 of 25 busy] stopping                                                                         &quot;
        ]
    }
    ok: [fiworker6] =&gt; {
        &quot;out.stdout_lines&quot;: [
            &quot;ubuntu   17308  5.7  6.1 1948728 957972 pts/1  Sl+  Dec01  63:18 sidekiq 4.2.3 banks [25 of 25 busy]                                                                                 &quot;,
            &quot;root     18126  0.0  0.0   4440   636 pts/5    S+   12:43   0:00 /bin/sh -c ps auwwx | grep sidekiq&quot;,
            &quot;root     18128  0.0  0.0  10460   912 pts/5    S+   12:43   0:00 grep sidekiq&quot;,
            &quot;ubuntu   23040  0.0  0.0  31808  9200 ?        Ss   Oct27  15:12 tmux new -s sidekiq&quot;
        ]
    }
    
As you can see in the first bit out output, fiworker5, I missed when I manually shut stuff down yesterday.  Oops.  And this brings us to our next playbook:

    playbook:
    #
    # MONDAY ansible-playbook -i ec2.py playbook_stop_sidekiq_with_prejudice.yml
    # ansible-playbook -i inventories/production2 playbook_stop_sidekiq_with_prejudice.yml
    #
    ---
  
    - hosts: worker
      become: yes
      remote_user: ubuntu
      gather_facts: true
      roles:
        - { role: kill_sidekiq_with_prejudice, tags: sidekiq}
    
    role:
    ---
    - name: kill_sidekiq_with_prejudice
      shell: ps -ef | grep sidekiq | grep -v grep | awk '{print $2}' | xargs kill -9
    
    output:
    (fiworker5 shut down on its own before this ran; sigh)
    
    ansible-playbook -i inventories/production2 playbook_stop_sidekiq_with_prejudice.yml

    PLAY [worker] ******************************************************************

    TASK [setup] *******************************************************************
    ok: [fiworker3]
    ok: [fiworkerbig]
    ok: [fiworker5]
    ok: [fiworker4]
    ok: [fiworker6]
    ok: [fiworker7]
    ok: [fiworker9]
    ok: [fiworker10]
    ok: [fiworker8]
    ok: [fiworker11]

    TASK [kill_sidekiq_with_prejudice : kill_sidekiq_with_prejudice] ***************
    changed: [fiworker5]
    changed: [fiworker6]
    changed: [fiworker3]
    changed: [fiworker4]
    changed: [fiworkerbig]
    changed: [fiworker7]
    changed: [fiworker9]
    changed: [fiworker8]
    changed: [fiworker10]
    changed: [fiworker11]

    PLAY RECAP *********************************************************************
    fiworker10                 : ok=2    changed=1    unreachable=0    failed=0
    fiworker11                 : ok=2    changed=1    unreachable=0    failed=0
    fiworker3                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker4                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker5                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker6                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker7                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker8                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker9                  : ok=2    changed=1    unreachable=0    failed=0
    fiworkerbig                : ok=2    changed=1    unreachable=0    failed=0

Given my [previous praise of pkill](https://fuzzygroup.github.io/blog/unix/2016/11/23/pkill-rocks.html), readers may be wondering why I used the old xargs trick.  Simply put I couldn't make pkill work.  There are google posts on the topic but I didn't have time to dig into it -- I **knew** that xargs had to work so I went with it.  Honestly I dont understand why Ansible doesn't have a process module -- it just seems so absolutely needed.

As with my previous example if there is interest, on Monday, I'll publish examples showing the dynamic inventory version of this.
</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/12/02/killing-sidekiq-on-aws-with-ansible.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/12/02/killing-sidekiq-on-aws-with-ansible.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>sidekiq</category>
        
        <category>rails</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Clearing Rails Log Files on AWS with Ansible</title>
        <description>If you are reading this and you've read more than a few things I've written then you know that I'm, well, a *wordy son of a bitch*.  I mean come on -- did you see the post on [AWS Partition Resizing](https://fuzzygroup.github.io/blog/aws/2016/11/26/fear-and-loathing-in-awsville-or-adventures-in-partition-resizing.html)?  I took like 7800 words to say &quot;can't do it&quot; -- apologies.  

Anyway the digression into wordiness is because like me, Rails log files are wordy son's of bitches -- the number of times in my career that I've had to log into a box and do the log dance:

    bundle exec rake log:clear
    
has to be measured in the hundreds if not thousands.  This morning I finally got mad enough to automate it with Ansible.  Here's how:

    playbook: playbook_logs_clear_rails_logs.yml
    
    #
    # MONDAY ansible-playbook -i ec2.py playbook_logs_clear_rails_logs.yml
    # ansible-playbook -i inventories/production2 playbook_logs_clear_rails_logs.yml
    #
    ---
  
    - hosts: worker
      become: yes
      remote_user: ubuntu
      gather_facts: true
      vars: 
        - app_path: /var/www/apps/banks/current
      roles:
        - { role: logs_clear_rails_logs, tags: logs}
    
    role: 
    folder: logs_clear_rails_logs
    file: tasks/main.yml
    
    ---
    - name: logs_clear_rails_logs
      shell: &quot;cd {{ app_path }} &amp;&amp; bundle exec rake log:clear&quot;
    
Here are the results:

    ansible-playbook -i inventories/production2 playbook_logs_clear_rails_logs.yml

    PLAY [worker] ******************************************************************

    TASK [setup] *******************************************************************
    ok: [fiworkerbig]
    ok: [fiworker5]
    ok: [fiworker3]
    ok: [fiworker6]
    ok: [fiworker4]
    ok: [fiworker9]
    ok: [fiworker8]
    ok: [fiworker11]
    ok: [fiworker10]
    ok: [fiworker7]

    TASK [logs_clear_rails_logs : logs_clear_rails_logs] ***************************
    changed: [fiworker3]
    changed: [fiworker6]
    changed: [fiworker5]
    changed: [fiworker4]
    changed: [fiworkerbig]
    changed: [fiworker7]
    changed: [fiworker8]
    changed: [fiworker9]
    changed: [fiworker10]
    changed: [fiworker11]

    PLAY RECAP *********************************************************************
    fiworker10                 : ok=2    changed=1    unreachable=0    failed=0
    fiworker11                 : ok=2    changed=1    unreachable=0    failed=0
    fiworker3                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker4                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker5                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker6                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker7                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker8                  : ok=2    changed=1    unreachable=0    failed=0
    fiworker9                  : ok=2    changed=1    unreachable=0    failed=0
    fiworkerbig                : ok=2    changed=1    unreachable=0    failed=0
    
Right now this is working with a static inventory file.  If anyone expresses interest, on Monday, I'll publish a revised version which uses the python boto module to clear log files based on all boxes on EC2 dynamically matching a criteria like a name tag.</description>
        <pubDate>Fri, 02 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/12/02/clearing-rails-log-files-on-aws-with-ansible.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/12/02/clearing-rails-log-files-on-aws-with-ansible.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>logs</category>
        
        <category>rails</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Debugging Christmas Lights</title>
        <description>In any family, over time, you specialize in your roles.  This is similar to a work environment where one person gets good at an aspect of the code base and then that person gets tasked with it on a regular basis.  In an ideal world everyone would always be 100% capable at all aspects of the code but we all know that's generally not the case.  In our family, Dad (me) puts up the Christmas lights.  Part of it is that I don't mind heights terribly, not the same as saying I like them but you get it but the other part of it is that *debugging crap that doesn't work* is really my very job definition.  And, a few days ago I wanted this:

![debugging_xmas_lights.jpg](/blog/assets/debugging_xmas_lights.jpg)

but after 10 minutes or so I got this instead:

![debugging_xmas_lights2.jpg](/blog/assets/debugging_xmas_lights2.jpg)

And while I was initially reminded of the Bob Rivers / Twisted Christmas album - &quot;I plug in one light and THEY ALL #$#(*$*#) go OUT&quot;, that wasn't this.  Everything worked perfectly -- until it all stopped.  Sigh.

The very essence of debugging is the quest for understanding and your key tool is *persistence*.  I've worked with a lot of smart people in my life, many of them dramatically smarter than I am but I haven't worked with many people who are as persistent.  Whether you are debugging software, hardware or even christmas lights, *persistence* is what you want.  If you just think of yourself as a dog with a bone, that's the right image.  

Ok then.  The first step was to get out the tools.  Happily I recently bought a [non-contact AC voltage tester from Amazon](https://www.amazon.com/gp/product/B019EJXUJU/ref=oh_aui_search_detailpage?ie=UTF8&amp;psc=1):

![debugging_xmas_lights2.jpg](/blog/assets/non_contact_tester.jpg)

What this does is essentially *sniff* electricity.  I'm not enough of a hardware guy to really understand it but I tried it and it works!  Given that I was doing this on a rainy day, not having to manipulate two test leads or a digital volt ohm meter was a huge, huge win.  This little gadget is actually sensitive enough that you can touch it to the end of a lightning cable and it will detect the voltage there so, apparently, it does both DC and AC.  Oh and did I mention it has an LED pen light built into it?  This thing rocks!

Whenever you have something, related to electricity, that happens after a period of latency, the natural suspect is somehow thermal or load related.  But here was the aggregate load:

* 1 power strip
* 4 strings of relatively low voltage lights
* 1 [Christmas Laser Projector](https://www.amazon.com/gp/product/B01JBTO16E/ref=oh_aui_search_detailpage?ie=UTF8&amp;psc=1)

Two of the strings and the projector were all plugged into a contractor grade, 20 amp, thick as hell extension cord coming from a GFCI outlet.  Call this one Big Green.  The remaining two strings were plugged into a separate contractor grade cord but one that was only 15 amp.  And call this Little Orange.  Both of these have served me thru tree house building, home remodeling, etc.

This isn't sufficient load to trip a breaker.  I grew up doing AC wiring stuff so I have no issues stating that as a fact.

So if we think of this in terms of debugging here are the different components at hand:

* power strip
* light string 1
* light string 2
* light string 3
* light string 4
* extension cord 1 - Big Green
* extension cord 2 - Little Orange
* GFCI outlet

Any one of these, or any combination of these, could be leading to this problem.  And since it only happened after 10 minutes, I knew that at a worst case I was looking at N * 10 minutes in terms of potential maximum time that I'd be trouble shooting this where N was the number of components, 8.  So this was potentially at least a 90 minute plus exercise.  Given that the amount of time involved was now known to be non trivial, it was time to start by eliminating things outright.

Note: My worst case estimate was actually incorrect.  If the problem was combinatorial in nature like string 1 interacting with string 3 and causing the outage, well, I'm glad I didn't realize this -- its a lot more options.

My first task was to remove the power strip.  My wife had wanted to add more lasers to the mix so that was there to support the end goal -- but we don't have them yet so it was a case of over-engineering.  So, remove powerstrip, rejigger wires, try again.  **10 minute and darkness!**

My next step was to look at this in terms of overall complexity.  The most complex component is generally the one most likely to fail.  That meant that I should pull the laser projector from the mix.  Even though it was brand, stinking new, it is definitely more complex than christmas lights so out it goes.  And, once again, **10 minute and darkness!**

The lights are only a year old but they are very poorly made.  Even though they are a name brand, Sylvania, they seem to epitomize the term *shite quality*.  Some of the time just screwing a bulb in blows out the bulb due to poor socket quality.  But my wife loves the visual so the chances of my being able to route these strands of lights to /dev/null approximates zero.  Given that I have issues with the light quality, the next step is to alternately remove each string of lights in turn:

*  **10 minute and darkness!**
*  **10 minute and darkness!**
*  **10 minute and darkness!**
*  **10 minute and darkness!**

*Grumble*.  Ok then so what's left.  At this point we're down to two extensions cords.  Little Orange isn't as robust as big green so let's take that out of the loop - **10 minute and darkness!**

Hm... The message seems clear -- we're down to Big Green so let's un plug it and substitute another one: 

* **10 minute and LIGHT!**
* **10 more minute and still LIGHT!**
* **10 more minute and still LIGHT!**

So we're good now but what went wrong?  To some extent if you debug something and you don't actually understand the failure cause at the end then you may have won the battle but you have perhaps lost the war as the real goal should always be understanding.

When I look a more detailed look at my Big Green extension cord I saw this:

![big_green_big_culprit.jpg](/blog/assets/big_green_big_culprit.jpg)

Apparently I've fixed this extension cord in the past -- and then it all made sense.  If you remember I have this all going into a GFCI outlet and I also mentioned that I was doing all this in the rain.  And that was the vital clue.  GFCI stands for Ground Fault Circuit Interrupter and it is a way for an electrical outlet to automatically shut down in dangerous conditions.  Since a GFCI outlet is more expensive than a standard outlet you don't see them everywhere but they are **required** for use in commercial bathrooms due to the potential issues with respect to water and electricity.  And if your home builder was any good you likely have them in your home bathrooms for the same reason.

So what seems to have been happening was that moisture was slowly affecting the Big Green extension cord.  After about 10 minutes it caused a GFCI issue which led to a power shut down.  Strangely it seemed to clear itself up almost immediately.  Odder still is that when I checked the extension cord with a circuit tester designed to catch GFCI wiring errors it reported just fine.  

The morals of the story seem to be:

* Don't fix your extension cord -- just replace it
* Debugging is the same when its software, hardware or christmas mights -- it call comes down to persistence
* Anything can be debugged, you just have to take it step by step, bit by bit

Merry Christmas!</description>
        <pubDate>Thu, 01 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/debugging/2016/12/01/debugging-christmas-lights.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/debugging/2016/12/01/debugging-christmas-lights.html</guid>
        
        <category>debugging</category>
        
        <category>christmas</category>
        
        <category>software_engineering</category>
        
        
        <category>debugging</category>
        
      </item>
    
      <item>
        <title>AWS, Ansible and Boto or Virtualization IS the Answer</title>
        <description>Historically I haven't been a fan of virtualization whether its Docker, Vagrant or something else.  This whole let's run a computer inside a computer trend, well, it feels lazy.  It feels like we have an excess of performance so let's virtualize!  Today, however, I think I may have turned a corner and started to become a fan.  

The situation in question was using Ansible to dynamically create EC2 instances.  [Nick](http://www.nickjanetakis.com/) and I have been hard at this for the past several days and we're nearing the end -- we can just about *taste* it.  Well, we could taste it and then yesterday we hit this crap:

    TASK [ec2_create_appdata_proxy : create_instance] ******************************
    fatal: [54.244.41.180]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;msg&quot;: &quot;boto required for this module&quot;}
    fatal: [54.218.52.37]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;msg&quot;: &quot;boto required for this module&quot;}
    	to retry, use: --limit @/Users/sjohnson/Dropbox/appdatallc/ansible/playbook_appdata_proxy_create_instances.retry

This was on:

* a brand new MacBook Pro
* clean OS install of whatever damn name comes after Capitan - Yosemite maybe?
* fresh installs of everything
* all components installed thru Ansible itself via my MacBook Pro configuration script

And no matter what we did, neither of us could come up with a way around this including:

* all kinds of sudo manipulations
* installing python's equivalent to rvm -- mkvirtual

While I don't know Nick's age or industry experience, he's a Docker expert, a Udemy star and smart.  My suspicion is that between the two of us we have an aggregate of 40 + industry years.  At least on the Ruby side alone we probably have close to 2 decades of experience.  And yet neither of us could get past an *install issue*: 

* Boto was present
* Boto worked via the command line
* Python was present (Boto is a python thing)

Now I tend to have boxes that run for years and cruft does accumulate.  But this wasn't that circumstance by any means.  This was a box that is so new that it is glorious to use.  And, even so, I'm hitting this crap.  So I know that cruft isn't the issue.  

Nick and I walked away from a morning of pair programming fairly pissed to be honest.  I took the position of &quot;Well, let's provision an instance on AWS and do all our Ansible execution from there.&quot;  Nick took the position of &quot;Let's try using Vagrant as a way to run Ansible locally and create the EC2 resources we need&quot; -- and Nick was **right**.

# The Vagrant File

Let's start with the Vagrant file:

    VAGRANTFILE_API_VERSION = '2'

    $provision = &lt;&lt;SCRIPT
    sudo apt-get install -y software-properties-common
    sudo apt-add-repository -y ppa:ansible/ansible
    sudo apt-add-repository -y ppa:chris-lea/python-boto
    sudo apt-get update
    sudo apt-get install -y ansible python-boto python-apt curl git unzip
    curl &quot;https://s3.amazonaws.com/aws-cli/awscli-bundle.zip&quot; -o &quot;awscli-bundle.zip&quot;
    unzip awscli-bundle.zip
    yes | sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
    sudo rm -rf awscli*
    SCRIPT

    Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
      config.vm.box = 'ubuntu/trusty64'
      config.ssh.insert_key = false

      config.vm.provider :virtualbox do |vm|
        vm.name = 'appdataansible'
        vm.memory = 512
        vm.cpus = 1
        vm.customize ['modifyvm', :id, '--natdnshostresolver1', 'on']
        vm.customize ['modifyvm', :id, '--ioapic', 'on']
      end

      config.vm.define :appdataansible
      config.vm.hostname = 'appdataansible'

      # When re-creating VMs, run ssh-keygen -R 192.168.88.2 to fix connect issues.
      config.vm.network :private_network, ip: '192.168.88.2'

      config.vm.provision 'shell', inline: $provision
      config.vm.provision 'file', source: '~/.aws', destination: '~/.aws'
  
      # TODO: Copy in your pem file for appdata (I didn't know the file name)
      config.vm.provision 'file', source: '/Users/sjohnson/.ssh/appdata_aws.pem', destination: '~/.ssh/appdata_aws.pem'
    end
    
All I did was:

* create a text file called Vagrantfile in my ansible root directory (where all my playbooks reside)
* Ran the command: **vagrant up**

This started the virtual machine and it loaded in all my AWS credentials from ~/.aws/credentials.  Once this was running I had an Ubuntu 14.04 distro running locally i.e. &quot;trusty&quot; and I could use it to execute my playbooks as follows:

    ansible-playbook  -i inventories/dummy playbook_appdata_proxy_create_elb.yml -vvv
    
That's the exact syntax I would have used locally.  Even -vvv for extended output is fully available.

Commands I used successfully within vagrant:

* ssh -i ~/.ssh/appdata_aws.pem ubuntu@ec2-54-149-81-245.us-west-2.compute.amazonaws.com (for logging directly into a created instance)
* aws ec2  describe-instances | grep PublicDnsName (for grabbing the DNS names from created instances)

Commands I used to get vagrant going:

* vagrant up (to build it)
* vagrant halt (when I needed to stop it)
* vagrant ssh (to get into it)
* cd /vagrant (to get into my locally mounted filesystem within vagrant)

End to end we spent less than 2 hours to get: 

* vagrant running
* a considerable amount of Ansible development done
* something actually working

Given how hard it can be at times to deal with virtual environments like rvm or its python equivalent and to deal with software troubleshooting this whole process really makes me wonder about whether I should be using Vagrant for other issues.  **Recommended**.</description>
        <pubDate>Thu, 01 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/12/01/aws-ansible-and-boto-or-virtualization-is-the-answer.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/12/01/aws-ansible-and-boto-or-virtualization-is-the-answer.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>boto</category>
        
        <category>virtualization</category>
        
        <category>vagrant</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Time Machine Backup Woes</title>
        <description>It is a fairly well understood IT truism that you don't really know how good your backup strategy is **until** you restore it.  I'm an IT professional and I've been one my whole life but, even so, I still got bit badly by backup woes.  About a month ago I took a moderately functioning 2011 Macbook Pro 15&quot; and did a full time machine back up on it.  Then I wiped it and played with Linux distros on it.  The machine did have valuable data on it but I did a full backup on it and I was beyond careful:

* the backup disk was labeled
* the backup disk was turned off
* the backup disk was physically unplugged

Given that I had valuable data on the disk, I wanted to be absolutely certain that nothing happened to it.  I always knew I'd restore it but I wasn't sure exactly *when*.  And then *when* turned out to be today.  Now, since the title includes the words &quot;backup&quot; and &quot;woes&quot; you do know that this isn't going to end well, right?  

# Problem the First -- 200 Gigs Free Space

My first inkling that something had gone very, very wrong came when I did a **df -h** and realized that my restored computer has roughly 200 gigs of free space.  Normally this box runs between 5 and 20 gigs free.  Clearly there is a problem.

# Problem the Second -- All Passwords Have to be Re-Entered

This was weird -- I noticed that sometimes I'd have to re-enter passwords when I know that they worked perfectly before.  I find this odd because it implies that not everything got restored.  This is troubling.

# Problem the Third -- iPhoto Doesn't Work At All

Interestingly I found that my copy of iPhoto told me that it wouldn't work under Capitan.  I don't know how an older version of iPhoto got left behind when this box was upgraded to capitan but, again, troubling.

I simply copied iPhoto from another Capitan box onto a USB stick and dropped it onto this machine and it worked beautifully and, praise Ghu, I have pictures of my kids again.  As I said in the beginning -- **valuable data**.

# Problem the Fourth -- WHERE ARE MY DAMN DATABASES?

And here comes the punch line -- the missing data seems to all be my MySQL databases.  A quick google shows that for the query &quot;time machine&quot; &quot;mysql&quot; (indicating that both terms are phrases and both are required) gives some [227,000 results](https://www.google.com/search?q=%22mysql%22+%22time+machine%22&amp;ie=utf-8&amp;oe=utf-8).  Clearly I'm not the only one with an issue here.  My guess is that the data I need is actually on the backup disc -- somewhere.  I suspect that Time Machine doesn't restore anything outside of /Users/ and the operating system itself.  And, yes, that's stupid.  Quite honestly this is likely the last time I'll ever trust Time Machine.  Sheesh. 

If anyone out there has experience with restoring MySQL databases from Time Machine I'd really appreciate some assistance.  Trying to ferret out their location from the back up disc is mildly impenetrable.  

# Conclusion and a Recommendation

I know someone out there is saying &quot;Carbon Copy Cloner&quot;.  Yep.  I know.  And I own a full copy, bought and paid for.  But I honestly thought that Time Machine was good enough for a personal backup.  Apparently, at least for a developer, it is **not**.  Tomorrow I am figuring out where MySQL stores its data on all my other OSX boxes and I'm relocating it to a location inside /Users/sjohnson with a vengeance.</description>
        <pubDate>Wed, 30 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/osx/2016/11/30/time-machine-backup-woes.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/osx/2016/11/30/time-machine-backup-woes.html</guid>
        
        <category>osx</category>
        
        <category>time_machine</category>
        
        <category>mysql</category>
        
        <category>mac</category>
        
        
        <category>osx</category>
        
      </item>
    
      <item>
        <title>Startup Learnings - Four Things that Betsy Devine Taught Me</title>
        <description>Although I'm now happily ensconced in the mid west, once upon a time, in a vastly different personal universe, I lived in Boston.  And I think the happiest times I had in Boston were the brief period where I was part of the informal Boston blogging crowd centered around [Dave Winer](http://www.scripting.com/), the [Berkman Center at Harvard](https://cyber.harvard.edu/) and a random crew of people that drifted into that orbit.  Among those people were:

* [Dave Winer](http://www.scripting.com/)
* [Betsy Devine](http://betsydevine.com/blog/)
* [Halley Suitt](http://halleyscomment.blogspot.com/)
* [Shimon Rura](http://rura.org/shimon/)
* [Mark Bernstein](http://www.markbernstein.org/)
* [Jessica Baumgart](http://blogs.harvard.edu/jkbaumga/)
* Myself and many others

All of us were drawn together to the hotness of a new medium and it had, as best as I can tell, the feeling of an [18th century salon](https://en.wikipedia.org/wiki/Salon_(gathering)).  Everything was new; the people were excited and it felt like something was brewing.

Today I'd like to talk about [Betsy Devine](https://en.wikipedia.org/wiki/Betsy_Devine) and what I learned from her.  I first met Betsy in roughly May 2003 as dated by [this url](https://fuzzygroup.github.io/blog/story.radio.weblogs.com/2003/05/11/boston-beach-blogging-bingo.html) where she was present; the pictures aren't present any more -- I'll fix that some other time so you'll have to take my word for it.

At the time I didn't really know Betsy very well but here's what I did and didn't know:

* I knew she was pleasant.  
* I knew she was clearly smart as a whip but I was in my mid 30s and Betsy was my Mom's age so I suspect I didn't give all the respect I should have.  
* I didn't know that she was a [published author](https://www.amazon.com/Longing-Harmonies-Themes-Variations-Physics/dp/0393305961/ref=sr_1_1?ie=UTF8&amp;qid=1479718439&amp;sr=8-1&amp;keywords=Longing+for+the+Harmonies) (yes I just ordered it and you likely should to if you're bothering to read this).  
* And I knew she was kind which is a character trait all to often overlooked and all too often ignored.

And then in 2004 Betsy's life changed dramatically -- her [husband](https://en.wikipedia.org/wiki/Frank_Wilczek), someone I knew slightly from having carved [this pumpkin at their house on October 22, 2003](http://betsydevine.com/blog/page/89/?s=devine), won the **Nobel Prize** in Physics.  At the time I met him, I was told &quot;hey - here's Betsy's husband - he teaches at MIT&quot;.  And, yes, MIT is impressive but he's a teacher (and a very pleasant fellow).  And so Betsy went from being just Betsy to being the wife of a legitimate celebrity.

# Learning the First: Celebrity Changes Things

In modern culture celebrity is an odd thing.  I work alone, from home, in a pitch dark room, most days so I clearly fall into the introvert camp and I've never cared much for it but I've had a few random encounters with celebrities over the years: 

* My first partner physically crashed into Bill Gates at a CD-ROM conference back in maybe '91 and almost took him out
* I stayed on Ted Turner's yacht once upon a time when my Dad and I were hunting
* My Dad's best friend, Jim Mattingly, [skippered Tenacious in the Fastnet Race in '79](http://forum.sailingscuttlebutt.com/cgi-bin/gforum.cgi?post=7816#7816).  He was then featured in posters that ran for years in the boating industry (I grew up in boating)
* [Andy Rooney](https://en.wikipedia.org/wiki/Andy_Rooney) used to shop at one of my family's wood working businesses and he was exactly the same in person as on 60 minutes

But, even with some exposure to celebrities all through my youth, I suspect on some level, post 2004, I viewed Betsy at least a little bit as &quot;wife of a nobel laureate&quot; instead of as &quot;Betsy&quot;.  Apologies for that.

Normally this is where I end this type of post but not this time.  Upon reflection I learned at least two more things from Betsy.

# Learning the Second: Never, Ever Underestimate the Brains People Have

[Bill Joy](https://en.wikipedia.org/wiki/Bill_Joy) is a computer scientist, a co-founder of Sun Microsystems, the primary author of BSD unix and too many other technical achievements to write down and the author of Joy's Law:

&gt; Joy's law is the principle that &quot;no matter who you are, most of the smartest people work for someone else,” \ [Source](https://en.wikipedia.org/wiki/Joy's_law_(management))

Now Bill Joy wrote this in the context of Sun, Microsoft and technical nerds and who they work for but I would actually expand it to be something far more expansive:

&gt; There are smart people all around you.  And at least one of them is almost certainly smarter than you are.

From my initial perspective Betsy was a mom, heck, she even looked like my mom.  What I didn't realize at the time, and wouldn't realize for literally years to come, was just how **damn smart** she was.  I should have taken a clue from the fact that she wrote humor professionally.  Comedians are always smart and that should have tipped me off.  Or I should have taken a clue from the fact that she had a Masters in Materials Science Engineering from Princeton.  How smart did she have to be to:

* get into princeton
* get into a technical field
* get into a technical field as a woman in 1978??
* get a master's degree that almost certainly required a thesis

I suspect that by any measure there's far more depth to Betsy's smarts then I'll ever know. There are literally smart people all around you; never, ever, underestimate the brains people have.

# Learning the Third: What Might Betsy Have Done?

Gender and identity politics is a dicey subject in America in 2016 so I write this with no small degree of trepidation.  It is the reason that as a profoundly white male of privilege -- the only college nickname I ever had was, I kid you not, *Captain Whitebread* -- I actually had Betsy read this in advance of posting since I didn't want to stick my foot in my mouth.

So my third learning is more of a zen like question -- Why Might Betsy Have Done?  As a woman, even an accomplished woman, in the late 1960s / 1970s, Betsy went down a fairly traditional route of wife and mother but also kept herself as a distinct entity.  That's a challenge because raising a family is an all consuming past time.  I say that as a father and husband who has seen that his wife has a harder &quot;job&quot; that he has.  And my wife certainly has a more important job than I have.  My work product is literally ephemeral -- I write lines of code that often don't survive a week.  My wife?  She's takes little people and day by day turns them into big people.  Every product I've ever written -- and I've written a lot of them -- will be gone someday, generally soon -- but my wife's &quot;work product&quot; will last forever.

So I have damn near infinite respect for what Betsy did.  But when I see how smart she is and how smart she has to be it makes me wonder what she might have done.  Perhaps in a slightly different universe she might have the Nobel prize.

So the next time you see a wife and mother and look at the accomplishments that their husband has you should really wonder what that wife and mother might have done on their own.

# Learning the Fourth: Engage More with Women at Work

For a brief period Betsy helped out with Feedster back in 2003 / 2004 and she added very real value.  One of the things I noticed was that she made a great team player and brought a very different perspective to things.  So the next time you see a woman at work, whether she's &quot;technical&quot; or not, recognize that she's almost certainly smart and there is a fair chance you're underestimating her.  Men are notorious for that and there's no reason for it in this day and age. 

And that's the fourth learning.

</description>
        <pubDate>Tue, 29 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/startup/2016/11/29/startup-learnings-two-things-that-betsy-devine-taught-me.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/startup/2016/11/29/startup-learnings-two-things-that-betsy-devine-taught-me.html</guid>
        
        <category>startup</category>
        
        <category>learnings</category>
        
        <category>betsy_devine</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>Native Apps Are Not Doomed</title>
        <description>So there was a meme that went around a week ago that I meant to jump on but, well, [database crash and my life was subsumed into disaster recovery](https://fuzzygroup.github.io/blog/aws/2016/11/26/fear-and-loathing-in-awsville-or-adventures-in-partition-resizing.html) so there you go.  Here are the relevant links:

* [Native Apps Are Doomed - 1](https://medium.com/javascript-scene/native-apps-are-doomed-ac397148a2c0#.knd91wksx)
* [Native Apps Are Doomed - 2](https://medium.com/javascript-scene/why-native-apps-really-are-doomed-native-apps-are-doomed-pt-2-e035b43170e9#.iipzbyled)
* [Hacker News Thread](https://news.ycombinator.com/item?id=13002598)

I don't know the author of these pieces but I really, really don't think he's at all correct.  His basic thesis seems to be that native app development is going to be replaced by some kind of &quot;progressive web apps&quot; -- essentially an embedded browser and the reasons are many:

* users install too few apps   
* app development is hard
* app development is expensive

All of these are good points but they are all also wrong and I'm going to explore this along two dimensions: The History of Cross Platform Development Tools and The Very Young.  The first is relevant along the technical axis and the second is relevant along the install axis.

# The History of Cross Platform Development Tools

The year was 1996 and I was being heavily courted by a company called Dataware.  They wanted to acquire my company, NTERGAID, and the CEO / Founder, Kurt Mueller, and I were out for a very, very nice meal at one of the, at the time, leading Boston restaurants.  Kurt was at his very charismatic best as he so often was and he pitched Dataware **hard**.  One of the things he held out to me was the possibility of cross platform development, something we were weak on.  I can't remember the exact words but it was something like this:

&gt; And we're committed to this new cross platform framework called XVT.  It will let you get your browser software onto platforms like the Mac easily and it is fantastic.

Ultimately Dataware did buy NTERGAID but that's not really relevant.  After we joined up, well, XVT was never actually used for any of the products we were involved in -- or for much of anything.  Simply put it just didn’t work well; apps it made didn’t feel right.

The position that the author is taking is that cross platform development is hard and non economical so you have to use progressive web apps.  NO YOU DON'T.  Progressive web apps are just another take on cross platform development tools and history has shown us that cross platform development tools don't work well.  

It is interesting to me that 20 years later, XVT is still around and still a niche tool: 

* [XVT Today](http://www.xvt.com/)
* [XVT 20 Years Ago](https://web.archive.org/web/19970605034017/http://www.xvt.com/docs/products.html) - Thanks Archive!

And lest you think that this is all about XVT, its not.  XVT is just an example.  I could just as easily have picked [QT](https://www.qt.io/) which has been around a similar amount of time.  

The bottom line about cross platform development tools is that the bulk of users only use one platform at a time.  Cross platform tools are great for the developer but they are generally irrelevant to users.  What users want are great feeling, great looking, performant apps.  And, at least in my experience and the experience of the industry, apps like that **NEVER** come from cross platform tools.  

**Note 1**: It might be different for back end / server software which doesn't have a UI but for beautiful apps that users actually want to use, those come solely from native development.

**Note 2**: Cross platform development tools go back way further in time than 1996.  I remember seeing ads for them back in Dr. Dobb's Journal starting in the mid 80s.

# The Very Young

One of the interesting points that this author, Eric Elliot, makes is that people don't install apps: 

&gt;  I mean that most web traffic comes from mobile devices, and that users install between 0–3 new apps per month, on average. That means that people aren’t spending a lot of time looking for new apps to try out in the app store, but they are spending lots of time on the web, where they might discover and use your app.

I think he's absolutely **wrong** here.  I have two kids and they both use either iOS or Google Play (really Amazon's Fire Tablet) constantly.  And their information access patterns are radically different from those of my wife and myself.  They **do not** surf the web.  And as an Internet professional, while that breaks my heart, as a parent I find it *profoundly* interesting.  They look at information in terms of can it be delivered in an app form or not.  They use the web certainly but they use it in terms of Googling for something, getting an answer and getting out.  I've never seen either of them lost in the web as so often happens to me or my wife.  

And the number of new apps they use -- it changes constantly.  For my youngest I have his iTunes account set to authorize app purchases through me.  So I see literally every app he gets and I know the rate of change.  Adults may only be installing 0 to 3 apps per month but kids are not.  And the kids of today are the adults of tomorrow.  

**Note**: Lest you think that I'm a closed minded, controlling parent, go search the iTunes app store for Feeld or 3somer or Trippple and then ask yourself if you'd want your 10 year old child being able to install that.

You should note that &quot;very young&quot; is perhaps misleading.  What I mean by it is &quot;the people growing up with iOS and Google Play&quot;.

# Conclusion / TLDR

I absolutely understand the desire for progressive web apps to work.  If they did it would:

* increase consumer choice
* decrease platform vendor power
* decrease developer costs dramatically

As a developer I want this too but, unfortunately, the software industry has now tried for well over 3 decades to build development tools for good cross platform experiences and they still **don't work**.  I see no reason that this will change in the near term; history is on my side.  This, coupled with the fact that younger people have no problem installing apps regularly, means that native apps have a long life in front of them.
</description>
        <pubDate>Tue, 29 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/mobile/2016/11/29/native-apps-are-not-doomed.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/mobile/2016/11/29/native-apps-are-not-doomed.html</guid>
        
        <category>apps</category>
        
        <category>iphone</category>
        
        <category>google_play</category>
        
        <category>android</category>
        
        <category>xvt</category>
        
        <category>qt</category>
        
        <category>software_development</category>
        
        <category>mobile</category>
        
        
        <category>mobile</category>
        
      </item>
    
      <item>
        <title>AWS Tech Note - Problems with Ubuntu 16.04 and Ansible</title>
        <description>When i started going heavy down the path of AWS, Ubuntu 14.04 was the default Ubuntu in AWS and it appeared at the top of the Operating System list when you built a new EC2 server.  I've run Ubuntu for **years** ever since I asked [Matt](https://ma.tt/) what distro WordPress used and he replied Ubuntu.  *blink*  At the time I looked at Ubuntu as a personal distro but, hey, if it is good enough for WordPress then it is good enough for my crappy, little farm of boxes.  And so I've been in Ubuntu ever since.

Today was the first bad Ubuntu experience that I've really ever had.  What happened was I went to bring up a new EC2 instance and I noticed that the default Ubuntu was now 16.04.  Hm... That's different but I'll try it.  So I trot out my ansible playbook and here's what happened:

    ansible-playbook -i inventories/proxy playbook_appdata_proxy.yml

    PLAY [all] *********************************************************************

    TASK [setup] *******************************************************************
    fatal: [adproxy2]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;module_stderr&quot;: &quot;&quot;, &quot;module_stdout&quot;: &quot;/bin/sh: 1: /usr/bin/python: not found\r\n&quot;, &quot;msg&quot;: &quot;MODULE FAILURE&quot;, &quot;parsed&quot;: false}
    fatal: [adproxy1]: FAILED! =&gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;module_stderr&quot;: &quot;&quot;, &quot;module_stdout&quot;: &quot;/bin/sh: 1: /usr/bin/python: not found\r\n&quot;, &quot;msg&quot;: &quot;MODULE FAILURE&quot;, &quot;parsed&quot;: false}

    NO MORE HOSTS LEFT *************************************************************
    [WARNING]: Could not create retry file 'playbook_appdata_proxy.retry'.         [Errno 2] No such file or directory: ''


    PLAY RECAP *********************************************************************
    adproxy1                   : ok=0    changed=0    unreachable=0    failed=1
    adproxy2                   : ok=0    changed=0    unreachable=0    failed=1

What??? There error is here: **&quot;/bin/sh: 1: /usr/bin/python: not found\r\n&quot;**.  Apparently Python is NOT a default install on Ubuntu 16.04 LTS.  That means that you can't use Ansible to bootstrap a box into functionality until you've first installed Python.

One of the single best attributes of Ansible is that you don't have to install anything to use it -- until now.  Unlike tools like Chef or Puppet which require an agent to be installed, Ansible has always just required nothing but python.  And since python used to be a default install you could trivially just &quot;ansible-ize&quot; a box into functionality.  This is a huge deal.  Happily Ubuntu 14.04 is still available -- you just have to dig down to the very bottom of the Operating Systems list to find it.

And yes I'm absolutely certain that there are very real reasons why I want 16.04 instead of 14.04 but at least for now the operational benefits of being able to bring a box up with Ansible far, far outweigh them.</description>
        <pubDate>Tue, 29 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/11/29/aws-tech-note-problems-with-ubuntu-16-04-and-ansible.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/11/29/aws-tech-note-problems-with-ubuntu-16-04-and-ansible.html</guid>
        
        <category>aws</category>
        
        <category>ansible</category>
        
        <category>ubuntu</category>
        
        <category>python</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Parenting Actually Does Work</title>
        <description>I don't think there is a parent alive, certainly not a parent of a teenager, who doesn't at some point want to throw their hands up in the air and go ***WHY AM I BOTHERING!!!!***. If you are a parent then you know the drill -- you talk and you talk and you talk and your kid never seems to get it.  Sometimes it feels like you are talking to a brick wall and other times it feels worse.  I have a teenage boy, 15, myself so I say this with a fairly high degree of authority.

What I wanted to pass on to other parents tho is that you actually **are** getting thru but it may take **years** for you to realize it.  

Here's an example from my own life.  As a young child I was tremendously clumsy.  And I'm not not sure that I am any better now but that's a different story.  As a clumsy child I had a tendency to knock glasses off the kitchen table which led to shattered glass *everywhere*.  And my mother, bless her heart, always had the same reaction:

* Everyone freeze!
* The furthest person away grabs the shoes.
* Cleanup with paper towels for the wet, then broom and then vacuum.

Earlier this afternoon I was assembling Christmas lights and the strand I was working on fell and **WHAM** about 10 1 1/2&quot; colored bulbs all exploded.  There was glass from one end of the kitchen to the other.  And you know what I did?  Well:

* Everyone freeze!
* The furthest person away grabs the shoes.
* Cleanup with broom and then vacuum (there was no wet so no paper towels).

I haven't gone thru this with my mom in maybe 35 years now but, today, when I needed it, it was there like a **reflex**, as automatic as breathing.

And that's what I mean by **parenting actually does work**.  Keep in mind that:

1.  You need to be consistent in what you do.
2.  You have to stay at it.
3.  You won't see the pay off for literally years.  You might never see it; I doubt I'll ever tell my Mom about today and she's the last person in the universe to read my blog.

Even if your kids appear not to be listening or even caring, if you stay at it, they'll pick it up even if it is by osmosis that they aren't even aware of.</description>
        <pubDate>Mon, 28 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/parenting/2016/11/28/parenting-actually-does-work.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/parenting/2016/11/28/parenting-actually-does-work.html</guid>
        
        <category>parenting</category>
        
        <category>fatherhood</category>
        
        
        <category>parenting</category>
        
      </item>
    
      <item>
        <title>A Poor Man's Load Balancer</title>
        <description>So I've spent the last few days dealing with proxy servers and trying to get a group of AWS instances, all running squid, to hide behind a load balancer.  And, alas, this seems to not be possible with AWS.  There are two basic issues:

* The proxy protocol header cannot be enabled via the AWS Console; why I do not know.  You have to script it and even though I did it, it does not seem to work.  [AWS Docs](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html)
* The proxy request doesn't seem to get to squid -- at least based on the squid log files.  

The situation is that a number of my IP addresses seem to have been dropped on a perma-ban list.  You can get around this a number of ways but one of the easier ones is to use a proxy server in between your content fetch and the content source.  Most of my content fetching software relies on the [Ruby Mechanize library](https://github.com/sparklemotion/mechanize) so all I have to do is:

    agent = Mechanize.new
    agent.set_proxy(proxy_url, proxy_port)
    page = agent.get(url)

And that will cause the fetch to go thru the proxy server first and then to the content source.  This nicely gets around the block at the expense of some performance due to the proxying.  

The reason for wanting a load balancer in the mix is to:

* Eliminate the need for a giant box as the proxy server; more, cheaper nodes is a better model, more amenable to the cloud.
* Provide more discrete ip addresses involved the proxying so we don't appear to have a single ip address doing everything

If the AWS load balancer doesn't work then there are options.  One option is to get up HA Proxy once again.  I've done this time and time again and it usually works at the expense of configuration woes and a moderate level of cursing. Not sailor cursing mind you and certainly not &quot;The Tree House Fell On Me Again&quot; level of cursing.  But after you've put together a load balancer with the AWS Console web forms, you don't ever want to use HA Proxy again.  

And this brings us to the title of this article - a **poor man's load balancer**.  Here's how we go about this:

* Have a list of proxy server urls in a config file 
* A routine which loads the urls into an array and returns a random element i.e. calling .sample.
* Pass that random proxy server into the create mechanize routine 

And that's what I'm now using.  This is by no means the degree of elegance that I was looking for but sometimes you don't always get what you want -- but you still have to get the job done.  
</description>
        <pubDate>Mon, 28 Nov 2016 00:00:00 -0500</pubDate>
        <link>http://fuzzyblog.io/blog/aws/2016/11/28/a-poor-man-s-load-balancer.html</link>
        <guid isPermaLink="true">http://fuzzyblog.io/blog/aws/2016/11/28/a-poor-man-s-load-balancer.html</guid>
        
        <category>aws</category>
        
        <category>proxy</category>
        
        <category>load_balancer</category>
        
        <category>ruby</category>
        
        
        <category>aws</category>
        
      </item>
    
  </channel>
</rss>

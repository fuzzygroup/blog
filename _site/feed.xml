<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FuzzyBlog</title>
    <description>Scott Johnson writing about the usual array of nerd stuff: AWS / Ansible / Ruby / Rails / Elixir / Misc.
</description>
    <link>https://fuzzygroup.github.io/blog/</link>
    <atom:link href="https://fuzzygroup.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 05 Jan 2017 08:39:03 -0500</pubDate>
    <lastBuildDate>Thu, 05 Jan 2017 08:39:03 -0500</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Scott's Rule of API Development</title>
        <description>&lt;p&gt;So on Tuesday I wrote a new api that allows an &lt;a href=&quot;https://www.ansible.com/&quot;&gt;Ansible&lt;/a&gt; playbook to register an AWS instance with an internal service I’m writing.  This is for the purpose of monitoring large jobs (think tens if not hundreds of lightweight EC2 instances) so that they can be shut down programmatically when they are done.&lt;/p&gt;

&lt;p&gt;In some kind of alternate universe I’d just do all this with &lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;AWS Lambda&lt;/a&gt; but I’m working in a monolithic rails code base so I used my &lt;a href=&quot;https://fuzzygroup.github.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html&quot;&gt;Large Datasets approach&lt;/a&gt; I documented earlier this week.&lt;/p&gt;

&lt;p&gt;My ansible playbook creates the EC2 instances, binds the whole process together and initiates everything – I can literally run one playbook and have 40 workers (or really N workers) created and an entire data set start processing along with getting an internal record of all the instances and the job so that I can do proper cost accounting, dynamic instance shut down and the whole nine yards.  This is a big step for me.  I am getting close to a technical goal that I’ve been working towards either for six months or about 7 years depending on how you measure it.&lt;/p&gt;

&lt;p&gt;So &lt;a href=&quot;http://nickjanetaks.com/&quot;&gt;Nick&lt;/a&gt; and I did all this fancy, fancy EC2 automation – AMI creation, instance creation, job launching – and the thing that &lt;strong&gt;&lt;em&gt;failed&lt;/em&gt;&lt;/strong&gt; was a simple &lt;em&gt;http post API&lt;/em&gt; call using the &lt;a href=&quot;http://docs.ansible.com/ansible/uri_module.html&quot;&gt;Ansible URI module&lt;/a&gt;.  Like everyone else in the industry I’ve written a post API, I don’t know – 50 times? 100 times? — and they always fail the first time.  And this brings me to what I’m going to immodestly call Scott’s Rule of API Development:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Scott’s Rule of API Development&lt;/strong&gt;.  Always, always, always test your APIs with &lt;a href=&quot;https://curl.haxx.se/&quot;&gt;curl&lt;/a&gt; from an external box before you actually try and use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Seriously – I hit this problem six months ago at &lt;a href=&quot;http://www.therachat.io&quot;&gt;TheraChat&lt;/a&gt; when I wrote their MVP.  I hit it yesterday.  I seemingly hit it every single damn time I make an API.  Hence Scott’s Rule of API Development.  Now, that said, what does using curl to test an api actually mean / how do you do it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create your API.&lt;/li&gt;
  &lt;li&gt;Write your curl statement. Here’s a sample which I used once upon a time for an authorization API (the -d tells curl to post it): &lt;em&gt;curl -i -d “api_key=forceawakens13928534aY&amp;amp;&amp;amp;mobile_number=317-531-4853&amp;amp;password=BLAH” “http://app.foo.com/api/authorize”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Test it locally against your development system.  Fix any bugs.  Lather / Rinse / Repeat.&lt;/li&gt;
  &lt;li&gt;Deploy your code.&lt;/li&gt;
  &lt;li&gt;Run it on another box that isn’t your actual API to make sure that there are no security glitches / remote code issues / etc.&lt;/li&gt;
  &lt;li&gt;If this API is part of a full stack Rails app then don’t forget to disable forgery protection in Application Controller with &lt;em&gt;protect_from_forgery with: :null_session&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;For bonus points store the curl commands that exercise your API as part of your code base and always try them before a release.  APIs can be tricky little buggers and they are both difficult to get and seem to break easily.  Regularly testing with curl prevents that and makes users of your API much, much happier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Yes test coverage should prevent what’s described in #7 but I’d strongly recommend belt and suspenders on this one.  When the people who rely on your API have issues you can always give them a curl statement and say “Well this works for me – what about you”.  You can’t do that we test coverage as APIs are often used from a variety of languages and environments but curl is absolutely &lt;strong&gt;universal&lt;/strong&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/software_engineering/2017/01/05/scott-s-rule-of-api-development.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/software_engineering/2017/01/05/scott-s-rule-of-api-development.html</guid>
        
        <category>software_engineering</category>
        
        <category>api</category>
        
        <category>curl</category>
        
        <category>ansible</category>
        
        <category>aws</category>
        
        <category>rails</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Returning to RSS Programming After 14 Years</title>
        <description>&lt;p&gt;There is a very odd, very good feeling when you as a programmer return to some technical task that you used to do – and you find that you do it &lt;strong&gt;better&lt;/strong&gt;. For myself I am once again dabbling in RSS programming and it feels &lt;strong&gt;great&lt;/strong&gt;.  It was almost 14 years ago to the day that I started Feedster based on an idea that &lt;a href=&quot;http://scripting.com/2003/03/09.html&quot;&gt;Dave&lt;/a&gt; put out there by linking to an idea that Dave Aiello  had.&lt;/p&gt;

&lt;p&gt;That one tiny mention on &lt;a href=&quot;http://www.scripting.com/&quot;&gt;Scripting.com&lt;/a&gt; inspired me to hack together a truly awful RSS search engine that in turn survived a slashdotting and then picked up a co-founder, a CEO, angel investment, a move to San Francisco, real venture funding and actual staff.&lt;/p&gt;

&lt;p&gt;It was, I believe, March of 2003 on a snowy night and now in January 2017 on another snowy night I am again writing feed processing code.&lt;/p&gt;

&lt;p&gt;The technically interesting part here is that the operations I did poorly back in 2003 are now flowing and gracefully from my fingers as I work.  Specifically I used to monitor feeds by doing a full fetch and then hashing the body with MD5 and comparing it to a stored value from the last time.  This time I &lt;em&gt;knew&lt;/em&gt; “Ok.  Do an HTTP head and look at just the header values”.  Additionally I am using a far better toolset in 2017 (ruby / rails / elixir / phoenix) than I ever did back in 2003 (php).&lt;/p&gt;

&lt;p&gt;If as an engineer you’ve never returned to a problem space after a long interval, I’d recommend it.  Just seeing how you tackle the same problem after a long break certainly makes you think.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/startup/2017/01/05/returning-to-rss-programming-after-14-years.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/startup/2017/01/05/returning-to-rss-programming-after-14-years.html</guid>
        
        <category>hyde</category>
        
        <category>rss</category>
        
        <category>rails</category>
        
        <category>startup</category>
        
        <category>software_engineering</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;The classical Rails architectural pattern is the &lt;em&gt;monolith&lt;/em&gt; – a giant application that encompasses all parts of your codebase. You know it, I know it, we all know it and all &lt;strong&gt;too well&lt;/strong&gt;.  Even though we all talk about micro services, Rails itself seems almost designed to prevent that architectural pattern.  You know what I’m talking about – you start by building a web app in Rails. Then you need an API so you quickly make an API controller within the same application.  Then you need an admin tool and sooner or later you end up with numbers like these:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+----------------------+-------+-------+---------+---------+-----+-------+
| Name                 | Lines |   LOC | Classes | Methods | M/C | LOC/M |
+----------------------+-------+-------+---------+---------+-----+-------+
| Controllers          | 36621 | 24073 |     147 |    1297 |   8 |    16 |
| Helpers              |  6688 |  5212 |       1 |     393 | 393 |    11 |
| Models               | 153716 | 81985 |    1446 |    9737 |   6 |     6 |
| Mailers              |     0 |     0 |       0 |       0 |   0 |     0 |
| Javascripts          | 25828 | 16579 |       0 |    1993 |   0 |     6 |
| Libraries            | 60197 | 36648 |     189 |    1571 |   8 |    21 |
| Integration tests    |    75 |    59 |       1 |       1 |   1 |    57 |
| Controller specs     | 18848 | 11227 |       0 |       0 |   0 |     0 |
| Feature specs        |  4239 |   529 |       0 |       0 |   0 |     0 |
| Helper specs         |  2366 |  1388 |       0 |       0 |   0 |     0 |
| Lib specs            |  4162 |  2948 |       0 |       0 |   0 |     0 |
| Model specs          | 54256 | 37837 |       0 |       3 |   0 | 12610 |
| View specs           |   534 |   320 |       0 |       2 |   0 |   158 |
+----------------------+-------+-------+---------+---------+-----+-------+
| Total                | 367530 | 218805 |    1784 |   14997 |   8 |    12 |
+----------------------+-------+-------+---------+---------+-----+-------+
  Code LOC: 164497     Test LOC: 54308     Code to Test Ratio: 1:0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s from one of my production applications – it has &lt;strong&gt;everything&lt;/strong&gt; and it is an absolute nightmare to maintain.  Yes it is terribly convenient to have a monolith but the number of potential interactions between different parts of the system are monstrous and just the time to launch Rails console is now approaching a full minute.&lt;/p&gt;

&lt;p&gt;So if a monolith has all these issues, why do we do it?  Well the answer really comes down to code re-use and code management.  Let’s consider, for example, what happens when you want to add a user api onto an existing application.  To start you probably want to use the new Rails 5 API mode.&lt;/p&gt;

&lt;p&gt;Thank about the task of building an API for your Rails application.  While Rails 5 offers the &lt;strong&gt;wonderful&lt;/strong&gt; new API mode, it is a &lt;em&gt;mode&lt;/em&gt; and it cannot be used as part of a full stack Rails application.&lt;/p&gt;

&lt;p&gt;rake task for copying in validations&lt;/p&gt;

&lt;p&gt;http://stackoverflow.com/questions/11372484/rails-put-validation-in-a-module-mixin&lt;/p&gt;

&lt;p&gt;require ‘api_key_validations’
  include ApiKeyValidations&lt;/p&gt;

&lt;p&gt;module ApiKeyValidations
  extend ActiveSupport::Concern&lt;/p&gt;

&lt;p&gt;included do
    validates_presence_of :name
    validates_presence_of :user
    #validates :name, :length =&amp;gt; { :minimum =&amp;gt; 2 }, :presence =&amp;gt; true, :uniqueness =&amp;gt; true
    #validates :name_seo, :length =&amp;gt; { :minimum =&amp;gt; 2 }, :presence =&amp;gt; true, :uniqueness =&amp;gt; true
  end
end&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Jan 2017 22:15:19 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/2017/01/04/breaking-the-rails-monolith-apart-01-validations.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/2017/01/04/breaking-the-rails-monolith-apart-01-validations.html</guid>
        
        
      </item>
    
      <item>
        <title>Getting Started with MongoDB and Rails</title>
        <description>&lt;p&gt;I am a well known MySQL fan if not fanatic.  I’ve used MySQL since 1999 and while I’ve had the occasional issue here and there, I’ve been immensely happy with it overall.  But times change and now I find myself using Mongo in the context of a Rails app and, well, I’m a bit lost.  This post writes down what I’ve learned – mostly about the operational side of Mongo and getting started.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;: Rails 5 / Ruby 2.3 / Mongo 3.4 / OSX; currently building a rails application called &lt;strong&gt;api&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-error-message&quot;&gt;The Error Message&lt;/h1&gt;

&lt;p&gt;What drove me into writing all of this down was I kept hitting this error message:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Mongoid::Errors::NoClientConfig: message:   No configuration could be found for a client named 'default'.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;useful-reading&quot;&gt;Useful Reading:&lt;/h1&gt;

&lt;p&gt;I read a number of things including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://gorails.com/guides/setting-up-rails-4-with-mongodb-and-mongoid&quot;&gt;GoRails&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tutorialspoint.com/mongodb/mongodb_create_database.htm&quot;&gt;TutorialsPoint&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/15354936/rails-engine-mongoid-no-configuration-could-be-found-for-a-session-named-def&quot;&gt;StackOverflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;I used HomeBrew to install Mongo:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew install mongodb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;getting-into-the-shell&quot;&gt;Getting into the Shell&lt;/h1&gt;

&lt;p&gt;After Mongo is installed you can get into the Mongo shell with the command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mongo
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;creating-your-first-database&quot;&gt;Creating Your First Database&lt;/h1&gt;

&lt;p&gt;Oddly the use command creates the database.  If the database doesn’t exist then it is automatically created:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;use api_development ENTER
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;making-mongo-work-under-rails&quot;&gt;Making Mongo Work Under Rails&lt;/h1&gt;

&lt;p&gt;Getting to this stage wasn’t all that hard.  The problem here is that I’ve never setup a new rails app from scratch with Mongo.  I’ve worked on Mongo stuff in the past but this was an entirely new thing. Here were the steps I followed.&lt;/p&gt;

&lt;h2 id=&quot;getting-rid-of-activerecord-in-your-application&quot;&gt;Getting Rid of ActiveRecord in Your Application&lt;/h2&gt;

&lt;p&gt;When you create your rails app, you need to add the –skip-active-record flag to turn off ActiveRecord entirely:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rails new api --skip-active-record --api
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note: This is an api mode Rails application so I’m passing in –api.&lt;/p&gt;

&lt;h2 id=&quot;creating-the-mongoidyml-file&quot;&gt;Creating the mongoid.yml File&lt;/h2&gt;

&lt;p&gt;rails g mongoid:config&lt;/p&gt;

&lt;h2 id=&quot;getting-mongoidyml-loaded&quot;&gt;Getting mongoid.yml Loaded&lt;/h2&gt;

&lt;p&gt;touch config/initializers/mongo.yml&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Jan 2017 06:44:54 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/rails/2017/01/04/getting-started-with-mongodb-and-rails.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/rails/2017/01/04/getting-started-with-mongodb-and-rails.html</guid>
        
        <category>rails</category>
        
        <category>mongo</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>On the Changes at Medium</title>
        <description>&lt;p&gt;So Ev has just announced major layoffs at Medium:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’ll start with the hard part: As of today, we are reducing our team by about one third — eliminating 50 jobs, mostly in sales, support, and other business functions. We are also changing our business model to more directly drive the mission we set out on originally. &lt;a href=&quot;https://blog.medium.com/renewing-mediums-focus-98f374a960be#.kh7coodv8&quot;&gt;Renewing Medium&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As someone who, on a vastly smaller scale, has been through this type of thing, I have nothing but sympathy for Ev.  Even when you are as monstrously successful as Ev (Blogger, Odeo, Twitter), layoffs always hurt.  You never hire people thinking that you are going to have to fire them.  You always hope that the people you hire will hire more people and so on.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://news.ycombinator.com/item?id=13321322&quot;&gt;Hacker News discussion&lt;/a&gt; on it has the normal nay sayers and people saying “But it is just a blogging platform”.  And, yes, Medium is a blogging platform but it is operating on a scale which is very, very hard to achieve.  The really interesting there is from the post by MG Siegler:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The numbers speak for themselves. 2 billion words written on Medium in the last year. 7.5 million posts during that time. 60 million monthly readers now.  &lt;a href=&quot;https://500ish.com/long-medium-b9ddfe2c3a0a#.wh8tzerfo&quot;&gt;Long Medium&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wow.  I had no idea that Medium was on that type of growth curve.  That is flat out astonishing.  When you consider the scaling issues at that type of size with the rich user interface that Medium offers and the integrated analytics, aggregation and notification, yes, I can see why they raised the amount of funding that they have.&lt;/p&gt;

&lt;p&gt;Now, that said, it does seem that they overstaffed at the business level.  And it seems that those are the positions being scrapped.  Interestingly at least some of the staff seem to be dealing with this well:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Newly ex-Median here. This was not a huge surprise. On the surface, this is a change in product strategy. The underlying story is the company positioning itself so it can survive an adverse environment if it needs to. It’s hard to fault managers for dealing with that potential (and its hard to deny that the next 2-4 years could be really bad). Hopefully not, but it would be malpractice not to prepare. So better to focus resources now than be walking dead in a year or so, jettison unnecessary products/projects, and hope for the best. It’s a great product, and with time and luck, they’ll sort out a good business model, but like the rest of the publishing world, they’re still sorting things out. Despite being one of those made redundant, I enjoyed being there, and wish them the best. On that note, you should ask yourself if you are prepared for winter, because winter is coming. &lt;a href=&quot;https://news.ycombinator.com/item?id=13322380&quot;&gt;Permalink&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/medium/2017/01/04/on-the-changes-at-medium.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/medium/2017/01/04/on-the-changes-at-medium.html</guid>
        
        <category>medium</category>
        
        <category>blogging</category>
        
        <category>startup</category>
        
        
        <category>medium</category>
        
      </item>
    
      <item>
        <title>Capistrano Failure - Asset Manifest Not Created</title>
        <description>&lt;p&gt;I setup a new Rails application using Capistrano earlier today and hit a fair number of odder than normal &lt;a href=&quot;http://capistranorb.com/&quot;&gt;Capistrano&lt;/a&gt; issues.  The biggest issue was around the asset pipeline and the asset manifest not being created.  The error message revolved around “cannot stat (pathname) manifest file”.&lt;/p&gt;

&lt;p&gt;Digging into the issue with Google revealed that it actually is a problem with Capistrano itself.  Here’s the &lt;a href=&quot;https://github.com/capistrano/rails/issues/111&quot;&gt;thread&lt;/a&gt;.  Upgrading to at least Capistrano 1.1.8 fixes this problem.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/rails/2017/01/04/capistrano-failure-asset-manifest-not-created.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/rails/2017/01/04/capistrano-failure-asset-manifest-not-created.html</guid>
        
        <category>rails</category>
        
        <category>capistrano</category>
        
        <category>asset_pipeline</category>
        
        
        <category>rails</category>
        
      </item>
    
      <item>
        <title>Ansible Unable to Find Boto Errors</title>
        <description>&lt;p&gt;Over the past several days I have been doing quite a bit of work with the ansible &lt;a href=&quot;http://docs.ansible.com/ansible/ec2_module.html&quot;&gt;EC2&lt;/a&gt; and &lt;a href=&quot;http://docs.ansible.com/ansible/ec2_ami_module.html&quot;&gt;AMI&lt;/a&gt; modules for dynamically creating instances and AMIs on AWS.  Ansible, however, doesn’t actually talk directly to AWS; it talks to AWS thru a python module named &lt;a href=&quot;https://github.com/boto/boto&quot;&gt;&lt;strong&gt;boto&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are a number of common problems that you might find when you the error “boto required for this module” or “unable to find boto”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make sure boto is installed: &lt;em&gt;sudo pip install boto&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Uninstall ansible entirely and then install it via Python: &lt;em&gt;sudo pip install ansible&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Run your playbook using sudo to ensure that the version of python is the one that comes from sudo&lt;/li&gt;
  &lt;li&gt;Eliminate &lt;em&gt;connection: local&lt;/em&gt; at the playbook level and move it to the task level&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you uninstall Ansible using HomeBrew or apt-get, you may find that your Galaxy roles have been uninstalled.  This can very badly impact your playbook execution so be careful.  If you want to avoid this then use the -p option when you install a role to specify that the role goes into a local directory of your choosing; &lt;a href=&quot;http://stackoverflow.com/questions/22201306/ansible-galaxy-roles-install-in-to-a-specific-directory&quot;&gt;Stack Overflow Explanation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The last one of these, eliminating connection: local, requires a bit of explaining.  Details can be found &lt;a href=&quot;https://github.com/ansible/ansible/issues/15019&quot;&gt;here&lt;/a&gt;.  When I first encountered this problem, my playbook looked like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- hosts: &quot;fi_app_metadata_monthly&quot;
  become: yes
  remote_user: ubuntu
  connection: local
  vars:
    - redis_ami_id: &quot;ami-XXXX&quot;
    - redis_security_group_id: &quot;sg-YYY&quot;
    - redis_instance_type: &quot;t2.micro&quot;
    - redis_tag_name: &quot;dynamic_redis_fi_app_metadata_update&quot;
    - redis_instance_count: 1
    - template_instance_id: &quot;i-UUUUUUUU&quot; # in future this comes in from command line
    - number_of_instances: 3                      
    - instance_type: &quot;m3.large&quot;                   
    - region: &quot;us-west-2&quot;
    - vpc_subnet_id: &quot;subnet-IIIIII&quot;
    - vpc_id: &quot;vpc-RRRR&quot;
    - group_id: &quot;sg-YYYY&quot;
    - aws_access_key: &quot;ERRERERE&quot;
    - key_name: &quot;appdata_aws&quot;
    - aws_secret_key: &quot;ERERERE&quot;
    - farm_job_name: &quot;monthly_fi_app_metadata_update_2017-01&quot;
    - farmer_address: &quot;ec2-9-9-9-9.us-west-2.compute.amazonaws.com&quot;
    - tag_name: &quot;fi_app_metadata&quot;
    - rake_task: &quot;bundle exec rake fi_farm_work:fi_app_metadata_update --trace&quot;
  roles:
    - { role: ec2_make_redis_instance_from_ami, tags: ec2_make_redis_instance_from_ami}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When the above playbook was run, it resulted in boto not being found errors.  The solution was to remove the line &lt;em&gt;connection: local&lt;/em&gt; from the playbook and move it to the task level:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: make the redis instance from the ami
  # note - you used to be able to do this at the role level; now it has to be at the task level
  # note - think of this as you are setting the connection context for the task being executed
  connection: local
  ec2:
    aws_access_key: &quot;&quot;
    aws_secret_key: &quot;&quot;
    region: &quot;&quot;
    image: &quot;&quot;
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This is one of those code errors that, honestly, I just don’t understand.  From talking with another Ansible buddy, &lt;a href=&quot;https://nickjanetakis.com/&quot;&gt;Nick&lt;/a&gt;, he confirmed that it used to work at the playbook level.  Perhaps this is just one of those perplexing changes that results from other other architectural work going on at the project level.&lt;/p&gt;
</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/ansible/2017/01/04/ansible-unable-to-find-boto-errors.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/ansible/2017/01/04/ansible-unable-to-find-boto-errors.html</guid>
        
        <category>ansible</category>
        
        <category>boto</category>
        
        <category>aws</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Tutorial How To Upgrade Your PS4 to 2 Terabytes of Storage</title>
        <description>&lt;p&gt;There are times when it seems like we live in a weird world of storage capacity imbalance.  We see this in two places:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;phones&lt;/li&gt;
  &lt;li&gt;gaming consoles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For phones we see this when a 12 megapixel camera shoots 4K video but the manufacturer sells it with only 32 gigs of ram.  And for gaming consoles we see this when a manufacturer ships a device with only a 500 gb drive but every single game you play might be 10+ gigs or more in size – and every game requires a full install + a multiple gig update.&lt;/p&gt;

&lt;p&gt;My kids have a PS4 – ok its actually mine but I’ve barely played it – and having gotten them the new PSVR for Christmas, we are officially out of space.  I had to delete some installed games just to make enough space to get things installed.  Clearly this is madness.&lt;/p&gt;

&lt;p&gt;My oldest son is an Xbox One gamer mostly and he has an external drive where he can just install all his extra games and it it works &lt;em&gt;beautifully&lt;/em&gt;.  Unfortunately the PS4 &lt;strong&gt;cannot&lt;/strong&gt; play games from an external drive and while this sucks green monkey chunks, it isn’t as bad as you might think because Sony made upgrading the hard drive in a PS4 brilliantly simple.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;You need a new 9 millimeter hard drive of at least bigger than your current.  I went with 2 tb.  Here’s the &lt;a href=&quot;https://www.amazon.com/gp/product/B00FRHTSK4/ref=oh_aui_detailpage_o01_s00?ie=UTF8&amp;amp;psc=1&quot;&gt;amazon link&lt;/a&gt; for about $80.  This is in a case so you can reuse the old drive in case you want to.  You also need a backup drive so you don’t lose anything.&lt;/li&gt;
  &lt;li&gt;Here are two youtube videos to watch: &lt;a href=&quot;https://www.youtube.com/watch?v=fkIyCXbiGZs&quot;&gt;Better&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=YDQL0qvt3Qk&quot;&gt;ok&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Take a blank, USB FAT, FAT32 or EXFAT drive of at least the same size as the drive in your PS4 (this should be either 500 gigs or 1 tb).  Connect the drive to the PS4.&lt;/li&gt;
  &lt;li&gt;Go into the PS4 settings and turn off all power down options so the PS4 stays on no matter what.&lt;/li&gt;
  &lt;li&gt;Go into each PS4 account and sync all trophies with the PlayStation Network.&lt;/li&gt;
  &lt;li&gt;Go into the PS4 settings and do a full backup.  This will take hours and hours.  For me it was like 6 hours.&lt;/li&gt;
  &lt;li&gt;Open the case which is done by powering down the system and disconnecting all cables.  Slide the case open per the video above.  Unscrew the hard drive and add a new one.  Any 9 millimeter USB 3 hard drive should do.&lt;/li&gt;
  &lt;li&gt;After you try and power the device on you’ll get a message on screen about “ps4 update file for reinstallation” which basically means &lt;em&gt;you have no operating system so go download one&lt;/em&gt; from Sony.&lt;/li&gt;
  &lt;li&gt;Take a USB stick with at least 1.5 gigs of free space on it and create a directory PS4 and then a directory PS4/UPDATE and place the PS4UPDATE.PUP file on it which you get from this [Sony PS4 download page]https://www.playstation.com/en-us/support/system-updates/ps4/.  Insert this into the PS4 and then power it on per their instructions holding down the power button for 7 seconds and it will prompt you to re-install the OS.&lt;/li&gt;
  &lt;li&gt;Once the PS4 is actually running again then go back into settings and select backup / restore and now choose restore and the backup drive will be loaded back onto your PS4 and all games, achievements, trophies, etc will be available to you.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/gaming/2017/01/03/tutorial-how-to-upgrade-your-ps4-to-2-terabytes-of-storage.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/gaming/2017/01/03/tutorial-how-to-upgrade-your-ps4-to-2-terabytes-of-storage.html</guid>
        
        <category>tutorial</category>
        
        <category>ps4</category>
        
        
        <category>gaming</category>
        
      </item>
    
      <item>
        <title>Recent PostMac Round Up</title>
        <description>&lt;p&gt;Well a new year and people are still unhappy about the state of the Mac.  Here are some interesting links worth following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chuqui.com/2017/01/apples-2016-in-review/&quot;&gt;Apple 2016 in Review&lt;/a&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=13307040&quot;&gt;HN Commentary&lt;/a&gt;  His point on Apple relying on data too much is likely correct.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=13307648&quot;&gt;Regular Restarts on OSX&lt;/a&gt;  Yep.  Right there with you.  Even my brand new box had to be restarted today after only 10 days of uptime.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@searls/warm-takes-on-microsofts-surface-pro-4-580f77634d2c#.7ix3536dx&quot;&gt;Warm Takes on Microsoft’s Surface Pro 4&lt;/a&gt;  All I can say here is that I have an equivalent setup to my OSX box on Linux for my daily work and I’ve had a far easier time than he has.  Overall a fantastic read.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=13299585&quot;&gt;Using a ThinkPad for Development under Linux&lt;/a&gt; (good discussion of gear and options here and also on the &lt;a href=&quot;https://news.ycombinator.com/item?id=13286150&quot;&gt;X1 Carbon article&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/postmac/2017/01/03/recent-postmac-round-up.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/postmac/2017/01/03/recent-postmac-round-up.html</guid>
        
        <category>apple</category>
        
        <category>osx</category>
        
        <category>postmac</category>
        
        <category>windows</category>
        
        
        <category>postmac</category>
        
      </item>
    
      <item>
        <title>Processing Large Datasets On AWS Using Ruby, Rails and SideKiq</title>
        <description>&lt;p&gt;Two days ago I did a data processing task which previously took me a week – overnight.  I did this using the following technology stack:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ruby&lt;/li&gt;
  &lt;li&gt;Rails&lt;/li&gt;
  &lt;li&gt;AWS&lt;/li&gt;
  &lt;li&gt;Sidekiq&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My platform was a “cluster” of 40 m3.large AWS ec2 instances.  We all see a lot about cloud computing and using AWS / Azure / Google Cloud to do these types of large jobs but you rarely see what I think of as the hard details:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How do you get code deployed?&lt;/li&gt;
  &lt;li&gt;How do you fix bugs?&lt;/li&gt;
  &lt;li&gt;How do you deal with Capistrano failures when a box isn’t available and Capistrano doesn’t give you good feedback?&lt;/li&gt;
  &lt;li&gt;How do you get things coordinated?&lt;/li&gt;
  &lt;li&gt;How do you kick off the overall job?&lt;/li&gt;
  &lt;li&gt;How do you know when it is done?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this blog post I’m going to illustrate how I managed these things.  I’m not saying that what I did was the only way to do this.  I’m not even saying that what I did was the best way to do this.  What I am saying is that this is a practical approach to ad hoc large data processing jobs using a ruby / rails / sidekiq approach.  And I’m going to describe how I did this without using cloud formation or another large, complicated AWS or third party API.  The only external tool I used was Ansible and even that was optional.&lt;/p&gt;

&lt;p&gt;For obvious reasons of company confidentiality I can’t go into the details of what the job was.  Suffice it to say:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a “lot” of data had to be “processed”&lt;/li&gt;
  &lt;li&gt;the actions were time consuming including deliberate sleep calls to avoid being blocked on the remote end&lt;/li&gt;
  &lt;li&gt;about 35,000 discrete data items needed to be processed.  With sleep calls at a randomized 10 to 15 seconds between each call that’s 350,000 to 525,000 seconds in aggregate compute time (less if threaded but too many threads and we get blocked)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, with that said, here’s how I went about this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;build an ec2 instance as a template&lt;/li&gt;
  &lt;li&gt;deploy the current code onto it&lt;/li&gt;
  &lt;li&gt;test&lt;/li&gt;
  &lt;li&gt;make an image&lt;/li&gt;
  &lt;li&gt;launch the job on the template box&lt;/li&gt;
  &lt;li&gt;launch more copies of the image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each of these is described below.&lt;/p&gt;

&lt;p&gt;My thanks go out to &lt;a href=&quot;http://www.nickjanetakis.com/&quot;&gt;Nick&lt;/a&gt; who was a consultant on this and paired on it throughout the process.  I also have to say thank you to &lt;a href=&quot;http://www.mikeperham.com&quot;&gt;Mike Perham&lt;/a&gt; who built &lt;a href=&quot;http://www.sidekiq.org&quot;&gt;Sidekiq&lt;/a&gt; which is at the heart of this.&lt;/p&gt;

&lt;h1 id=&quot;step-1---build-an-ec2-template-instance&quot;&gt;Step 1 - Build an EC2 “Template” Instance&lt;/h1&gt;

&lt;p&gt;The first step is that like with everything AWS you need an instance.  Picking the right instance type isn’t a topic that I’m going to cover here.  I did know that I needed a reduced thread count so I wasn’t terribly worried about memory.  We had already arranged with Amazon for up to 200 m3.large instances so that’s what I went with.  I didn’t worry terribly about whether not not I had the perfect instance type – I just used what was available.&lt;/p&gt;

&lt;p&gt;A m3.large is 7.5 gb of RAM and 8 gigs of storage so that’s perfectly fine for a Rails app of even large size.&lt;/p&gt;

&lt;p&gt;After I created the box I provisioned to run my Rails app as &lt;a href=&quot;https://fuzzygroup.github.io/blog/category.html#ansible&quot;&gt;per all the things I’ve written about using Ansible&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once we get this machine built out we’re going to be using it as a template for making more machines later hence my referring to this as a “template” instance.&lt;/p&gt;

&lt;h1 id=&quot;step-2---deploy-the-current-code-base-with-capistrano&quot;&gt;Step 2 - Deploy the Current Code Base with Capistrano&lt;/h1&gt;

&lt;p&gt;The next step was to get my code base onto the box using Capistrano.  I just added this box to my ~/.ssh/config file and then dropped the hostname into my config/deploy/production.rb file and did a normal deploy.&lt;/p&gt;

&lt;h1 id=&quot;step-3---test-test-test&quot;&gt;Step 3 - Test, Test, Test&lt;/h1&gt;

&lt;p&gt;At this point we have a single instance running our rails application.  We need to make very, very sure that this is working correctly because our next step is to make an &lt;strong&gt;image&lt;/strong&gt; of this instance and then use AWS to launch N copies of the image.  Here’s what you want to test:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;connectivity to your database&lt;/li&gt;
  &lt;li&gt;connectivity to your redis&lt;/li&gt;
  &lt;li&gt;that the job process code works&lt;/li&gt;
  &lt;li&gt;that sidekiq works&lt;/li&gt;
  &lt;li&gt;that your thread count is tuned properly&lt;/li&gt;
  &lt;li&gt;that sidekiq starts on boot&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This last point, that sidekiq starts on boot, is the key thing that you need to ascertain.  Since sidekiq is what’s going to run our jobs and we don’t want to manually ssh into each machine, we need a way for the job to start.  If sidekiq starts on boot then job processing begins automatically when the machine starts up.&lt;/p&gt;

&lt;p&gt;The only real way to verify this is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/sbin/reboot
log back into machine
ps auwwx | grep side
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you see sidekiq running then you have things configured correctly and sidekiq is starting on boot.&lt;/p&gt;

&lt;h1 id=&quot;step-4---make-an-image&quot;&gt;Step 4 - Make an Image&lt;/h1&gt;

&lt;p&gt;At this point you know that things work and you might be thinking - “Ok I now create a bunch more boxes; provision them and deploy with capistrano.”  That’s absolutely correct from a classical hosting perspective and absolutely wrong in a cloud environment.  The far easier, far faster approach is to make an &lt;em&gt;image&lt;/em&gt;. An image is simply a full disk copy of the instance that you can use to replicate the machine.  If you’re an old school PC guy then think of this as ghosting the machine.  Where installing things from scratch or even provisioning from ansible takes hours or minutes, cloning takes only a few minutes and then AWS can launch your instance in parallel so 40 machines might come up in just a minute or two.&lt;/p&gt;

&lt;p&gt;On your EC2 instance list select the instance and then on the Actions menu select Image, Create Image.  You’ll need to give it a name and the more descriptive, the better  It will take a minute or two but Amazon will make it just fine.&lt;/p&gt;

&lt;p&gt;Note: Making an instance shuts down the machine fully to make sure that any open files are backed up.  Keep this in mind since you’ll need to re-login to the machine for Step 5.&lt;/p&gt;

&lt;h1 id=&quot;step-5---launch-the-job-using-sidekiq-and-re-test-to-be-sure&quot;&gt;Step 5 - Launch the Job Using Sidekiq and Re-test to be Sure&lt;/h1&gt;

&lt;p&gt;At this point you’re ready to actually launch the job using sidekiq and start processing on one instance.  You can do this with the Rails console or a Rake task.  I prefer a rake task. Here’s what my rake task looked like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;task :some_large_job =&amp;gt; :environment do
  search_urls = MiscClass.large_urls_collection
  search_urls.each do |search_url|
    MiscCkassWorker.perform_async(search_url)
  end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That built a redis queue and gave each method to sidekiq as an asynchronous call to be processed.  Check your sidekiq log file to make sure that things are going ok.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If You find that there are changes you need to make then you’ll need to re-create the image as per Step 4.&lt;/p&gt;

&lt;p&gt;As long as things are looking fine then it is time for Step 6 – launching more copies of the image.&lt;/p&gt;

&lt;h1 id=&quot;step-6---launch-n-more-copies-of-the-image&quot;&gt;Step 6 - Launch N More Copies of the Image&lt;/h1&gt;

&lt;p&gt;The final step is to launch more copies of the image.  Because the job is already queued into redis and running, as soon as you launch any more instances the copy of sidekiq which runs on boot will start pulling jobs and processing them.&lt;/p&gt;

&lt;p&gt;Launch an instance the way you create any instance, only this time you’ll select that you want to make the instance from “My AMIs”] and then pick the image that you created in Step 4.  You can then tell AWS how many copies of the image you want made.  I specified 40 and then it is the normal AWS instance creation options like security groups and such.  Sadly all of these options aren’t defined solely in the instance itself.&lt;/p&gt;

&lt;p&gt;Note: The AWS command line tools or ansible code can be used to automate this further.&lt;/p&gt;

&lt;h1 id=&quot;step-7---make-your-wife-a-margarita&quot;&gt;Step 7 - Make Your Wife a Margarita&lt;/h1&gt;

&lt;p&gt;Well you can celebrate how you want but that’s what I did.  I checked the sidekiq queue the next morning and it was at 0.  I checked the database and we had generated 2,500 new records which was about what I expected.&lt;/p&gt;

&lt;h1 id=&quot;epiphany---realize-youre-making-an-appliance&quot;&gt;Epiphany - Realize You’re Making an Appliance!&lt;/h1&gt;

&lt;p&gt;I’m writing this blog post now having done this a dozen times or more. What finally made all this click in my head is the realization that what I’m doing here is making an &lt;em&gt;appliance&lt;/em&gt; or actually a &lt;em&gt;farm&lt;/em&gt; of appliances. An appliance is a tool which does one thing and does it well.  If you think about what we’ve done here is that we’ve made a ruby appliance in the form of an AWS image which eats data and (presumably) excretes some type of database record.&lt;/p&gt;

&lt;h1 id=&quot;circling-back-to-the-hard-questions-mentioned-earlier&quot;&gt;Circling Back to The Hard Questions Mentioned Earlier&lt;/h1&gt;

&lt;p&gt;At the start of this piece I mentioned a number of hard questions like deployment, bug fixing, etc.  Each of these is addressed below.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How do you get code deployed?  Capistrano is currently our tool for code deployment.  If we need to get a code fix onto the boxes we built off the template we add the ec2 host name into our SSH config and then just do a deploy.  We are currently writing a simple deployer in Ansible to make deploy easier and more integral with the entire process.  Hopefully I’ll be able to open source that at some point.  Yes we looked at &lt;a href=&quot;https://github.com/ansistrano/deploy&quot;&gt;Anistrano&lt;/a&gt; but Anistrano lacks critical rails features like bundle install which I find to be an absolute show stopper on using it.&lt;/li&gt;
  &lt;li&gt;How do you fix bugs?  We try very hard to test up front to avoid having to fix bugs on a long running job.  We streamlined our testing and focused hard on it before the jobs began deliberately to minimize bugs.&lt;/li&gt;
  &lt;li&gt;How do you deal with Capistrano failures when a box isn’t available and Capistrano doesn’t give you good feedback?  This remains an issue.  When Capistrano fails on a multiple box deploy it often isn’t clear why and Capistrano is specifically designed to stop when a single box in a deploy fails.  This contrasts nicely with Ansible which is specifically designed to continue despite failing.&lt;/li&gt;
  &lt;li&gt;How do you get things coordinated?  Coordination is always, always hard.  I have some interesting ideas on management tools for pulling this together but it isn’t time yet to implement them.&lt;/li&gt;
  &lt;li&gt;How do you kick off the overall job?  We use a Rake task which is my default for automation and is documented above.&lt;/li&gt;
  &lt;li&gt;How do you know when it is done? We don’t have a great answer yet on this.   Again I have some interesting ideas but we’re not yet at the implementation stage yet.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>https://fuzzygroup.github.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/rails/2017/01/03/processing-large-datasets-on-aws-using-ruby-rails-and-sidekiq.html</guid>
        
        <category>ruby</category>
        
        <category>rails</category>
        
        <category>sidekiq</category>
        
        <category>aws</category>
        
        
        <category>rails</category>
        
      </item>
    
  </channel>
</rss>

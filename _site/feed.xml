<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FuzzyBlog</title>
    <description>Scott Johnson writing about the usual array of nerd stuff: AWS / Ansible / Ruby / Rails / Elixir / Misc.
</description>
    <link>https://fuzzygroup.github.io/blog/</link>
    <atom:link href="https://fuzzygroup.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 24 Oct 2016 07:49:56 -0400</pubDate>
    <lastBuildDate>Mon, 24 Oct 2016 07:49:56 -0400</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>AWS Tutorial 21 - Naming Your EC2 Machines, Defining Your Bash Prompt and More</title>
        <description>&lt;p&gt;It has been said that there are really only two hard problems in computer science – &lt;a href=&quot;http://martinfowler.com/bliki/TwoHardThings.html&quot;&gt;naming things and cache invalidation&lt;/a&gt;.  Once upon a time I would have argued against that but increasingly I tend to fall into this camp.  I would tend to argue that if you can’t name something that you don’t &lt;em&gt;understand&lt;/em&gt; it.  And I’ve recently run up against this in terms of my EC2 instances:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;how should they be named&lt;/li&gt;
  &lt;li&gt;where should that name be represented&lt;/li&gt;
  &lt;li&gt;how should the bash prompt look&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I know this sounds simple but its really not.  We’re still at the stage of actively debugging our EC2 cluster of boxes and I’m regularly zipping into a box or out of a box via SSH along with frequent cursing such as “Dang it – why isn’t X working #&lt;em&gt;$#&amp;amp;$()#$&lt;/em&gt;#($)”.  I know according to a lot of people you’re not supposed to bother SSH’ing into EC2 instances – they are supposed to be single purpose, ephemeral, etc.  But even though everything is live, well, we’re still debugging &lt;strong&gt;everything&lt;/strong&gt;.  Sorry folks but that’s just the state of my life right now.&lt;/p&gt;

&lt;p&gt;So I’ve got some number of EC2 instances – more than 10, less than 1000 and any one of them could fail at any point.  I might learn about failures from &lt;a href=&quot;https://fuzzygroup.github.io/blog/aws/2016/10/20/aws-tutorial-20-adding-machine-and-process-monitoring-to-your-aws-instance-with-inspeqtor.html&quot;&gt;Inspeqtor&lt;/a&gt; or &lt;a href=&quot;https://fuzzygroup.github.io/blog/aws/2016/10/16/aws-tutorial-17-wrapping-up-our-ssh-issues-by-using-monit-for-process-monitoring.html&quot;&gt;Monit&lt;/a&gt; or by some other means (log files).  Some failures I might discard and others I might need to investigate at the box level by ssh’ing in.  So what has this told us:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We need to easily be able to get into any box at any time&lt;/li&gt;
  &lt;li&gt;Any alerts that we get need to include the machine name&lt;/li&gt;
  &lt;li&gt;We need to know what box we’re in when we are in it (otherwise its easy to get confused)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And what do we know about working with EC2 boxes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The instance id is really all that Amazon cares about&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given that we know that EC2 machines are supposed to be non snowflakes, ephemeral and automatically provisioned, this, to me, argues for a naming convention something like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PrefixRoleNumber
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;with those tokens defined as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;prefix - something distinct to the overall project; allows you to have different machines in the same aws account with the same role&lt;/li&gt;
  &lt;li&gt;role - what the machine is doing; its function; something like worker or web&lt;/li&gt;
  &lt;li&gt;number - just an integer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So if you this all together you get a naming structure like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fiworker1&lt;/li&gt;
  &lt;li&gt;fiweb1&lt;/li&gt;
  &lt;li&gt;fiworker199&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And then if you pair this with an ssh config file where names like these are mapped to hosts, particularly if you generate your SSH config file dynamically, you can easily ssh into any box by typing something like &lt;strong&gt;ssh fiworker199&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Note: If we are going to have a single AWS account manage boxes across multiple projects then we may want to inject the prefix into the role as well.  This would allow automated manipulation of classes of boxes by just tapping into the ec2 name / value space.  If we don’t do this then we might manipulate say all web servers for all projects at a time instead of just the web servers for a single project.&lt;/p&gt;

&lt;p&gt;So now we know how we want to name boxes, the question comes up as to our bash prompt (or zshell prompt if you’re fancy).  I’m not a bash expert by any means but I do know the importance of a good prompt.  What we likely want is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;username@local_ip@machine_name@instance_id:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s long but it actually gives us everything we need.  The advantage to a token like @ being injected is that when you double click a portion of the prompt you’ll get that information selected so you’re just a command+c away from copying it to the clipboard for use somewhere else.&lt;/p&gt;

&lt;h1 id=&quot;the-ansible-side-of-things&quot;&gt;The Ansible Side of Things&lt;/h1&gt;

&lt;p&gt;Given how much I do with Ansible, you’re likely not surprised that I’m going to automate this.  Given the number of boxes, automation is the only way to go and Ansible, while quirky, is a fantastic tool.&lt;/p&gt;

&lt;p&gt;Given how well my previous approach of illustrating the structure and then giving a git repo for the code went recently, I’m doing the same thing again:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tree .
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here’s the git repo.&lt;/p&gt;

&lt;p&gt;One of the tricks that we’re going to use to make this work is push variables at the host level into the inventory file.  This is a very simple trick&lt;/p&gt;

&lt;p&gt;Here’s how to use it:&lt;/p&gt;

</description>
        <pubDate>Sat, 22 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/aws/2016/10/22/aws-tutorial-21-what-should-your-ec2-bash-prompt-look-like.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/aws/2016/10/22/aws-tutorial-21-what-should-your-ec2-bash-prompt-look-like.html</guid>
        
        <category>aws</category>
        
        <category>bash</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Startup Learnings What Francois Schiettecatte Taught Me</title>
        <description>&lt;p&gt;Its been more than a few weeks since I’ve written anything categorized as startup so I guess its time for one of these.  In the Spring of 2003, I was happily running Feedster. We were maybe a few months old – and I saw we but it was really just me – and an email showed up over the transom that read something like this: “I found your contact info thru a DNS lookup.  My name is &lt;a href=&quot;https://fschiettecatte.wordpress.com&quot;&gt;Francois Schiettecatte&lt;/a&gt; and I have a small search engine called rss-search and I’m just 15 miles from you, perhaps we should have lunch”.  At lunch our personalities basically clicked and and a few weeks later, Francois and I were partners.  We felt:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;that we were stronger together than we were apart&lt;/li&gt;
  &lt;li&gt;Feedster was a substantially stronger brand than rss-search&lt;/li&gt;
  &lt;li&gt;our technological strengths complemented each other; Francois was strong on the core search engine and I had a better approach towards data management and structuring of everything&lt;/li&gt;
  &lt;li&gt;it feel too serendipitous – two people of entirely different backgrounds ending up 15 miles from each other doing the exact same thing at the exact same time.  Talk about signals from the universe…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway Francois and I then built Feedster together from that point forward.  I became the external face of the company while he focused on the search engine core that he had brought to the table.  We hired &lt;a href=&quot;https://fuzzygroup.github.io/blog/startup/2016/09/02/what-scott-rafer-taught-me.html&quot;&gt;Scott Rafer&lt;/a&gt; together.  And while it was, at times fractious, that’s normal for startup life.  He was a strong technical co-founder and I never regretted it.&lt;/p&gt;

&lt;p&gt;When I first met Francois at his condo in Salem, Mass, I saw his workspace and it was glorious – dual samsung flat screen monitors, a long workspace and an &lt;a href=&quot;http://www.hermanmiller.com/about-us/press/press-releases/all/herman-miller-launches-new-aeron-chair.html&quot;&gt;&lt;strong&gt;Aeron chair&lt;/strong&gt;&lt;/a&gt;.  At that time I was still using the Nth in a long succession of crappy Stables / Office Max chairs.  I had big chairs, small steno chairs, chairs with arms and chairs without.  Each one always cost around $100 and they all, universally &lt;strong&gt;sucked&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;That day Francois taught me a valuable lesson: &lt;strong&gt;It is ok to spend money on yourself; even on something as simple as a chair&lt;/strong&gt;.  When you think about it, my job could easily be described as &lt;em&gt;I sit professionally, 7 days a week&lt;/em&gt;.  I spend more time in an office chair than I do with my kids, my wife or any other place in my house.  Yes that includes sleep – I average maybe 6 hours per night so that’s 42 hours per week – but I work an easy 70 plus hours per week.&lt;/p&gt;

&lt;p&gt;It would be years later before I got an Aeron chair of my very own.  My wife, bless her heart, took me to Carmel Indiana for a Father’s Day surprise and lo and behold she had arranged for me to get an Aeron chair.  I’ve now been sitting in that same chair going on a decade now and it is still fantastic.&lt;/p&gt;

&lt;p&gt;Francois was brilliant at figuring out what to spend money on and doing it well.  I’d like to think that I picked up at least some of that from him.&lt;/p&gt;

</description>
        <pubDate>Fri, 21 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/startup/2016/10/21/startup-learnings-what-francois-schiettecatte-taught-me.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/startup/2016/10/21/startup-learnings-what-francois-schiettecatte-taught-me.html</guid>
        
        <category>startup</category>
        
        
        <category>startup</category>
        
      </item>
    
      <item>
        <title>AWS Tutorial 20 - Adding Machine and Process Monitoring To Your AWS Instances with Inspeqtor</title>
        <description>&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: Inspeqtor is an excellent piece of open source software, any errors are mine and mine alone.  This was fully tested by deploying onto a clean EC2 instance and verifying that it functioned correctly end to end.&lt;/p&gt;

&lt;p&gt;One of the aspects of cloud computing versus traditional hosting is that with cloud computing you tend to work with computing resources that, in general, are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;less &lt;strong&gt;powerful&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;less &lt;strong&gt;reliable&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;have less &lt;strong&gt;storage&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally there tend to be &lt;strong&gt;more&lt;/strong&gt; of these resources.  One way to term this might be that traditional data centers are &lt;em&gt;molecular&lt;/em&gt; where as cloud computing is more &lt;em&gt;atomic&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In my own experience, I ran a data center for 5 years without having to worry about process monitoring and tools like Monit or Inspeqtor but the very first time I put my AWS data center under heavy load, I found things &lt;a href=&quot;https://fuzzygroup.github.io/blog/tag.html#ssh&quot;&gt;crashing right, left and center&lt;/a&gt;.  All of my problems were magically solved simply by the addition of &lt;a href=&quot;https://mmonit.com/monit/&quot;&gt;Monit&lt;/a&gt; to watch dog the sidekiq process and restart it when it gets too large.  And while this solved my sidekiq problem, two nights ago, I ran out of disc space on a key resource – my MariaDB instance.&lt;/p&gt;

&lt;p&gt;One approach would be to continue to use Monit and add rules to it for disc space monitoring but I’ve been intrigued by the &lt;em&gt;simple&lt;/em&gt; configuration that &lt;a href=&quot;http://www.mikeperham.com&quot;&gt;Mike Perham’s&lt;/a&gt; &lt;a href=&quot;https://github.com/mperham/inspeqtor&quot;&gt;Inspeqtor&lt;/a&gt; offers.  Sidekiq has served me well as of late and Mike’s support, even &lt;a href=&quot;https://fuzzygroup.github.io/blog/ruby/2016/10/10/a-conversation-with-mike-perham.html&quot;&gt;the free community support&lt;/a&gt;, he offers is fantastic.  So rather than double down on Monit, I’m going to branch out and use Inspeqtor.&lt;/p&gt;

&lt;h1 id=&quot;goals&quot;&gt;Goals&lt;/h1&gt;

&lt;p&gt;We want to use Inspeqtor as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;be configured on all boxes by ansible so we never have to do anything manually&lt;/li&gt;
  &lt;li&gt;function on Ubuntu 14.04 with upstart&lt;/li&gt;
  &lt;li&gt;deliver alerts by email (sendmail) that contain the problem and the instance id&lt;/li&gt;
  &lt;li&gt;monitor sidekiq&lt;/li&gt;
  &lt;li&gt;monitor apache&lt;/li&gt;
  &lt;li&gt;monitor disc space&lt;/li&gt;
  &lt;li&gt;monitor ram&lt;/li&gt;
  &lt;li&gt;monitor load&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;inspeqtor-vs-monit&quot;&gt;Inspeqtor vs Monit&lt;/h1&gt;

&lt;p&gt;Monit and Inspeqtor are two very different animals.  Whereas Monit is a general purpose monitoring tool, Inspeqtor is specialized focusing on process that are run thru &lt;a href=&quot;https://github.com/mperham/inspeqtor/wiki/Initd&quot;&gt;init.d / upstart&lt;/a&gt; as well as generalized machine configuration.  So while you can technically do more with Monit, you’ll have a much easier time doing what you generally need with Inspeqtor.&lt;/p&gt;

&lt;h1 id=&quot;configuring-sendmail&quot;&gt;Configuring Sendmail&lt;/h1&gt;

&lt;p&gt;Inspeqtor can work with a number of different email delivery approaches from gmail to a local sendmail instance.  The configuration for Inspeqtor for different email delivery engines looks like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#
# This is the default alert statement.  This tells Inspeqtor where to
# send alert emails.
#
# Here we'll configure the default to send email alerts via gmail to &quot;dev@example.com&quot;
#
# send alerts via gmail
#   with username mike, password fuzzbucket, to_email dev@example.com

#
# Here's a generic email example, not requiring Google Mail.
# Your SMTP server must accept Authentication/TLS.
#
# send alerts via email with
#   username bob,
#   password &quot;foo bar baz&quot;,
#   smtp_server smtp.example.com,
#   tls_port 587,
#   to_email analytics@example.com,
#   from_email inspeqtor@example.com

#
# Here is another generic email example, not requiring authentication.
# Your local SMTP server must be listening on port 25.
#
send alerts via email with
  smtp_server localhost,
  to_email fuzzygroup@gmail.com,
  from_email inspeqtor@
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’ve got the other approaches commented out just showing the local smtp_server (in my case sendmail).&lt;/p&gt;

&lt;p&gt;Here’s an ansible role to configure sendmail:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p ansible_root/roles/sendmail/tasks
touch ansible_root/roles/sendmail/tasks/main.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Edit the file main.yml and add these lines:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: install sendmail 
  apt: name=sendmail state=present
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In your playbook, call this role as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- { role: sendmail, tags: sendmail}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here’s how to verify if your local sendmail instance is actually running:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo &quot;ficrawler1 My test email being sent from sendmail&quot; | /usr/sbin/sendmail fuzzygroup@gmail.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Check your inbox for the message.  You may find that you need to check a spam or junk folder since this isn’t a modern mail server using SPIF / DKIM standards.  If the message didn’t arrive then you need to troubleshoot and figure out why.&lt;/p&gt;

&lt;h1 id=&quot;configuring-inspeqtor-with-ansible&quot;&gt;Configuring Inspeqtor with Ansible&lt;/h1&gt;

&lt;p&gt;Inspeqtor relies on several files that determine how it works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;/etc/inspeqtor/inspeqtor.conf – how the overall inspeqtor instance runs and how to notifies&lt;/li&gt;
  &lt;li&gt;/etc/inspeqtor/host.inq – what to monitor about the host itself&lt;/li&gt;
  &lt;li&gt;/etc/inspeqtor/services.d/WHATEVER_YOU_WANT_TO_MONITOR.inq&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Examples of each of these are given below.&lt;/p&gt;

&lt;h2 id=&quot;here-is-etcinspeqtorinspeqtorconf&quot;&gt;Here is /etc/inspeqtor/inspeqtor.conf&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#
# Welcome to the global Inspeqtor config file!
#

#
# The cycle time is how often Inspeqtor will capture metrics and
# verify rules, in seconds.
#
set cycle_time 15

#
# The deploy length is the maximum length of your application deploys, in
# seconds. If you start a deploy and then never signal its finish, Inspeqtor will
# time out the deploy after this many seconds and start checking rules again.
#
# This is a failsafe.  Normally you will signal Inspeqtor when your
# deploys finish.
#
set deploy_length 300

#
# Set logging level, legal values are:
#   warn
#   info (default)
#   debug (-l debug)
#   verbose (-l verbose)
# At info, inspeqtor will not log anything when everything is ok.
#
set log_level info

# Inspeqtor Pro can send collected metrics to Statsd
# set statsd_location localhost:8125

#
# This is the default alert statement.  This tells Inspeqtor where to
# send alert emails.
#
# Here we'll configure the default to send email alerts via gmail to &quot;dev@example.com&quot;
#
# send alerts via gmail
#   with username mike, password fuzzbucket, to_email dev@example.com

#
# Here's a generic email example, not requiring Google Mail.
# Your SMTP server must accept Authentication/TLS.
#
# send alerts via email with
#   username bob,
#   password &quot;foo bar baz&quot;,
#   smtp_server smtp.example.com,
#   tls_port 587,
#   to_email analytics@example.com,
#   from_email inspeqtor@example.com

#
# Here is another generic email example, not requiring authentication.
# Your local SMTP server must be listening on port 25.
#
send alerts via email with
  smtp_server localhost,
  to_email fuzzygroup@gmail.com,
  from_email inspeqtor@ip-172-31-38-2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;here-is-etcinspeqtorhostinq&quot;&gt;Here is /etc/inspeqtor/host.inq&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;check host
  if load:1 &amp;gt; 1 for 2 cycles then alert
  if load:5 &amp;gt; 1 then alert
  if cpu:user &amp;gt; 95% for 2 cycles then alert
  if swap &amp;gt; 20% for 2 cycles then alert
  if disk:/ &amp;gt; 90% then alert
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;here-is-etcinspeqtorservicesdserviceinqtemplate&quot;&gt;Here is /etc/inspeqtor/services.d/service.inq.template&lt;/h2&gt;

&lt;p&gt;This is a generic starting point template to monitor any service in /etc/init.d&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat  /etc/inspeqtor/services.d/service.inq.template
# NOTE this file should be renamed to &amp;lt;name&amp;gt;.inq where name is explained below.
#
# Inspeqtor is designed to monitor a host and the services running
# on that host. Services must be controlled by your OS's init system:
# upstart, systemd, launchd or runit.
#
# Inspeqtor knows how to monitor services for each major init system,
# as long as you give the exact name of that service.
#
# In systemd:
#   /usr/lib/systemd/system/&amp;lt;name&amp;gt;.service
# In upstart:
#   /etc/init/&amp;lt;name&amp;gt;.conf
# In runit:
#   /etc/service/&amp;lt;name&amp;gt;/run
# In launchd:
#   ~/Library/LaunchAgents/&amp;lt;name&amp;gt;.plist
#
# Supporting traditional init.d is a little trickier, see the
# https://github.com/mperham/inspeqtor/wiki/Initd wiki page
# for more details. tl;dr You need to populate a PID file at
# /var/run/&amp;lt;name&amp;gt;.pid or /var/run/&amp;lt;name&amp;gt;/&amp;lt;name&amp;gt;.pid
#

#
# Here we define the service to monitor. The name of the service
# ('mysql') must match the name that your init system uses.
# You'll want to rename this file to mysql.inq to match.
#
check service mysql

  #
  # if you want to monitor daemon-specific metrics, you'll need
  # to tell Inspeqtor how to connect to the daemon.
  # See https://github.com/mperham/inspeqtor/wiki/Daemon-Specific-Metrics
  #
  #with username root, socket /var/run/mysqld/mysqld.sock

  #
  # Add any normal process metrics you want to verify.
  #
  if memory:rss &amp;gt; 2g then alert

  #
  # Since a cycle defaults to 15 seconds, this rule triggers if
  # there's excessive CPU usage for more than 30 seconds.
  #
  if cpu:user &amp;gt; 90% for 2 cycles then alert

  #
  # Alert if we see too many queries or slow queries. These are
  # examples of Daemon-Specific Metrics.
  #
  #if mysql:Queries &amp;gt; 100/sec for 2 cycles then alert
  #if mysql:Slow_queries &amp;gt; 1/sec for 2 cycles then alert    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For more on writing your own inq files, see the &lt;a href=&quot;https://github.com/mperham/inspeqtor/wiki/INQ-Configuration&quot;&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;here-is-my-sample-sidekiqinq-file&quot;&gt;Here is my sample sidekiq.inq file&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;check service sidekiq
  if memory:rss &amp;gt; 6g then alert, restart
  if cpu:user &amp;gt; 95% for 2 cycles then alert
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;configuring-inspeqtor-with-ansible-1&quot;&gt;Configuring Inspeqtor with Ansible&lt;/h2&gt;

&lt;p&gt;Rather than write out a playbook, roles and template files manually, I hosted it on &lt;a href=&quot;https://github.com/fuzzygroup/ansible_inspeqtor&quot;&gt;github&lt;/a&gt;.  Clone it from there and adapt it for your needs.  But, in case you’re curious, here is the overall structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tree
.
├── ansible.cfg
├── group_vars
│   └── all
├── inventories
│   └── ficrawler11
├── playbook_inspeqtor.yml
├── playbooks
├── readme.md
└── roles
    ├── inspeqtor
    │   ├── files
    │   │   ├── apache.inq
    │   │   ├── host.inq
    │   │   ├── inspeqtor.conf
    │   │   └── sidekiq.inq
    │   └── tasks
    │       └── main.yml
    ├── sendmail
    │   └── tasks
    │       └── main.yml
    └── setup
        └── tasks
            └── main.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The setup task exists to register an ansible variable that gives the instance-id so it can be used in alerting.  This is handled by calling the &lt;a href=&quot;https://fuzzygroup.github.io/blog/aws/2016/09/07/aws-tutorial-getting-your-instance-id.html&quot;&gt;instance id api&lt;/a&gt; which I covered previously.  While there is an instance_ids method in the &lt;a href=&quot;http://docs.ansible.com/ansible/ec2_module.html&quot;&gt;Ansible EC2 module&lt;/a&gt;, this approach means you don’t have your security keys as its a private API you only call from inside the instance itself.&lt;/p&gt;

&lt;h1 id=&quot;managing-inspeqtor-on-a-daily-basis&quot;&gt;Managing Inspeqtor on a Daily Basis&lt;/h1&gt;

&lt;p&gt;With almost any Unix tool you need to know how to do at least two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start / stop&lt;/li&gt;
  &lt;li&gt;view logs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Start / Stop on Ubuntu is handled with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service inspeqtor restart
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Logs can be viewed with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo tail -f /var/log/upstart/inspeqtor.log  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;more-info&quot;&gt;More Info&lt;/h1&gt;

&lt;p&gt;More info on Inspeqtor can be found on the &lt;a href=&quot;https://github.com/mperham/inspeqtor/wiki&quot;&gt;wiki&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/aws/2016/10/20/aws-tutorial-20-adding-machine-and-process-monitoring-to-your-aws-instance-with-inspeqtor.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/aws/2016/10/20/aws-tutorial-20-adding-machine-and-process-monitoring-to-your-aws-instance-with-inspeqtor.html</guid>
        
        <category>aws</category>
        
        <category>inspeqtor</category>
        
        <category>ubuntu</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Review - Indianpolis Ansible Meetup</title>
        <description>&lt;p&gt;I attended the Indianapolis Ansible meetup last night and it was a wonderful, albeit amusing, success.  The thrust of the meetup was watching an Ansible Fest video on TDD and Ansible.  And, well, it was a hoot!  I have rarely, if ever, watched a conference video that got something so damn wrong.  All the videos are &lt;a href=&quot;https://www.ansible.com/videos-ansiblefest-sf-2016&quot;&gt;here&lt;/a&gt; and most looked excellent.  The video we watched and that I mocked mercilessly is &lt;a href=&quot;https://www.ansible.com/beginners-guide-to-testing-infrastructure-as-code&quot;&gt;here&lt;/a&gt;.  The problems with it were many including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The date on the video was 2016 but it felt like we had time traveled back a decade or more “there’s this thing called extreme programming”&lt;/li&gt;
  &lt;li&gt;The guy who made it terms himself a “Agile Coach” which confirms everything I’ve ever thought about methodologies&lt;/li&gt;
  &lt;li&gt;He advocated that you test not your main ansible playbook but a playbook specifically designed for being tested.  Sheesh.  That’s ludicrous.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All problems aside, Jason and Nick did a great job hosting a meetup and I’ll definitely be back.  While the video was laughable, here’s some of the things I learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you’re going to test Ansible stuff then use &lt;a href=&quot;http://serverspec.org&quot;&gt;ServerSpec&lt;/a&gt; which is basically RSpec for machine configuration.&lt;/li&gt;
  &lt;li&gt;I’m not the only one who despises the RSpec expect syntax - ServerSpec is using .should also.  Yay!&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cloudone.com&quot;&gt;CloudOne&lt;/a&gt; is doing really interesting things with IOT right here in Indianapolis about 10 minutes from my house.  Bizarre!&lt;/li&gt;
  &lt;li&gt;Brixx Pizza is not particularly good (the post meetup food).  Good flavor but horrible crust.&lt;/li&gt;
  &lt;li&gt;I’m not the only one who finds almost everything on Ansible Galaxy to be poorly done and barely usable.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understanding the scope of your variables in Ansible is confusing to everyone not just me.  I recommended:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://coderwall.com/p/13lh6w/dump-all-variables&quot;&gt;https://coderwall.com/p/13lh6w/dump-all-variables&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/f500/ansible-dumpall&quot;&gt;https://github.com/f500/ansible-dumpall&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;They recommended &lt;a href=&quot;https://github.com/nickjj/ansigenome&quot;&gt;Ansible Genome&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can also use the &lt;a href=&quot;http://stackoverflow.com/questions/18839509/where-can-i-get-a-list-of-ansible-pre-defined-variables&quot;&gt;setup&lt;/a&gt; command which has a tricky syntax.  I finally got it to work with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ansible -m setup ficrawlerbig -i inventories/production2 -u ubuntu
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and it produced this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ficrawlerbig | SUCCESS =&amp;gt; {
    &quot;ansible_facts&quot;: {
        &quot;ansible_all_ipv4_addresses&quot;: [
            &quot;172.31.36.55&quot;,
            &quot;172.17.42.1&quot;
        ],
        &quot;ansible_all_ipv6_addresses&quot;: [
            &quot;fe80::42c:36ff:fe65:9b93&quot;
        ],
        &quot;ansible_architecture&quot;: &quot;x86_64&quot;,
        &quot;ansible_bios_date&quot;: &quot;05/11/2016&quot;,
        &quot;ansible_bios_version&quot;: &quot;4.2.amazon&quot;,
        &quot;ansible_cmdline&quot;: {
            &quot;BOOT_IMAGE&quot;: &quot;/boot/vmlinuz-3.13.0-92-generic&quot;,
            &quot;console&quot;: &quot;ttyS0&quot;,
            &quot;ro&quot;: true,
            &quot;root&quot;: &quot;UUID=87d0529b-216b-4930-9b54-45b0cdca9c06&quot;
        },
        &quot;ansible_date_time&quot;: {
            &quot;date&quot;: &quot;2016-10-19&quot;,
            &quot;day&quot;: &quot;19&quot;,
            &quot;epoch&quot;: &quot;1476872950&quot;,
            &quot;hour&quot;: &quot;10&quot;,
            &quot;iso8601&quot;: &quot;2016-10-19T10:29:10Z&quot;,
            &quot;iso8601_basic&quot;: &quot;20161019T102910892399&quot;,
            &quot;iso8601_basic_short&quot;: &quot;20161019T102910&quot;,
            &quot;iso8601_micro&quot;: &quot;2016-10-19T10:29:10.892455Z&quot;,
            &quot;minute&quot;: &quot;29&quot;,
            &quot;month&quot;: &quot;10&quot;,
            &quot;second&quot;: &quot;10&quot;,
            &quot;time&quot;: &quot;10:29:10&quot;,
            &quot;tz&quot;: &quot;UTC&quot;,
            &quot;tz_offset&quot;: &quot;+0000&quot;,
            &quot;weekday&quot;: &quot;Wednesday&quot;,
            &quot;weekday_number&quot;: &quot;3&quot;,
            &quot;weeknumber&quot;: &quot;42&quot;,
            &quot;year&quot;: &quot;2016&quot;
        },
        &quot;ansible_default_ipv4&quot;: {
            &quot;address&quot;: &quot;172.31.36.55&quot;,
            &quot;alias&quot;: &quot;eth0&quot;,
            &quot;broadcast&quot;: &quot;172.31.47.255&quot;,
            &quot;gateway&quot;: &quot;172.31.32.1&quot;,
            &quot;interface&quot;: &quot;eth0&quot;,
            &quot;macaddress&quot;: &quot;06:2c:36:65:9b:93&quot;,
            &quot;mtu&quot;: 9001,
            &quot;netmask&quot;: &quot;255.255.240.0&quot;,
            &quot;network&quot;: &quot;172.31.32.0&quot;,
            &quot;type&quot;: &quot;ether&quot;
        },
        &quot;ansible_default_ipv6&quot;: {},
        &quot;ansible_devices&quot;: {
            &quot;xvda&quot;: {
                &quot;holders&quot;: [],
                &quot;host&quot;: &quot;&quot;,
                &quot;model&quot;: null,
                &quot;partitions&quot;: {
                    &quot;xvda1&quot;: {
                        &quot;sectors&quot;: &quot;16755795&quot;,
                        &quot;sectorsize&quot;: 512,
                        &quot;size&quot;: &quot;7.99 GB&quot;,
                        &quot;start&quot;: &quot;16065&quot;
                    }
                },
                &quot;removable&quot;: &quot;0&quot;,
                &quot;rotational&quot;: &quot;0&quot;,
                &quot;sas_address&quot;: null,
                &quot;sas_device_handle&quot;: null,
                &quot;scheduler_mode&quot;: &quot;deadline&quot;,
                &quot;sectors&quot;: &quot;16777216&quot;,
                &quot;sectorsize&quot;: &quot;512&quot;,
                &quot;size&quot;: &quot;8.00 GB&quot;,
                &quot;support_discard&quot;: &quot;0&quot;,
                &quot;vendor&quot;: null
            }
        },
        &quot;ansible_distribution&quot;: &quot;Ubuntu&quot;,
        &quot;ansible_distribution_major_version&quot;: &quot;14&quot;,
        &quot;ansible_distribution_release&quot;: &quot;trusty&quot;,
        &quot;ansible_distribution_version&quot;: &quot;14.04&quot;,
        &quot;ansible_dns&quot;: {
            &quot;nameservers&quot;: [
                &quot;172.31.0.2&quot;
            ],
            &quot;search&quot;: [
                &quot;us-west-2.compute.internal&quot;
            ]
        },
        &quot;ansible_docker0&quot;: {
            &quot;active&quot;: false,
            &quot;device&quot;: &quot;docker0&quot;,
            &quot;id&quot;: &quot;8000.56847afe9799&quot;,
            &quot;interfaces&quot;: [],
            &quot;ipv4&quot;: {
                &quot;address&quot;: &quot;172.17.42.1&quot;,
                &quot;broadcast&quot;: &quot;global&quot;,
                &quot;netmask&quot;: &quot;255.255.0.0&quot;,
                &quot;network&quot;: &quot;172.17.0.0&quot;
            },
            &quot;macaddress&quot;: &quot;56:84:7a:fe:97:99&quot;,
            &quot;mtu&quot;: 1500,
            &quot;promisc&quot;: false,
            &quot;stp&quot;: false,
            &quot;type&quot;: &quot;bridge&quot;
        },
        &quot;ansible_domain&quot;: &quot;us-west-2.compute.internal&quot;,
        &quot;ansible_env&quot;: {
            &quot;HOME&quot;: &quot;/home/ubuntu&quot;,
            &quot;LANG&quot;: &quot;en_US.UTF-8&quot;,
            &quot;LC_ALL&quot;: &quot;en_US.UTF-8&quot;,
            &quot;LC_MESSAGES&quot;: &quot;en_US.UTF-8&quot;,
            &quot;LOGNAME&quot;: &quot;ubuntu&quot;,
            &quot;MAIL&quot;: &quot;/var/mail/ubuntu&quot;,
            &quot;PATH&quot;: &quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&quot;,
            &quot;PWD&quot;: &quot;/home/ubuntu&quot;,
            &quot;RAILS_ENV&quot;: &quot;production&quot;,
            &quot;SHELL&quot;: &quot;/bin/bash&quot;,
            &quot;SHLVL&quot;: &quot;1&quot;,
            &quot;SSH_CLIENT&quot;: &quot;64.184.12.117 61220 22&quot;,
            &quot;SSH_CONNECTION&quot;: &quot;64.184.12.117 61220 172.31.36.55 22&quot;,
            &quot;SSH_TTY&quot;: &quot;/dev/pts/0&quot;,
            &quot;TERM&quot;: &quot;xterm-256color&quot;,
            &quot;USER&quot;: &quot;ubuntu&quot;,
            &quot;XDG_RUNTIME_DIR&quot;: &quot;/run/user/1000&quot;,
            &quot;XDG_SESSION_ID&quot;: &quot;93&quot;,
            &quot;_&quot;: &quot;/bin/sh&quot;
        },
        &quot;ansible_eth0&quot;: {
            &quot;active&quot;: true,
            &quot;device&quot;: &quot;eth0&quot;,
            &quot;ipv4&quot;: {
                &quot;address&quot;: &quot;172.31.36.55&quot;,
                &quot;broadcast&quot;: &quot;172.31.47.255&quot;,
                &quot;netmask&quot;: &quot;255.255.240.0&quot;,
                &quot;network&quot;: &quot;172.31.32.0&quot;
            },
            &quot;ipv6&quot;: [
                {
                    &quot;address&quot;: &quot;fe80::42c:36ff:fe65:9b93&quot;,
                    &quot;prefix&quot;: &quot;64&quot;,
                    &quot;scope&quot;: &quot;link&quot;
                }
            ],
            &quot;macaddress&quot;: &quot;06:2c:36:65:9b:93&quot;,
            &quot;module&quot;: &quot;ixgbevf&quot;,
            &quot;mtu&quot;: 9001,
            &quot;pciid&quot;: &quot;0000:00:03.0&quot;,
            &quot;promisc&quot;: false,
            &quot;type&quot;: &quot;ether&quot;
        },
        &quot;ansible_fips&quot;: false,
        &quot;ansible_form_factor&quot;: &quot;Other&quot;,
        &quot;ansible_fqdn&quot;: &quot;ip-172-31-36-55.us-west-2.compute.internal&quot;,
        &quot;ansible_gather_subset&quot;: [
            &quot;hardware&quot;,
            &quot;network&quot;,
            &quot;virtual&quot;
        ],
        &quot;ansible_hostname&quot;: &quot;ip-172-31-36-55&quot;,
        &quot;ansible_interfaces&quot;: [
            &quot;lo&quot;,
            &quot;docker0&quot;,
            &quot;eth0&quot;
        ],
        &quot;ansible_kernel&quot;: &quot;3.13.0-92-generic&quot;,
        &quot;ansible_lo&quot;: {
            &quot;active&quot;: true,
            &quot;device&quot;: &quot;lo&quot;,
            &quot;ipv4&quot;: {
                &quot;address&quot;: &quot;127.0.0.1&quot;,
                &quot;broadcast&quot;: &quot;host&quot;,
                &quot;netmask&quot;: &quot;255.0.0.0&quot;,
                &quot;network&quot;: &quot;127.0.0.0&quot;
            },
            &quot;ipv6&quot;: [
                {
                    &quot;address&quot;: &quot;::1&quot;,
                    &quot;prefix&quot;: &quot;128&quot;,
                    &quot;scope&quot;: &quot;host&quot;
                }
            ],
            &quot;mtu&quot;: 65536,
            &quot;promisc&quot;: false,
            &quot;type&quot;: &quot;loopback&quot;
        },
        &quot;ansible_lsb&quot;: {
            &quot;codename&quot;: &quot;trusty&quot;,
            &quot;description&quot;: &quot;Ubuntu 14.04.4 LTS&quot;,
            &quot;id&quot;: &quot;Ubuntu&quot;,
            &quot;major_release&quot;: &quot;14&quot;,
            &quot;release&quot;: &quot;14.04&quot;
        },
        &quot;ansible_machine&quot;: &quot;x86_64&quot;,
        &quot;ansible_machine_id&quot;: &quot;3b8fd0372b0e32185bef793d5787f9de&quot;,
        &quot;ansible_memfree_mb&quot;: 60182,
        &quot;ansible_memory_mb&quot;: {
            &quot;nocache&quot;: {
                &quot;free&quot;: 62374,
                &quot;used&quot;: 2046
            },
            &quot;real&quot;: {
                &quot;free&quot;: 60182,
                &quot;total&quot;: 64420,
                &quot;used&quot;: 4238
            },
            &quot;swap&quot;: {
                &quot;cached&quot;: 0,
                &quot;free&quot;: 0,
                &quot;total&quot;: 0,
                &quot;used&quot;: 0
            }
        },
        &quot;ansible_memtotal_mb&quot;: 64420,
        &quot;ansible_mounts&quot;: [
            {
                &quot;device&quot;: &quot;/dev/xvda1&quot;,
                &quot;fstype&quot;: &quot;ext4&quot;,
                &quot;mount&quot;: &quot;/&quot;,
                &quot;options&quot;: &quot;rw,discard&quot;,
                &quot;size_available&quot;: 748208128,
                &quot;size_total&quot;: 8309932032,
                &quot;uuid&quot;: &quot;&quot;
            }
        ],
        &quot;ansible_nodename&quot;: &quot;ip-172-31-36-55&quot;,
        &quot;ansible_os_family&quot;: &quot;Debian&quot;,
        &quot;ansible_pkg_mgr&quot;: &quot;apt&quot;,
        &quot;ansible_processor&quot;: [
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;,
            &quot;GenuineIntel&quot;,
            &quot;Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz&quot;
        ],
        &quot;ansible_processor_cores&quot;: 16,
        &quot;ansible_processor_count&quot;: 16,
        &quot;ansible_processor_threads_per_core&quot;: 1,
        &quot;ansible_processor_vcpus&quot;: 16,
        &quot;ansible_product_name&quot;: &quot;HVM domU&quot;,
        &quot;ansible_product_serial&quot;: &quot;NA&quot;,
        &quot;ansible_product_uuid&quot;: &quot;NA&quot;,
        &quot;ansible_product_version&quot;: &quot;4.2.amazon&quot;,
        &quot;ansible_python&quot;: {
            &quot;executable&quot;: &quot;/usr/bin/python&quot;,
            &quot;has_sslcontext&quot;: false,
            &quot;type&quot;: &quot;CPython&quot;,
            &quot;version&quot;: {
                &quot;major&quot;: 2,
                &quot;micro&quot;: 6,
                &quot;minor&quot;: 7,
                &quot;releaselevel&quot;: &quot;final&quot;,
                &quot;serial&quot;: 0
            },
            &quot;version_info&quot;: [
                2,
                7,
                6,
                &quot;final&quot;,
                0
            ]
        },
        &quot;ansible_python_version&quot;: &quot;2.7.6&quot;,
        &quot;ansible_selinux&quot;: false,
        &quot;ansible_service_mgr&quot;: &quot;upstart&quot;,
        &quot;ansible_ssh_host_key_dsa_public&quot;: &quot;AAAAB3NzaC1kc3MAAACBAPWIrEktKYrh0VIi4NX/RsuaR4nCi4+MiHz4JOI0jUu1b5EtL8QNE2wXWFOi9NqYDWe35E3Gfep8xUKHsFlVtnEXqnIebPYXwtB39eIolnJP7zuVLDGLB2Ny5/+apN0Md6Kr6ewX75MsvLqb8QlTF8xcp2gzLJycAr4lW5uI+sI3AAAAFQDtkRxdu5rw6iIhDnHNAIO/G0NiewAAAIALCsKgsr9wB1273VND4VLHf9/TjSWBQkNX8BBE+oMlAbmLIPHpNnl7aHmdyyJ6EPXGztqYtF2uacMhSxj5T05I8pqQgjPRDzoJuk1DSS/Q0JmLBkmMHDZxEzsOBcMLzlwSxNaBRhXfMJhCAcv1/rVQeioMIzK0H7RinmO1ypmUmwAAAIBD+Mpmrt9fdTs+442QOOgmAtNI/BsdFBncVPkVHroRH5gyAgmkt/a0D1MFNmv+FF63NXrcmpsuAAwiYsn6vF0ijwEre59vsT7IB4xmsY+2Sv1t11lUUWgPI1RgHCUZnbuAbRhceran34eiIC8gGQO94RkTpuR4+mJeVE8p/kO96w==&quot;,
        &quot;ansible_ssh_host_key_ecdsa_public&quot;: &quot;AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBMjrf03fvK2638wDbB+gsL6z5AQtXHFB37O6H8sJV0fJI++Jl1bOE5vEUQIJgm7KAjY5E78gBMbl93yY9uMJHdc=&quot;,
        &quot;ansible_ssh_host_key_ed25519_public&quot;: &quot;AAAAC3NzaC1lZDI1NTE5AAAAIP/0jQTWuLTf5s0puYbxJABXIs6vSv8DvLWWdRPr7Fb8&quot;,
        &quot;ansible_ssh_host_key_rsa_public&quot;: &quot;AAAAB3NzaC1yc2EAAAADAQABAAABAQDqBYCkVLZtWSjaN4bb5wJAKuxqWlWGoorvmRFsFkdNjUVP8HAV/ImcdJ/ifkPn0Vfoa0ioNHB0AJuyawUDKq6bS/8/rm4rcL2fuFSCfRXTUde+KWuMUOFyMGKRTuqJZxei3BLlbJCdsJRJGC5DDgQf9vlWhTYMlRMDgZKEai2Dii/2mHGN0gZRF/9OXkwRJvj3hASj43B4DnIJ5MKdfmjRDOA9NuGaL45KoA1j4Vjds96QZ4g8Wo1HBI/6iZZRohq58RvRvP/uIJrrQjcANXTYa4tIEgqHyIpuylLDWAtuDrj6/XninBs4oZu3++p8PE7eu++O0iq8IXk9B72VSKuF&quot;,
        &quot;ansible_swapfree_mb&quot;: 0,
        &quot;ansible_swaptotal_mb&quot;: 0,
        &quot;ansible_system&quot;: &quot;Linux&quot;,
        &quot;ansible_system_capabilities&quot;: [
            &quot;&quot;
        ],
        &quot;ansible_system_capabilities_enforced&quot;: &quot;True&quot;,
        &quot;ansible_system_vendor&quot;: &quot;Xen&quot;,
        &quot;ansible_uptime_seconds&quot;: 617356,
        &quot;ansible_user_dir&quot;: &quot;/home/ubuntu&quot;,
        &quot;ansible_user_gecos&quot;: &quot;Ubuntu&quot;,
        &quot;ansible_user_gid&quot;: 1000,
        &quot;ansible_user_id&quot;: &quot;ubuntu&quot;,
        &quot;ansible_user_shell&quot;: &quot;/bin/bash&quot;,
        &quot;ansible_user_uid&quot;: 1000,
        &quot;ansible_userspace_architecture&quot;: &quot;x86_64&quot;,
        &quot;ansible_userspace_bits&quot;: &quot;64&quot;,
        &quot;ansible_virtualization_role&quot;: &quot;guest&quot;,
        &quot;ansible_virtualization_type&quot;: &quot;xen&quot;,
        &quot;module_setup&quot;: true
    },
    &quot;changed&quot;: false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Which isn’t anything about the variables in my playbook but I think I have a much better handle on how to use ansible system level variables like the machine’s memory size which would let me dynamically adjust my sidekiq thread count for example.&lt;/p&gt;

&lt;p&gt;Here’s an example of the dumpall approach:&lt;/p&gt;

&lt;p&gt;create a playbook called playbook_dump_variables.yml and put this in it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- hosts: all
  tasks:
  - template:
      src: templates/dump_variables
      dest: /tmp/ansible_variables
  - fetch:
      src: /tmp/ansible_variables
      dest: &quot;_ansible_variables&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;create a template called templates/dump_variables and put this in it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HOSTVARS (ANSIBLE GATHERED, group_vars, host_vars) :



PLAYBOOK VARS:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Run it with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ansible-playbook -i inventories/production2 playbook_dump_variables.yml -u ubuntu
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Unfortunately this results in a low level python error (“RepresenterError: cannot represent an object: 172.31.36.55”) at the Yaml class level as described &lt;a href=&quot;https://github.com/openshift/openshift-ansible/issues/2401&quot;&gt;here&lt;/a&gt; which seems to be tied to Ansible’s Unicode implementation.&lt;/p&gt;

&lt;p&gt;This is really a pity since I think it is exactly what is needed but people seem to be actively working on the bug which is at least good.&lt;/p&gt;

&lt;p&gt;After this failed, I tried the coderwall trick above and that failed with different errors:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fatal: [ficrawler3]: FAILED! =&amp;gt; {&quot;changed&quot;: false, &quot;failed&quot;: true, &quot;msg&quot;: &quot;AnsibleUndefinedVariable: Unable to look up a name or access an attribute in template string (Module Variables (\&quot;group_vars\&quot;):\n--------------------------------\n \n\n).\nMake sure your variable name does not contain invalid characters like '-': unbound method default() must be called with JSONEncoder instance as first argument (got StrictUndefined instance instead)&quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Sheesh!  This shouldn’t be all that hard.  Sigh.&lt;/p&gt;
</description>
        <pubDate>Wed, 19 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/ansible/2016/10/19/review-indianpolis-ansible-meetup.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/ansible/2016/10/19/review-indianpolis-ansible-meetup.html</guid>
        
        <category>ansible</category>
        
        <category>tdd</category>
        
        <category>humor</category>
        
        <category>serverspec</category>
        
        
        <category>ansible</category>
        
      </item>
    
      <item>
        <title>Gluten Free Cookies for a Wedding</title>
        <description>&lt;p&gt;After just finishing my post about &lt;a href=&quot;https://fuzzygroup.github.io/blog/cooking/2016/09/26/how-to-become-a-better-cook.html&quot;&gt;How to Improve Your Cooking&lt;/a&gt;, my wife asked me &lt;em&gt;Can we make cookies for X’s wedding?&lt;/em&gt;  Oy.  X is a close friend of ours and she’s &lt;strong&gt;gluten free&lt;/strong&gt;.  And not any of the bakeries in the St Louis bakery are able to produce Gluten Free cookies either in bulk or apparently at all.  This calls for my standard &lt;a href=&quot;https://fuzzygroup.github.io/recipes/cookie/2016/09/25/max-s-gluten-free-double-chocolate-cookies.html&quot;&gt;gluten free cookie recipe&lt;/a&gt; but the volume calls for a re-examination and planning.  And, in typical nerdy fashion, we must start with some math and a benchmark.  And, yes, this is actually how an engineer approaches cooking at least when it matters.&lt;/p&gt;

&lt;p&gt;Since my wife and son will be doing the actual baking, I’m documenting the full process since its an interesting application of math to domestic science.&lt;/p&gt;

&lt;h1 id=&quot;part-1-the-double-chocolate-cookies&quot;&gt;Part 1: The Double Chocolate Cookies&lt;/h1&gt;

&lt;h2 id=&quot;step-1-math&quot;&gt;Step 1: Math&lt;/h2&gt;

&lt;p&gt;Note: The addition of a second cookie type was a late change based on a successful cooking experiment with a new recipe.  Oy.  Let’s increase the complexity!&lt;/p&gt;

&lt;p&gt;My wife estimates we need to make 250 cookies, 125 double chocolate and 125 peanut butter.  This calls for a Google sheet to work up some numbers:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.google.com/spreadsheets/d/16IW5moFWTUneeNsLTIMt-c6uhXCAtQXOxy0p9yPSt0A/pubhtml&quot;&gt;https://docs.google.com/spreadsheets/d/16IW5moFWTUneeNsLTIMt-c6uhXCAtQXOxy0p9yPSt0A/pubhtml&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note: There’s some kind of subtle spreadsheet error since I ended up with too many chocolate chips at the end; I think I didn’t allow for the right number of chips per batch or something.&lt;/p&gt;

&lt;p&gt;This tells us that we need to bake for 1.5 hours w/ one sheet per oven (we have 2 ovens).  A quick optimization is to bake two sheets per oven.  This will give a slight increase in batch time from 12 minutes to 15 minutes but dramatically increase our thru put.&lt;/p&gt;

&lt;h2 id=&quot;step-2-benchmark&quot;&gt;Step 2: Benchmark&lt;/h2&gt;

&lt;p&gt;A benchmark is the standard way we need to get our metrics.  And the metric we need here is “How many cookies does a batch of batter make?”  This is going to require a standard method of sizing each cookie which means we need a &lt;a href=&quot;https://www.amazon.com/OXO-Grips-Medium-Cookie-Scoop/dp/B0000CDVD2/ref=sr_1_3?s=kitchen&amp;amp;ie=UTF8&amp;amp;qid=1474891817&amp;amp;sr=1-3&amp;amp;keywords=cookie+scoop&quot;&gt;cookie scoop&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once we have a cookie scoop then the next step is to bake a batch of cookies and figure out how many cookies a single batch of batter makes.  From that we can then figure out how many batches of cookie batter are needed.&lt;/p&gt;

&lt;h2 id=&quot;step-3-rewrite-the-recipe-for-weight-not-volume&quot;&gt;Step 3: Rewrite the Recipe for Weight not Volume&lt;/h2&gt;

&lt;p&gt;Something most american cooks are not aware of is that measuring dry ingredients by volume instead of weight is an american thing and it dates back to the westward expansion.  What happened was an enterprising cook book author took the perspective that scales were heavy and fragile so why not rewrite all her recipes using just a volume measure.  And that one thing changed the face of american baking.  Now from a technical perspective the measure of dry ingredients can vary as much as 25% depending on how you fill the measuring up and that clearly can affect the result.  Here’s a rewrite of our cookie recipe into a weight based approach:&lt;/p&gt;

&lt;p&gt;Weight based recipes are generally easier to execute because you can sit a mixing bowl on a scale and then tare the scale back to zero after you add each ingredient.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3/4	cups gluten free flour, Bobs Red Mill ==&amp;gt; 4.25 oz&lt;/li&gt;
  &lt;li&gt;3/4 cups buck wheat flour ==&amp;gt; 4 oz&lt;/li&gt;
  &lt;li&gt;2/3 cups coca powder ==&amp;gt; 2.5 oz&lt;/li&gt;
  &lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
  &lt;li&gt;1/2 tsp salt&lt;/li&gt;
  &lt;li&gt;1	cup butter flavored baking crisco (i.e. 1 stick); this is 2 sticks of actual butter&lt;/li&gt;
  &lt;li&gt;2/3	cups sugar ==&amp;gt; 5 oz&lt;/li&gt;
  &lt;li&gt;2/3	cups brown sugar ==&amp;gt; 5.75 oz&lt;/li&gt;
  &lt;li&gt;2	eggs&lt;/li&gt;
  &lt;li&gt;4	T rice milk or soy milk (dairy milk can be used) ==&amp;gt; 2 oz milk&lt;/li&gt;
  &lt;li&gt;1	T vanilla ==&amp;gt; 0.5 oz&lt;/li&gt;
  &lt;li&gt;1	bag semi sweet chocolate chips w/o milk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So here’s the full rewrite:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Preheat both ovens to 350 and line four baking sheets with parchment paper.&lt;/li&gt;
  &lt;li&gt;In a mixing bowl combine:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;4.25 oz Bob’s red mill gluten free flour&lt;/li&gt;
  &lt;li&gt;4 oz buck wheat flour&lt;/li&gt;
  &lt;li&gt;2.5 oz cocoa powder&lt;/li&gt;
  &lt;li&gt;1/2 tsp salt&lt;/li&gt;
  &lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;In the bowl for the standing mixer add the below ingredients and beat until fluffy:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2 sticks butter&lt;/li&gt;
  &lt;li&gt;5 oz brown sugar&lt;/li&gt;
  &lt;li&gt;5.75 oz brown sugar&lt;/li&gt;
  &lt;li&gt;2 eggs&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Add the below ingredients and mix:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2 oz milk&lt;/li&gt;
  &lt;li&gt;0.5 oz vanilla&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Add 1 bag chips and beat until combined.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use cookie scoop to place on cookie sheet and bake for 14 to 15 minutes rotating the sheets once at the 7 minute mark.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;step-4-efficient-execution&quot;&gt;Step 4: Efficient Execution&lt;/h2&gt;

&lt;p&gt;Making 125 of anything means that you want to do it efficiently.  The real constraints here are the fixed machinery of a kitchen specifically:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;KitchenAid Stand Mixer (we have 1)&lt;/li&gt;
  &lt;li&gt;Ovens (we have 2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To optimize the overall process, we can get a second mixing bowl for the stand mixer allowing two batches of cookie dough to be prepared at the same time.  We can also add additional identical cookie sheets to make sure that the baking process is as uniform as possible.  The efficiency gains from a second mixing bowl and additional cookie sheets far out weigh their relatively minimal cost.&lt;/p&gt;

&lt;h1 id=&quot;part-2-the-peanut-butter-cookies&quot;&gt;Part 2: The Peanut Butter Cookies&lt;/h1&gt;

&lt;h2 id=&quot;steps-1-2-4&quot;&gt;Steps 1, 2, 4&lt;/h2&gt;

&lt;p&gt;Given the detailed write up above, we don’t need to do this again.  The only real difference here is adapting the cookie recipe from volume to weight.&lt;/p&gt;

&lt;h2 id=&quot;step-3-rewrite-the-recipe-for-weight-not-volume-1&quot;&gt;Step 3: Rewrite the Recipe for Weight not Volume&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1 cup smooth peanut butter =&amp;gt; 9 oz (thanks &lt;a href=&quot;https://www.google.com/search?q=weight+one+cup+peanut+butter&amp;amp;ie=utf-8&amp;amp;oe=utf-8&quot;&gt;Google&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;1/2 cup light brown sugar =&amp;gt; 3.5 oz&lt;/li&gt;
  &lt;li&gt;1/2 cup white sugar =&amp;gt; 3.5 oz&lt;/li&gt;
  &lt;li&gt;1 large egg&lt;/li&gt;
  &lt;li&gt;1 tsp baking soda&lt;/li&gt;
  &lt;li&gt;1 tsp vanilla&lt;/li&gt;
  &lt;li&gt;1/4 tsp kosher salt&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So a rewritten version give us:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Preheat both ovens to 350 and line four baking sheets with parchment paper.&lt;/li&gt;
  &lt;li&gt;In a mixing bowl combine:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;9 oz peanut butter&lt;/li&gt;
  &lt;li&gt;3.5 oz brown sugar&lt;/li&gt;
  &lt;li&gt;3.5 oz white sugar&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Add below and beat until combined&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;egg&lt;/li&gt;
  &lt;li&gt;baking soda&lt;/li&gt;
  &lt;li&gt;vanilla&lt;/li&gt;
  &lt;li&gt;salt&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Use cookie scoop to place on cookie sheet and flatten with tines of fork and bake for 10 minutes rotating the sheets once at the 5 minute mark.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;postscript&quot;&gt;Postscript&lt;/h1&gt;

&lt;p&gt;Well I’ve achieved success:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/cookies_and_more_cookies.jpg&quot; alt=&quot;cookies_and_more_cookies.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing to know is that gluten free cookies come out softer than normal cookies.  This means they need to air dry for quite a long time to get hard enough to travel. The double chocolate chip cookies usually need 24 to 36 hours of sitting out before they are crisp enough.  The peanut butter cookies can generally make do with 8 to 12 hours of air drying time.&lt;/p&gt;

&lt;p&gt;The only downside to my success was that during the execution I figured out how to make them more efficiently still.  Rewritten below are both recipes.&lt;/p&gt;

&lt;h2 id=&quot;double-chocolate-chip-cookies&quot;&gt;Double Chocolate Chip Cookies&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Preheat both ovens to 350 and line four baking sheets with parchment paper.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a four cup measuring cup combine:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;4.25 oz Bob’s red mill gluten free flour&lt;/li&gt;
      &lt;li&gt;4 oz buck wheat flour&lt;/li&gt;
      &lt;li&gt;2.5 oz cocoa powder&lt;/li&gt;
      &lt;li&gt;1/2 tsp salt&lt;/li&gt;
      &lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the bowl for the standing mixer add the below ingredients and beat until fluffy:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2 sticks butter&lt;/li&gt;
  &lt;li&gt;5 oz white sugar&lt;/li&gt;
  &lt;li&gt;5.75 oz brown sugar&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2 eggs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add the below ingredients and mix:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2 oz milk&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;0.5 oz vanilla&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add 1 bag chips and beat until combined.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Use cookie scoop to place on cookie sheet and bake for 14 to 15 minutes rotating the sheets once at the 7 minute mark.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;peanut-butter-cookies&quot;&gt;Peanut Butter Cookies&lt;/h2&gt;

&lt;p&gt;Note: This is effectively a double batch made in a single mixing bowl for volume.  It will make at least 24 cookies (4 trays).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Preheat both ovens to 350 and line four baking sheets with parchment paper.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In a stand mixer combine:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;18 oz peanut butter&lt;/li&gt;
      &lt;li&gt;7 oz brown sugar&lt;/li&gt;
      &lt;li&gt;7 oz white sugar&lt;/li&gt;
      &lt;li&gt;2 eggs&lt;/li&gt;
      &lt;li&gt;2 tsp baking soda&lt;/li&gt;
      &lt;li&gt;2 tsp vanilla&lt;/li&gt;
      &lt;li&gt;1/2 tsp salt&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Use cookie scoop to place on cookie sheet and flatten with tines of fork (both ways; you’re making a grid).  Put in oven.&lt;/li&gt;
  &lt;li&gt;Set a 10 minute timer for the whole batch and a 5 minute timer so they can be rotated between top and bottom racks of the oven&lt;/li&gt;
  &lt;li&gt;Cool for at least 8 hours before you package for transport or try to put away.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 19 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/cooking/2016/10/19/gluten-free-cookies-for-a-wedding.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/cooking/2016/10/19/gluten-free-cookies-for-a-wedding.html</guid>
        
        <category>wedding</category>
        
        <category>cookies</category>
        
        <category>cooking</category>
        
        
        <category>cooking</category>
        
      </item>
    
      <item>
        <title>Adapting your Rails Tmux Development Flow to a Docker Development Flow</title>
        <description>&lt;p&gt;I first experimented with Docker back in summer of 2014 when I was bringing up a new data center and experimenting with development tools.  My buddy &lt;a href=&quot;http://dasari.me&quot;&gt;Dv&lt;/a&gt; remembers this all too well. This was back in the days where compose was still called fig and nothing actually worked all that well.  And since it didn’t work all that well, I quickly noped away.  But, even then, Docker had the feel of something important, something seminal.  Fast forward two years and Docker is all the rage and a bright luminary in the tech world.&lt;/p&gt;

&lt;p&gt;As a long time engineer, and yes you can translate that to “old guy”, I’m naturally conservative.  And, as such, I haven’t moved to Docker as part of my primary development flow.  To this point I’ve been using Docker as a way to treat applications as APIs and I’ve had a lot of success with that.  However, after a recent &lt;a href=&quot;https://fuzzygroup.github.io/blog/ruby/2016/10/15/brew-xz-and-nokogiri-and-tmux-an-unmitigated-disaster.html&quot;&gt;unmitigated disaster&lt;/a&gt;, involving the wonderful but troubled Nokogiri, I’m far more willing to explore Docker.&lt;/p&gt;

&lt;p&gt;In this blog post I’m going to use my open source &lt;a href=&quot;https://github.com/fuzzygroup/aws_monitor&quot;&gt;AWS Monitor&lt;/a&gt; codebase as the example.  This is a Rails application that is basically a Rake task which monitors a series of Ansible hosts to make sure that you can ssh into them.  It was written as part of my &lt;a href=&quot;https://fuzzygroup.github.io/blog/tag.html#ssh&quot;&gt;AWS / SSH hell period&lt;/a&gt; where I had a large application on AWS where the SSH servers would stay alive no longer than about 30 minutes.&lt;/p&gt;

&lt;h1 id=&quot;my-personal-development-flow&quot;&gt;My Personal Development Flow&lt;/h1&gt;

&lt;p&gt;I have a personal development flow that could essentially be described as “Lots and lots of terminal windows”.  I use a terminal window to represent each of the “stages” of Rails development:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;running server&lt;/li&gt;
  &lt;li&gt;generic command line&lt;/li&gt;
  &lt;li&gt;command line for deploy&lt;/li&gt;
  &lt;li&gt;rails console&lt;/li&gt;
  &lt;li&gt;test results&lt;/li&gt;
  &lt;li&gt;database sql window&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I used to manage all this with tabs in a terminal window but as I added more and more rails projects to my workflow that tended not to scale up.  Now my approach is to use the combination of &lt;a href=&quot;https://tmux.github.io&quot;&gt;Tmux&lt;/a&gt; and &lt;a href=&quot;https://github.com/tmuxinator/tmuxinator&quot;&gt;Tmuxinator&lt;/a&gt;, two open source projects.  Tmux acts as a virtual window manager allowing one terminal to front any number of character applications and Tmuxinator acts as an easy to use configuration manager for Tmux.  Here’s a picture of my typical approach to development:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/tmux_rails_dev_flow.png&quot; alt=&quot;tmux_rails_dev_flow.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I can call this interface up with ` (my meta key) and then go up and down between the different virtual terminal windows.  If you’re really sophisticated with Tmux / Tmuxinator you can actually compose up a real IDE but that’s a level of terminal nerdery that I don’t go to (at least for now).&lt;/p&gt;

&lt;p&gt;Given that all Rails apps are essentially the same architecture, this approach serves me well – it makes all the resources I need for Rails development no more than a few arrow keys away.&lt;/p&gt;

&lt;h1 id=&quot;adapting-this-for-docker&quot;&gt;Adapting this for Docker&lt;/h1&gt;

&lt;p&gt;If you think about how Docker works then adapting this isn’t readily apparent.  And then you talk with a &lt;a href=&quot;http://www.nickjanetakis.com/&quot;&gt;Docker expert&lt;/a&gt; and he teaches you how to &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/attach/&quot;&gt;attach to a running container&lt;/a&gt; and you start to get a glimmer of how to do this.  But then, when you stop for lunch, it might all fly out of your head.  That’s ok – it happened to me – but then I woke out of a dead sleep at 2 am two nights ago with all of this crystal clear.  And that’s when I wrote most of this.&lt;/p&gt;

&lt;p&gt;The realization that I had at 2 am was as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the Tmux window to run the docker-compose up –build&lt;/li&gt;
  &lt;li&gt;Use another Tmux window to attach to it&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;example-1-for-aws-monitor---my-local-awsmonitorlocalyml&quot;&gt;Example 1 for Aws Monitor - My Local aws_monitor_local.yml&lt;/h1&gt;

&lt;p&gt;My &lt;a href=&quot;https://github.com/fuzzygroup/aws_monitor&quot;&gt;AWS Monitor code base&lt;/a&gt; is the one I chose to adapt for this.  Here’s the local Tmuxinator file I use to run this (note since this is is such a minimal rails app, it only has two terminal windows):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# http://projectidealism.com/posts/2013/9/20/my-tmux-configuration-with-tmuxinator
# ~/.tmuxinator/tula.yml
# you can make as many tabs as you wish...

# NOTE -- this file belongs wherever tmuxinator looks for its configuration files

project_name: aws_monitor_local_aws
project_root: ~/Dropbox/fuzzygroup/rails/aws_monitor
socket_path: /tmp/tmux-aws-monitor
pre_tab: rvm use ruby-2.3.1@aws_monitor
windows:
  - cmdline:
  - console1: bundle exec rails c
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;example-2-for-aws-monitor---my-local-awsmonitordockeryml&quot;&gt;Example 2 for Aws Monitor - My Local aws_monitor_docker.yml&lt;/h1&gt;

&lt;p&gt;Here’s the equivalent for Docker:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# http://projectidealism.com/posts/2013/9/20/my-tmux-configuration-with-tmuxinator
# ~/.tmuxinator/tula.yml
# you can make as many tabs as you wish...

# NOTE -- this file belongs wherever tmuxinator looks for its configuration files

project_name: aws_monitor_local_aws
project_root: ~/Dropbox/fuzzygroup/rails/aws_monitor
socket_path: /tmp/tmux-aws-monitor
#pre_tab: rvm use ruby-2.3.1@aws_monitor
windows:
  - run_docker: cd ~/Dropbox/fuzzygroup/rails/aws_monitor &amp;amp;&amp;amp; docker-compose up --build
  - attach1: docker exec -it -v /Users:/Users awsmonitor_website_1 bash
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;dockerfile&quot;&gt;Dockerfile&lt;/h1&gt;

&lt;p&gt;My Dockerfile file for the aws_monitor app isn’t terribly interesting but it does have one interesting thing – it ends with a CMD statement which runs a rake task:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CMD rake aws_monitor:ansible_hosts --trace 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Since all we’re looking for aws_monitor to do is run continuously checking the ability to ssh into boxes, this is fine.&lt;/p&gt;

&lt;h1 id=&quot;docker-composeyml&quot;&gt;docker-compose.yml&lt;/h1&gt;

&lt;p&gt;Here’s the docker-compose.yml file&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '2'

services:
  postgres:
    image: 'postgres:9.5'
    environment:
      POSTGRES_USER: 'orats_base'
      POSTGRES_PASSWORD: 'yourpassword'
    ports:
      - '5432:5432'
    volumes:
      - 'postgres:/var/lib/postgresql/data'

  redis:
    image: 'redis:3.2-alpine'
    command: redis-server --requirepass yourpassword
    ports:
      - '6379:6379'
    volumes:
      - 'redis:/var/lib/redis/data'

  website:
    depends_on:
      - 'postgres'
      - 'redis'
    build: .
    ports:
      - '3000:3000'
    volumes:
      - '/Users:/Users'
      - '.:/orats_base'
      #- '/Users/sjohnson/.ssh/:/ssh_stuff'
      #- '/Users/sjohnson/Dropbox/music_alerts/:/music_alerts'
    env_file:
      - '.env'

  sidekiq:
    depends_on:
      - 'postgres'
      - 'redis'
    build: .
    command: sidekiq -C config/sidekiq.yml.erb
    volumes:
      - '.:/orats_base'
    env_file:
      - '.env'

  cable:
    depends_on:
      - 'redis'
    build: .
    command: puma -p 28080 cable/config.ru
    ports:
      - '28080:28080'
    volumes:
      - '.:/orats_base'
    env_file:
      - '.env'

volumes:
  redis:
  postgres:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The one interesting thing here is this line:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;volumes:
  - '/Users:/Users'
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;What this does is map my mac’s host filesytem into the Docker filesystem.  The reason I’m doing this is that my aws_monitor code needs access to my PEM certificate for the SSH login (among other files).  By mapping this in as a logical volume I don’t have to have have my PEM file as a part of my git repo.  I also don’t have to worry about Figaro or another approach to environment configuration.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Docker is still new to me and I’m still fine tuning this but I do think it has real potential.&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/docker/2016/10/18/adapting-your-rails-tmux-development-flow-to-a-docker-development-flow.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/docker/2016/10/18/adapting-your-rails-tmux-development-flow-to-a-docker-development-flow.html</guid>
        
        <category>rails</category>
        
        <category>docker</category>
        
        <category>tmux</category>
        
        
        <category>docker</category>
        
      </item>
    
      <item>
        <title>Understanding Systems By Observation - Dropbox</title>
        <description>&lt;p&gt;One of the best bits of computer science I ever learned, I learned in 1989 from my first business partner, Brian Giedt.  We were at a Society for Technical Communications (stc) conference on Technical Documentation and my partner was trying to impress a pretty girl.  And I watch him look at an animation product and pretty much instantly &lt;strong&gt;grok&lt;/strong&gt; how it was doing the animation.  Where I saw a pretty flow of images, he looked at it and understood how the animation was being done.  That was the very first time I saw someone really understand something about the &lt;em&gt;internals&lt;/em&gt; from its &lt;em&gt;externals&lt;/em&gt;.  And once I knew it was possible – I’ve striven to do it as often as I can.  Very often, if you set up the right set of circumstances, you’ll realize exactly how something has to be implemented internally.&lt;/p&gt;

&lt;p&gt;Let’s use Dropbox as an example.  We all know that Dropbox transfers the content you put in it to all other machines you have hooked up to it.  And that’s a simple 1 to many transfer.  But how does Dropbox work when you already have content in it and you re-arrange it?  Does it resend everything or does it figure out what it has to do and send a command stream to do it instead?&lt;/p&gt;

&lt;p&gt;A few minutes ago I:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;created a folder within a folder in Dropbox&lt;/li&gt;
  &lt;li&gt;moved about 15 gb of video files in initial folder to the new folder&lt;/li&gt;
  &lt;li&gt;checked on my iPad about a minute later&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And all the files I moved were in the new folder already.  Here’s what this tells me&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There’s no way that Dropbox deleted and re-transmitted the files in that time; it is simply impossible&lt;/li&gt;
  &lt;li&gt;What Dropbox has to be doing is sending commands that amount to move THIS from HERE to THERE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The best I’ve seen to do this is you set up conditions that you know can’t be argued with by physical constraints.  I knew that 15 gb of video data was a big arse chunk of data.  If I had used say a megabyte, well, I wouldn’t have really known if it was a full bandwidth re-arrange or a command stream.  By setting up such a large test, well, I knew something smarter had to be going on.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/software_engineering/2016/10/16/understanding-systems-by-observation-dropbox.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/software_engineering/2016/10/16/understanding-systems-by-observation-dropbox.html</guid>
        
        <category>software_engineering</category>
        
        
        <category>software_engineering</category>
        
      </item>
    
      <item>
        <title>Thinking About a Ruby Driven AWS Lambda Approach for Big Data Computing</title>
        <description>&lt;p&gt;So I have a computing problem to solve where the amount of data to process vastly exceeds even my desire to spin up EC2 instances.  I really do actually enjoy &lt;a href=&quot;https://fuzzygroup.github.io/blog/tag.html#ansible&quot;&gt;Ansible&lt;/a&gt; but at some point you have to cry to the heavens and shout out “There must be a better way!”.  This is literally an “oh crap” moment when I realized exactly the scale of the problem.&lt;/p&gt;

&lt;p&gt;Note: I’m not at liberty here to give specifics so I’m talking in generalities quite a bit in this post.  Apologies.&lt;/p&gt;

&lt;p&gt;AWS has a lot of tools that can be applied to big data process but two come to mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/emr/&quot;&gt;Elastic Map Reduce&lt;/a&gt; or EMR&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/lambda/&quot;&gt;Lambda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;EMR is older and harder to use so I’m going to avoid it for now in favor of the new hotness – &lt;strong&gt;Lambda&lt;/strong&gt;.  Here’s the brag statement about Lambda:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Run code without thinking about servers.
Pay for only the compute time you consume.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s a pretty compelling statement to make and it is actually close to a holy grail of distributed computing.&lt;br /&gt;
I have a big data problem that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Involves about 15,000 unique domains that need to be checked against an index of data&lt;/li&gt;
  &lt;li&gt;Where if the domain is a match a secondary request needs to be made and several data points extracted from an api&lt;/li&gt;
  &lt;li&gt;The results of #2 need to be posted to an API that we control&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For valid separation of concerns issues the data fed into 1 is separate from the code in 1 (its driven by a separate git repo).  And all of our code is in Ruby which is not a supported Lambda language.  This raises some actual issues in terms of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How do we make this work&lt;/li&gt;
  &lt;li&gt;How do we get our existing ruby code to run in Lambda&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is an interesting approach to packaging up our existing ruby code using Traveling Ruby and then run that &lt;a href=&quot;https://www.krautcomputing.com/blog/2016/02/29/how-to-run-ruby-scripts-on-aws-lambda-using-ansible/&quot;&gt;package on Lambda&lt;/a&gt;.  But before we goto that level of effort perhaps we need to examine the metrics on the codebase in question:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rake stats
+----------------------+--------+--------+---------+---------+-----+-------+
| Name                 |  Lines |    LOC | Classes | Methods | M/C | LOC/M |
+----------------------+--------+--------+---------+---------+-----+-------+
| Controllers          |      3 |      3 |       1 |       0 |   0 |     0 |
| Helpers              |      2 |      2 |       0 |       0 |   0 |     0 |
| Jobs                 |      2 |      2 |       1 |       0 |   0 |     0 |
| Models               |      3 |      3 |       1 |       0 |   0 |     0 |
| Mailers              |      4 |      4 |       1 |       0 |   0 |     0 |
| Channels             |      8 |      8 |       2 |       0 |   0 |     0 |
| Javascripts          |     29 |      4 |       0 |       1 |   0 |     2 |
| Libraries            |    480 |    366 |      12 |      38 |   3 |     7 |
| Tasks                |      7 |      6 |       0 |       0 |   0 |     0 |
| Controller tests     |      0 |      0 |       0 |       0 |   0 |     0 |
| Helper tests         |      0 |      0 |       0 |       0 |   0 |     0 |
| Model tests          |      0 |      0 |       0 |       0 |   0 |     0 |
| Mailer tests         |      0 |      0 |       0 |       0 |   0 |     0 |
| Integration tests    |      0 |      0 |       0 |       0 |   0 |     0 |
| Lib specs            |    270 |    231 |       0 |       1 |   0 |   229 |
+----------------------+--------+--------+---------+---------+-----+-------+
| Total                |    808 |    629 |      18 |      40 |   2 |    13 |
+----------------------+--------+--------+---------+---------+-----+-------+
  Code LOC: 398     Test LOC: 231     Code to Test Ratio: 1:0.6
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So we’re actually only talking about 12 classes, 38 methods and 366 LOC.  So this isn’t large at all.  We know that one approach to running this is running it as packaged ruby.  Another approach would be &lt;em&gt;code that writes code&lt;/em&gt; but before we talk about this, let’s talk about the execution context.&lt;/p&gt;

&lt;p&gt;This application needs to run every time period from 2013 to present.  And depending on the results from our initial prototype, that will be either monthly or quarterly.  So if this is quarterly we need to run it 12 times - (4 in 2013, 4 in 2015, 4 in 2016) and if its monthly then we need to run it 36 times.  And we’ll need to run it monthly going forward.  And, each time, the input data is different.&lt;/p&gt;

&lt;h1 id=&quot;code-that-writes-code&quot;&gt;Code that Writes Code&lt;/h1&gt;

&lt;p&gt;There are lots of different approaches to code that writes code – generators, real AI and template based approaches.  For this I would use a template based approach as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Port the Ruby code to JavaScript using a single file approach as much as possible.&lt;/li&gt;
  &lt;li&gt;Locate the input data we need to send to the initial api near the top as essentially a global array of values.  For test purposes you only need to use 1 or 2; only enough to prove the concept.&lt;/li&gt;
  &lt;li&gt;Convert the code in #2 to a template just as Rails uses, say, an ERB template (you can actually use ERB in other contexts than views).&lt;/li&gt;
  &lt;li&gt;Treat the underlying ruby code as a systems automation tool which:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;operates over an array of dates from the past to the present&lt;/li&gt;
  &lt;li&gt;keeps track of what has been submitted to AWS Lambda&lt;/li&gt;
  &lt;li&gt;Reads the template in #3 and fills in the data based on the internal api.&lt;/li&gt;
  &lt;li&gt;packages everything per the &lt;a href=&quot;http://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html&quot;&gt;Lambda spec&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;uses the &lt;a href=&quot;http://docs.aws.amazon.com/sdkforruby/api/Aws/Lambda/Client.html&quot;&gt;AWS Lambda Ruby APIs&lt;/a&gt; to submit the job&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this approach while we’re not using Lambda to actually execute our Ruby code, we’re getting the same result, using Lambda’s native JavaScript / node support but still using our overall Ruby framework to run everything and manage the process.  And this will have the advantage of generating an auditable code base that we can dig into if we find any problems.&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/aws/2016/10/16/thinking-about-a-ruby-driven-aws-lambda-approach-for-big-data-computing.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/aws/2016/10/16/thinking-about-a-ruby-driven-aws-lambda-approach-for-big-data-computing.html</guid>
        
        <category>aws</category>
        
        <category>ruby</category>
        
        <category>lambda</category>
        
        <category>software_engineering</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>AWS Tutorial 19 - Back to the Basics, Let's Talk AMIs and EC2 basics</title>
        <description>&lt;p&gt;I just used Hyde to examine my blog and I realized that I have written almost 20,000 words on AWS since 8/23/16 (note some of that is still unfinished and in draft form).  Using my standard writer metric of 250 words per page, that’s 78 printed pages.  Wow.  And, alas, I realize that there are still things I haven’t written about.  And some of them are the sort of basic things that you either just ignore or that you accept by rote - “I know, we’ll use Ubuntu, we love Ubuntu!”.  And, yes, that would be me.  So let’s take a deeper dive here at some of the basic options when you build an EC2 instance.&lt;/p&gt;

&lt;h1 id=&quot;what-is-an-ec2-instance&quot;&gt;What is an EC2 Instance?&lt;/h1&gt;

&lt;p&gt;An EC2 instance is just a server in Amazon’s cloud.  And, from what I can tell, pretty much everything AWS offers comes down to a server somehow.  When you build an EC2 instance you have to base it on an operating system which is called an AMI and there are a bunch of options that define what AMI you want to pick:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Quick Start - the most popular options&lt;/li&gt;
  &lt;li&gt;My AMIs - these are amis that you have saved from a machine you already built&lt;/li&gt;
  &lt;li&gt;AWS Marketplace - these are commercial offerings from vendors&lt;/li&gt;
  &lt;li&gt;Community AMIs - these are generally open source AMIs and the number is enormous – more than 50,000 when I checked&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s a picture of the initial EC2 instance selection web page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/aws/aws_ami_ec2.png&quot; alt=&quot;aws_ami_ec2.png.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a few basic options that you really want to keep in mind:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linux Distro&lt;/strong&gt;.  This is important but can’t be written in a bullet point so it is discussed below.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;32 / 64 bit&lt;/strong&gt;.  There’s no real reason to not go 64 bit.  And if you have a reason then you should be writing this not reading it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Virtualization Type&lt;/strong&gt;.  This should always be HVM as PVM is being phased out.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Root Device Type&lt;/strong&gt;.  This should pretty much always be set to EBS.  EBS allows you to turn off the volume without losing the data on the instance and thus allows you to resize your instance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-linux-distro-question&quot;&gt;The Linux Distro Question&lt;/h1&gt;

&lt;p&gt;Asking anyone in the Open Source world what is the best Linux flavor or “distro” (that’s short for distribution) is a bit like asking someone their favorite color – the answer is always different and always subjective.  And while there are differences, in the end, it is all Linux and if you can use one Linux then you can use a different Linux.  I know there are serious Linux folk that read this line and are gnashing their teeth and I apologize.&lt;/p&gt;

&lt;p&gt;Personally I’ve used at different times:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Red Hat&lt;/li&gt;
  &lt;li&gt;Gentoo (I even had a whole data center of more than 100 Gentoo boxes)&lt;/li&gt;
  &lt;li&gt;Ubuntu 12&lt;/li&gt;
  &lt;li&gt;Ubuntu 14&lt;/li&gt;
  &lt;li&gt;Mandriva&lt;/li&gt;
  &lt;li&gt;Suse&lt;/li&gt;
  &lt;li&gt;Debian&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And I’m pretty sure there were some others; that’s just want I can remember using.  The short answer is you want to pick a Linux distribution that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;matches what you want to do&lt;/li&gt;
  &lt;li&gt;is well supported&lt;/li&gt;
  &lt;li&gt;is something you understand&lt;/li&gt;
  &lt;li&gt;has a package manager that you can deal with&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-amazon-linux-distribution&quot;&gt;The Amazon Linux Distribution&lt;/h1&gt;

&lt;p&gt;Interestingly Amazon has their own Linux distribution.  I don’t have a ton of experience with it yet but I’m keenly interested in it and I really like their focus on performance.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good support for Docker&lt;/li&gt;
  &lt;li&gt;Good support for at least somewhat modern development tools; Ruby, PHP and Python all installed right from the start&lt;/li&gt;
  &lt;li&gt;Good package support for the basics - mysql, postgres, etc&lt;/li&gt;
  &lt;li&gt;AWS command line tools installed standard&lt;/li&gt;
  &lt;li&gt;Good support for the AWS ECS&lt;/li&gt;
  &lt;li&gt;Cool text mode EC2 login logo that makes me smile whenever I see it&lt;/li&gt;
  &lt;li&gt;They seem to really care about performance.  The 2016.09 release notes specifically call out the &lt;a href=&quot;https://aws.amazon.com/amazon-linux-ami/2016.09-release-notes/&quot;&gt;7 seconds of boot time&lt;/a&gt; that they cut out.  Sounds silly but its a big deal when you have a lot of machines.  And given that they write the billing rules, they could easily use that 7 seconds in their favor.  The fact that they don’t gives me an incredible amount of confidence in AWS’s billing practices.  Go AWS!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It can’t run anywhere but Amazon.&lt;/li&gt;
  &lt;li&gt;It can’t run on Vagrant for local development&lt;/li&gt;
  &lt;li&gt;Yum / RPM as package managers; this is a personal choice but I vastly prefer apt-get&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-important-is-this-with-automated-provisioning&quot;&gt;How Important Is this With Automated Provisioning?&lt;/h1&gt;

&lt;p&gt;In the days where you configured Linux manually, picking the right distribution was actually quite important.  Thanks to automated provisioning tools like Ansible, I’m not so sure now.  I’ve already used Ansible to move from one version of Linux to another and its just not that hard.  If you write your Ansible playbook properly and abstract things like the username into variables, you can modify it pretty easily to go between distros.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In the end you likely want to pick:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A well supported Linux distro.  I’d recommend either Ubuntu, RedHat or the Amazon Linux AMI&lt;/li&gt;
  &lt;li&gt;64 Bit&lt;/li&gt;
  &lt;li&gt;HVM Virtualization&lt;/li&gt;
  &lt;li&gt;EBS Root Device&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 16 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/aws/2016/10/16/aws-tutorial-19-back-to-the-basics-let-s-talk-amis.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/aws/2016/10/16/aws-tutorial-19-back-to-the-basics-let-s-talk-amis.html</guid>
        
        <category>aws</category>
        
        <category>ami</category>
        
        <category>linux</category>
        
        <category>ec2</category>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>AWS Tutorial 18 - When You've Lost Your Web Server, How to Find an AWS Resource</title>
        <description>&lt;p&gt;I find myself, at the time of this writing, in the middle of an embarrassing situtation for a web professional.  You see, the situation is this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I wrote a new feature&lt;/li&gt;
  &lt;li&gt;I deployed my new feature&lt;/li&gt;
  &lt;li&gt;I refreshed my page&lt;/li&gt;
  &lt;li&gt;My feature isn’t there&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Great Googly Moogly!  I’ve lost my web server!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let me explain one of the things about cloud hosting that’s disconcerting.  When you first move to the cloud, your impulse is to organize your computing resources the way you used to.  So if you used to have say 3 clusters of powerful machines, that’s what you do.  Then you realize just how mind blowingly powerful a platform like AWS actually is and you start to think about &lt;strong&gt;Single Purpose Servers&lt;/strong&gt;.  A single purpose server is just what it sounds like – it does one thing.  And that’s fantastic because it makes trouble shooting so much easier.  When a server does only one thing, well, its easy to know if its broken.  And that’s great but do you know what the side effect of that is?  You don’t have a handful of servers anymore, you have a lot.  Me?  I’ve got over &lt;strong&gt;20&lt;/strong&gt; right now.  And somewhere in there is my web server.  But I can’t find it.  In this tutorial we’re going to quickly and easily figure this out.&lt;/p&gt;

&lt;h1 id=&quot;start-with-a-hypothesis&quot;&gt;Start with a Hypothesis&lt;/h1&gt;

&lt;p&gt;As normal we’re going to start with a theory - that is one of these three boxes:  fimariadb, ficrawler1, ficrawler2.  So our diagnostic dance, crude tho it may be, is:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ssh into one of the boxes&lt;/li&gt;
  &lt;li&gt;sudo su -&lt;/li&gt;
  &lt;li&gt;apache2ctl stop&lt;/li&gt;
  &lt;li&gt;reload the page&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the page comes up, well, we know it wasn’t that one box.  So you then lather, rinse, repeat for each of the other 2 boxes.  And, at the end, we’re going to find out that it was none of these.&lt;/p&gt;

&lt;p&gt;You might be saying “Hey wait a minute – why would a web front end be on a box that does crawling?”  Well I’m still feeling all this out and I initially went for the old model where every box could do everything.  And that was a bad decision but I still have to live it for at least a little while longer.&lt;/p&gt;

&lt;h1 id=&quot;formulate-a-new-hypothesis---lets-use-ping&quot;&gt;Formulate a New Hypothesis - Let’s Use Ping!&lt;/h1&gt;

&lt;p&gt;Since our first plan failed, we need a new plan.  The program ping is a basic IP networking tool which lets us send a packet to a destination and if it answers, well, that means its alive.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ping banks.finavd.com
PING web-1166333941.us-west-2.elb.amazonaws.com (52.41.182.115): 56 data bytes
64 bytes from 52.41.182.115: icmp_seq=0 ttl=47 time=67.589 ms
64 bytes from 52.41.182.115: icmp_seq=1 ttl=47 time=67.301 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Ah ha!  We have an ip address of 52.41.182.115.  I know! I know!  I know!  I’ll just search for that ip address on EC2 dashboard.  And it will fail.  Now the smart kids in the back are already chuckling to themselves and they know the answer.&lt;/p&gt;

&lt;h1 id=&quot;hypothesis-3-elb-is-being-used&quot;&gt;Hypothesis 3: ELB Is Being Used&lt;/h1&gt;

&lt;p&gt;If you look at the url that responded, NOT the ip address, the answer is revealed:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;web-1166333941.us-west-2.elb.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You see the text string “.elb.”?  That means that a piece of software called an Elastic Load Balancer is sitting in front of the http request and distributing the load out to one or more EC2 instances.  If you’ve ever used HAProxy, well, ELB is that only far, far better.  Let’s goto the AWS Console and select the Load Balancers option from the choices on the left:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/aws/aws_elb_01_overview.png&quot; alt=&quot;aws_elb_01_overview.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we’ll see an overview of all of our load balancers and their basic settings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/assets/aws/aws_elb_02_instances.png&quot; alt=&quot;aws_elb_02_instances.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clicking the instances tab shows us where the HTTP request is being sent.  We can now goto the ec2 console and figure out what we need.  If you put the machine names into your &lt;a href=&quot;https://fuzzygroup.github.io/blog/aws/2016/09/20/aws-tutorial-08-using-ssh-s-config-file-with-your-aws-boxes.html&quot;&gt;SSH Config as I recommended&lt;/a&gt; then you might not even need to goto the console.  In my case I just needed to know the names worker2 and worker2a and I know that they’re in my ssh config file and I can just add those boxes to my Capistrano deploy process.  And the “bug” is fixed!&lt;/p&gt;

&lt;h1 id=&quot;conclusion-and-suggestion&quot;&gt;Conclusion and Suggestion&lt;/h1&gt;

&lt;p&gt;I know that it must seem like I’m a bit of a buffoon – how can you lost a web server after all?  Well, things do happen when you move fast.  You start with one plan and then it doesn’t work and before you know it you have something working but its not where you originally planned.  And you mean to fix it but you get busy and then the next &lt;a href=&quot;https://fuzzygroup.github.io/blog/aws/2016/10/01/aws-tutorial-10-diagnosing-ssh-failures-or-when-ping-works-but-ssh-fails.html&quot;&gt;crisis&lt;/a&gt; happens and you’re not even in the same head space any more.  And by the time you return to it over 10 days have passed.&lt;/p&gt;

&lt;p&gt;Here are some suggestions for setting up your AWS architecture to avoid this kind of silliness:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name things well.&lt;/li&gt;
  &lt;li&gt;Name things logically.&lt;/li&gt;
  &lt;li&gt;Use the key value options when you set up your EC2 servers.  For example, having keys for both name and role might have helped.&lt;/li&gt;
  &lt;li&gt;Remember that there are often abstractions around everything.&lt;/li&gt;
  &lt;li&gt;Try and use single purpose servers from the start.  Yes the number of discrete servers increases complexity but their very single purpose nature makes debugging vastly easier.  And keep in mind that Amazon offers free servers.  Even a t2.micro free instance has 1 gig of ram and 8 gigs of storage.  I know that sounds funny but travel back in your head 5 years and that’s a beefy server and its &lt;strong&gt;FREE&lt;/strong&gt;.  If you’re just running something small, say Redis, Memcached, sendmail, etc that might be enough for a lot of applications.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 16 Oct 2016 00:00:00 -0400</pubDate>
        <link>https://fuzzygroup.github.io/blog/aws/2016/10/16/aws-tutorial-18-when-you-ve-lost-you-web-server-how-to-find-an-aws-resource.html</link>
        <guid isPermaLink="true">https://fuzzygroup.github.io/blog/aws/2016/10/16/aws-tutorial-18-when-you-ve-lost-you-web-server-how-to-find-an-aws-resource.html</guid>
        
        <category>aws</category>
        
        
        <category>aws</category>
        
      </item>
    
  </channel>
</rss>

---
layout: post
title: Managing the Testing and Deployment of Large Scale Data Parsers
date: 
type: post
published: false
status: draft
categories: []
tags: []
meta: {}
author:
  login: fuzzyblog
  email: eric@appdata.com
  display_name: fuzzyblog
  first_name: Scott
  last_name: Johnson
---
<p>We just had the situtation at Feedster where our lead Crawler Developer, Nick, has developed a new parser and our management wanted to deploy it -- <strong>immediately</strong>.  So, early this morning (if you watch this blog then you'll see that I've been grinding since the wee hours; I've been working straight since 3 am so its now already a 14 hour day with no end in easy sight), I had to write one of my "Cranky Founder" emails that basically outlined the issues and argued (strongly) for more testing.</p>
<p>Now given that past developers who've worked with me might argue that I'm in the "Ready Fire Aim" school of software development, some of them (if not you) might be surprised that I'm arguing for heavy testing.  After all its just a parser right ?  I mean we can just roll back the code to the old parser, right?  </p>
<p>Ahem.  No.  Here's why.</p>
<ol>
<li></li>
<li></li>
<li></li>
</ol>
<p>How to Test Large Scale Data Parsers</p>
<p>So what are we going to do.  Well it looks something like this and Nick will be responsible for writing a test plan and having Francois and myself review it.</p>
<ol>
<li>Make a set of known good feeds.</li>
<li>Make a set of known bad feeds.</li>
<li>Take a known test set like the Mark Pilgrim benchmark.</li>
<li>Take a mock random sampling of feeds from Feedster's database.  I say Mock Random because you don't want 100</li>
</ol>
